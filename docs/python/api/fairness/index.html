<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dalex.fairness API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<style>.homelink{display:block;font-size:2em;font-weight:bold;color:#555;padding-bottom:.5em;border-bottom:1px solid silver}.homelink:hover{color:inherit}.homelink img{max-width:50%;max-height:6em;margin:auto;margin-bottom:.3em}</style>
<link rel="canonical" href="https://dalex.drwhy.ai/python/api/dalex/fairness/">
<link rel="icon" type="image/png" href="https://dalex.drwhy.ai/favicon.svg"/>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dalex.fairness</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/__init__.py#L1-L9" class="git-link">Browse git</a>
</summary>
<pre><code class="python">from ._group_fairness.object import GroupFairnessClassification, GroupFairnessRegression
from ._group_fairness.mitigation import reweight, roc_pivot, resample
__all__ = [
    &#34;GroupFairnessClassification&#34;,
    &#34;GroupFairnessRegression&#34;,
    &#34;reweight&#34;,
    &#34;roc_pivot&#34;,
    &#34;resample&#34;
]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dalex.fairness.resample"><code class="name flex">
<span>def <span class="ident">resample</span></span>(<span>protected, y, type='uniform', probs=None, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns indices of observations for data.</p>
<p>Method of bias mitigation. Similarly to 'reweight'
this method computes desired number of observations just
if the protected variable was independent from y and
on this basis decides if this subgroup with certain class
(favorable or not) should be more or less numerous. Than performs
oversampling or undersampling depending on the case.
If type of sampling is set to 'preferential' and probs
are provided than instead of uniform sampling preferential
sampling will be performed. Preferential sampling depending on the case
will sample observations close to border or far from border.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>protected</code></strong> :&ensp;<code>np.ndarray (1d)</code></dt>
<dd>Vector, preferably 1-dimensional np.ndarray containing strings,
which denotes the membership to a subgroup.
NOTE: List and pd.Series are also supported; however, if provided,
they will be transformed into a np.ndarray (1d) with dtype 'U'.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pd.Series</code> or <code>pd.DataFrame</code> or <code>np.ndarray (1d)</code></dt>
<dd>Target variable with outputs / scores. It shall have the same length as <code>protected</code></dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'uniform', 'preferential'}</code></dt>
<dd>Type indicates what strategy to use when choosing the samples.
(default is 'uniform')</dd>
<dt><strong><code>probs</code></strong> :&ensp;<code>np.ndarray (1d)</code></dt>
<dd>Vector with probabilities for each sample. Note that this should be
probabilities for favourable outcome. For the best performance they
should be consistent with 'y' but it is not required. This argument
is required when using strategy of type 'preferential'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Print messages about changes of types in 'y' and 'protected' (default is <code>True</code>).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray (1d)</code></dt>
<dd>Array with indices for the data.</dd>
</dl>
<h2 id="notes">Notes</h2>
<pre><code>- &lt;https://link.springer.com/content/pdf/10.1007/s10115-011-0463-8.pdf&gt;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/_group_fairness/mitigation.py#L147-L262" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def resample(protected, y, type = &#39;uniform&#39;, probs = None,  verbose = True):
    &#34;&#34;&#34;Returns indices of observations for data.

    Method of bias mitigation. Similarly to &#39;reweight&#39;
    this method computes desired number of observations just
    if the protected variable was independent from y and
    on this basis decides if this subgroup with certain class
    (favorable or not) should be more or less numerous. Than performs
    oversampling or undersampling depending on the case.
    If type of sampling is set to &#39;preferential&#39; and probs
    are provided than instead of uniform sampling preferential
    sampling will be performed. Preferential sampling depending on the case
    will sample observations close to border or far from border.

    Parameters
    -----------
    protected : np.ndarray (1d)
        Vector, preferably 1-dimensional np.ndarray containing strings,
        which denotes the membership to a subgroup.
        NOTE: List and pd.Series are also supported; however, if provided,
        they will be transformed into a np.ndarray (1d) with dtype &#39;U&#39;.
    y : pd.Series or pd.DataFrame or np.ndarray (1d)
        Target variable with outputs / scores. It shall have the same length as `protected`
    type : {&#39;uniform&#39;, &#39;preferential&#39;}
        Type indicates what strategy to use when choosing the samples.
        (default is &#39;uniform&#39;)
    probs : np.ndarray (1d)
        Vector with probabilities for each sample. Note that this should be
        probabilities for favourable outcome. For the best performance they
        should be consistent with &#39;y&#39; but it is not required. This argument
        is required when using strategy of type &#39;preferential&#39;
    verbose : bool
        Print messages about changes of types in &#39;y&#39; and &#39;protected&#39; (default is `True`).

    Returns
    -----------
    numpy.ndarray (1d)
        Array with indices for the data.

    Notes
    -----------
        - https://link.springer.com/content/pdf/10.1007/s10115-011-0463-8.pdf
    &#34;&#34;&#34;
    if type == &#39;preferential&#39; and probs is None:
        raise ParameterCheckError(&#34;when using type &#39;preferential&#39; probabilities (probs) must be provided&#34;)

    if type not in set([&#39;uniform&#39;, &#39;preferential&#39;]):
        raise ParameterCheckError(&#34;type must be either &#39;uniform&#39; or &#39;preferential&#39;&#34;)


    protected = basic_checks.check_protected(protected, verbose)
    y = basic_checks.check_y(y, verbose)

    if type == &#39;preferential&#39;:
        try:
            probs = np.asarray(probs)
            helper.verbose_cat(&#34;converted &#39;probs&#39; to numpy array&#34;, verbose=verbose)
        except Exception:
            raise ParameterCheckError(&#34;try converting &#39;probs&#39; to 1D numpy array&#34;)

        if probs.ndim != 1 or len(probs) != len(y):
            raise ParameterCheckError(&#34;probs parameter must 1D numpy array with the same length as y&#34;)


    weights = reweight(protected, y, verbose=False)

    expected_size =  dict.fromkeys(np.unique(protected))
    for key in expected_size.keys():
        expected_size[key] = dict.fromkeys(np.unique(y))

    for subgroup in expected_size.keys():
        for value in np.unique(y):
            case_weights = weights[(subgroup == protected) &amp; (value == y)]
            case_size = len(case_weights)
            weight = case_weights[0]
            expected_size[subgroup][value] = round(case_size * weight)

    indices = []

    for subgroup in expected_size.keys():
        for value in np.unique(y):
            current_case = np.arange(len(y))[(protected == subgroup) &amp; (y == value)]
            expected = expected_size[subgroup][value]
            actual = np.sum((protected == subgroup) &amp; (y == value))
            if expected == actual:
                 indices += list(current_case)

            elif expected &lt; actual:
                if type == &#39;uniform&#39;:
                    indices += list(np.random.choice(current_case, expected, replace=False))
                else:
                    sorted_current_case = current_case[np.argsort(probs[current_case])]
                    if value == 0:
                        indices += list(sorted_current_case[:expected])
                    if value == 1:
                        indices += list(sorted_current_case[-expected:])
            else:
                if type == &#39;uniform&#39;:
                    u_ind = list(np.repeat(current_case, expected // actual))
                    u_ind += list(np.random.choice(current_case, expected % actual))

                    indices += u_ind

                else:
                    sorted_current_case = current_case[np.argsort(probs[current_case])]
                    p_ind = list(np.repeat(current_case, expected // actual))

                    if expected % actual != 0:
                        if value == 0:
                            p_ind += list(sorted_current_case[-(expected % actual):])
                        if value == 1:
                            p_ind += list(sorted_current_case[:(expected % actual)])

                    indices += p_ind

    return np.array(indices)</code></pre>
</details>
</dd>
<dt id="dalex.fairness.reweight"><code class="name flex">
<span>def <span class="ident">reweight</span></span>(<span>protected, y, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Obtain weights for model training and mitigate bias in Statistical Parity.</p>
<p>Method produces weights for each subgroup for each class.
Firstly, it assumes that protected variable and class are
independent and calculates expected probability of this
certain event (that subgroup == a and class = c).
Than it calculates the actual probability of this event
based on empirical data. Finally the weight is quotient
of those probabilities.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>protected</code></strong> :&ensp;<code>np.ndarray (1d)</code></dt>
<dd>Vector, preferably 1-dimensional np.ndarray containing strings,
which denotes the membership to a subgroup.
NOTE: List and pd.Series are also supported; however, if provided,
they will be transformed into a np.ndarray (1d) with dtype 'U'.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pd.Series</code> or <code>pd.DataFrame</code> or <code>np.ndarray (1d)</code></dt>
<dd>Target variable with outputs / scores. It shall have the same length as <code>protected</code></dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Print messages about changes of types in 'y' and 'protected' (default is <code>True</code>).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray (1d)</code></dt>
<dd>Array with sample (case) weights</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li><a href="https://link.springer.com/content/pdf/10.1007/s10115-011-0463-8.pdf">https://link.springer.com/content/pdf/10.1007/s10115-011-0463-8.pdf</a></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/_group_fairness/mitigation.py#L12-L62" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def reweight(protected, y, verbose = True):
    &#34;&#34;&#34;Obtain weights for model training and mitigate bias in Statistical Parity.

    Method produces weights for each subgroup for each class.
    Firstly, it assumes that protected variable and class are
    independent and calculates expected probability of this
    certain event (that subgroup == a and class = c).
    Than it calculates the actual probability of this event
    based on empirical data. Finally the weight is quotient
    of those probabilities.

    Parameters
    -----------
    protected : np.ndarray (1d)
        Vector, preferably 1-dimensional np.ndarray containing strings,
        which denotes the membership to a subgroup.
        NOTE: List and pd.Series are also supported; however, if provided,
        they will be transformed into a np.ndarray (1d) with dtype &#39;U&#39;.
    y : pd.Series or pd.DataFrame or np.ndarray (1d)
        Target variable with outputs / scores. It shall have the same length as `protected`
    verbose : bool
        Print messages about changes of types in &#39;y&#39; and &#39;protected&#39; (default is `True`).

    Returns
    -----------
    numpy.ndarray (1d)
        Array with sample (case) weights

    Notes
    -----------
    - https://link.springer.com/content/pdf/10.1007/s10115-011-0463-8.pdf
    &#34;&#34;&#34;
    y = basic_checks.check_y(y, verbose)
    protected = basic_checks.check_protected(protected, verbose)

    if not len(y) == len(protected):
        raise ParameterCheckError(&#34;protected and target (y) must have the same length&#34;)

    weights = np.repeat(None, len(y))

    for subgroup in np.unique(protected):
        for c in np.unique(y):

            Xs = np.sum(protected == subgroup)
            Xc = np.sum(y == c)
            Xsc = np.sum((protected == subgroup) &amp; (c == y))
            Wsc = (Xs * Xc) / (len(y) * Xsc)

            weights[(protected == subgroup) &amp; (y == c)] = Wsc

    return weights</code></pre>
</details>
</dd>
<dt id="dalex.fairness.roc_pivot"><code class="name flex">
<span>def <span class="ident">roc_pivot</span></span>(<span>explainer, protected, privileged, cutoff=0.5, theta=0.05, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Reject Option based Classification pivot</p>
<p>Reject Option based Classifier is post-processing bias
mitigation method. Method changes the predictions of model
(probabilities) and returns new explainer with modified 'y_hat'.
Probabilities that are made for privileged subgroup and are
favorable, and close to cutoff are pivoted to the other side
of the cutoff. The opposite happens for unprivileged observations
(changing unfavorable and close to cutoff observations to favorable
by pivoting probabilities from left of the cutoff to right).
By this potentially wrongfully labeled observations are
assigned different labels. Note that 1 in y in Explainer
should indicate favorable outcome.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>explainer</code></strong> :&ensp;<code>Explainer</code></dt>
<dd>Explainer made from classification model.</dd>
<dt><strong><code>protected</code></strong> :&ensp;<code>np.ndarray (1d)</code></dt>
<dd>Vector, preferably 1-dimensional np.ndarray containing strings,
which denotes the membership to a subgroup.
NOTE: List and pd.Series are also supported; however, if provided,
they will be transformed into a np.ndarray (1d) with dtype 'U'.</dd>
<dt><strong><code>privileged</code></strong> :&ensp;<code>str</code></dt>
<dd>Subgroup that is suspected to have the most privilege.
It needs to be a string present in <code>protected</code>.</dd>
<dt><strong><code>cutoff</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold for probabilistic output of a classifier.</dd>
<dt><strong><code>theta</code></strong> :&ensp;<code>float</code></dt>
<dd>Value that indicates the radius of the area where values
are pivoted. The default is (0.05) which means that
the probabilities of privileged class within
(cutoff, cutoff+ theta)
will be pivoted to the other
side of the cutoff. The opposite thing will happen for
unprivileged subgroup.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Print messages about changes of types in 'y' and 'protected' (default is <code>True</code>).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Explainer class object</code></dt>
<dd>Explainer with changed 'y_hat'</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/6413831/">https://ieeexplore.ieee.org/document/6413831/</a></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/_group_fairness/mitigation.py#L64-L145" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def roc_pivot(explainer, protected, privileged, cutoff = 0.5, theta = 0.05, verbose = True):
    &#34;&#34;&#34;Reject Option based Classification pivot

    Reject Option based Classifier is post-processing bias
    mitigation method. Method changes the predictions of model
    (probabilities) and returns new explainer with modified &#39;y_hat&#39;.
    Probabilities that are made for privileged subgroup and are
    favorable, and close to cutoff are pivoted to the other side
    of the cutoff. The opposite happens for unprivileged observations
    (changing unfavorable and close to cutoff observations to favorable
    by pivoting probabilities from left of the cutoff to right).
    By this potentially wrongfully labeled observations are
    assigned different labels. Note that 1 in y in Explainer
    should indicate favorable outcome.

    Parameters
    -----------
    explainer: Explainer
        Explainer made from classification model.

    protected : np.ndarray (1d)
        Vector, preferably 1-dimensional np.ndarray containing strings,
        which denotes the membership to a subgroup.
        NOTE: List and pd.Series are also supported; however, if provided,
        they will be transformed into a np.ndarray (1d) with dtype &#39;U&#39;.
    privileged : str
        Subgroup that is suspected to have the most privilege.
        It needs to be a string present in `protected`.
    cutoff: float
        Threshold for probabilistic output of a classifier.
    theta: float
        Value that indicates the radius of the area where values
        are pivoted. The default is (0.05) which means that
        the probabilities of privileged class within
        (cutoff, cutoff+ theta)  will be pivoted to the other
        side of the cutoff. The opposite thing will happen for
        unprivileged subgroup.
    verbose : bool
        Print messages about changes of types in &#39;y&#39; and &#39;protected&#39; (default is `True`).

    Returns
    -----------
    Explainer class object
        Explainer with changed &#39;y_hat&#39;

    Notes
    -----------
    - https://ieeexplore.ieee.org/document/6413831/
    &#34;&#34;&#34;



    if not isinstance(explainer, dalex.Explainer):
        raise ParameterCheckError(&#34;explainer must be of type &#39;Explainer&#39;&#34;)

    if explainer.model_type != &#39;classification&#39;:
        raise ParameterCheckError(&#34;model in explainer must be binary classification type&#34;)

    # same checking as in epsilon
    theta = checks.check_epsilon(theta, &#39;theta&#39;)
    cutoff = checks.check_epsilon(cutoff, &#39;cutoff&#39;)

    protected = basic_checks.check_protected(protected, verbose)
    privileged = basic_checks.check_privileged(privileged, protected, verbose)

    exp = copy.deepcopy(explainer)
    probs = exp.y_hat

    if not len(probs) == len(protected):
        raise ParameterCheckError(&#34;protected and target (y) must have the same length&#34;)

    is_close = np.abs(probs - cutoff) &lt; theta
    is_privileged = privileged == protected
    is_favorable = probs &gt; cutoff

    probs[is_close &amp; is_privileged &amp; is_favorable] = cutoff - (probs[is_close &amp; is_privileged &amp; is_favorable] - cutoff)
    probs[is_close &amp; np.logical_not(is_privileged) &amp; np.logical_not(is_favorable)] = cutoff + (cutoff - probs[is_close &amp; np.logical_not(is_privileged) &amp; np.logical_not(is_favorable)])

    probs[probs &lt; 0] = 0
    probs[probs &gt; 1] = 1

    return exp</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dalex.fairness.GroupFairnessClassification"><code class="flex name class">
<span>class <span class="ident">GroupFairnessClassification</span></span>
<span>(</span><span>y, y_hat, protected, privileged, label, verbose=False, cutoff=0.5, epsilon=0.8)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/_group_fairness/object.py#L8-L234" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class GroupFairnessClassification(_FairnessObject):

    def __init__(self, y, y_hat, protected, privileged, label, verbose=False, cutoff=0.5, epsilon=0.8):

        super().__init__(y, y_hat, protected, privileged, verbose)
        checks.check_classification_parameters(y, y_hat, protected, privileged, verbose)
        cutoff = checks.check_cutoff(self.protected, cutoff, verbose)
        self.cutoff = cutoff
        epsilon = checks.check_epsilon(epsilon)
        self.epsilon = epsilon

        sub_confusion_matrix = utils.SubgroupConfusionMatrix(
            y_true=self.y,
            y_pred=self.y_hat,
            protected=self.protected,
            cutoff=self.cutoff
        )

        sub_confusion_matrix_metrics = utils.SubgroupConfusionMatrixMetrics(sub_confusion_matrix)
        df_ratios = utils.calculate_ratio(sub_confusion_matrix_metrics, self.privileged)
        parity_loss = utils.calculate_parity_loss(sub_confusion_matrix_metrics, self.privileged)

        self._subgroup_confusion_matrix = sub_confusion_matrix
        self._subgroup_confusion_matrix_metrics_object = sub_confusion_matrix_metrics
        self.metric_scores = sub_confusion_matrix_metrics.to_horizontal_DataFrame()
        self.parity_loss = parity_loss
        self.result = df_ratios
        self.label = label

    def fairness_check(self, epsilon=None, verbose=True):
        &#34;&#34;&#34;Check if classifier passes various fairness metrics

        Fairness check is an easy way to check if the model is fair.
        For that, this method uses 5 popular metrics of group fairness.
        Model is considered to be fair if confusion matrix metrics are
        close to each other. This arbitrary decision is based on epsilon,
        which defaults to `0.8` (it matches the four-fifths 80% rule).

        Methods in use: Equal opportunity, Predictive parity, Predictive equality,
        Statistical parity and Accuracy equality.

        Parameters
        -----------
        epsilon : float, optional
            Parameter defines acceptable fairness scores. The closer to `1` the
            more strict the verdict is. If the ratio of certain unprivileged
            and privileged subgroup is within the `(epsilon, 1/epsilon)` range,
            then there is no discrimination in this metric and for this subgroups
            (default is `0.8`, which is set during object initialization).
        verbose : bool
            Shows verbose text about potential problems 
            (e.g. `NaN` in model metrics that can cause misinterpretation).

        Returns
        -----------
        None (prints console output)

        &#34;&#34;&#34;

        utils.universal_fairness_check(self,
                                       epsilon,
                                       verbose,
                                       num_for_not_fair=2,
                                       num_for_no_decision=1,
                                       metrics=utils.fairness_check_metrics())

    def plot(self,
             objects=None,
             type=&#39;fairness_check&#39;,
             title=None,
             show=True,
             **kwargs):
        &#34;&#34;&#34;
        Parameters
        -----------
        objects : array_like of GroupFairnessClassification objects
            Additional objects to plot (default is `None`).
        type : str, optional
            Type of the plot. Default is `&#39;fairness_check&#39;`.
            When the type of plot is specified, user may provide additional
            keyword arguments (`**kwargs`) which will be used in creating
            plot of certain type. Below there is list of types:

            - fairness_check:
                fairness_check plot visualizes the fairness_check method
                for one or more GroupFairnessClassification objects.
                It accepts following keyword arguments:
                 &#39;epsilon&#39; - which denotes the decision
                             boundary (like in `fairness_check` method)
            - metric_scores:
                metric_scores plot shows real values of metrics.
                Each model displays values in each metric and each subgroup.
                Vertical lines show metric score for privileged
                subgroup and points connected with the lines
                show scores for unprivileged subgroups.
                This plot is simple and it does
                not have additional keyword arguments.
            - stacked:
                stacked plot shows cumulated parity loss from chosen
                metrics. It stacks metrics on top of each other.
                It accepts following keyword arguments:
                &#39;metrics&#39; - list of metrics to be plotted. The metrics are taken
                            from parity_loss attribute of the object.
                            Default is `[&#34;TPR&#34;, &#34;ACC&#34;, &#34;PPV&#34;, &#34;FPR&#34;, &#34;STP&#34;]`.
            - radar:
                radar plot shows parity loss of provided metrics. It does it
                in form of radar (spider) chart. The smaller the field of
                figure the better.
                It accepts following keyword arguments:
                &#39;metrics&#39; - list of metrics to be plotted. The metrics are taken
                            from parity_loss attribute of the object.
                            Default is `[&#34;TPR&#34;, &#34;ACC&#34;, &#34;PPV&#34;, &#34;FPR&#34;, &#34;STP&#34;]`.
            - performance_and_fairness:
                performance_and_fairness plot shows relation between chosen
                performance and fairness metrics. The fairness metric axis is
                reversed, because the higher the model the less bias it has.
                Thanks to that it is more intuitive to look at because
                the best models are in top right corner.
                It accepts following keyword arguments:
                &#39;fairness_metric&#39; - single fairness metric to be plotted on Y axis.
                                   The metric is taken from parity_loss attribute\
                                   of the object. The default is &#34;TPR&#34;
                &#39;performance_metric&#39; - single performance metric. One of `{&#39;recall&#39;,
                                       &#39;precision&#39;,&#39;accuracy&#39;,&#39;auc&#39;,&#39;f1&#39;}`.
                                       Metrics apart from &#39;auc&#39; are
                                       cutoff-sensitive. Default is &#34;accuracy&#34;
            - heatmap:
                heatmap shows parity loss of metrics in form of heatmap. The less
                parity loss model has, the more fair it is.
                It accepts following keyword arguments:
                &#39;metrics&#39; - list of metrics to be plotted. The metrics are taken
                            from parity_loss attribute of the object.
                            Default is &#39;all&#39; which stands for all available metrics.
            - ceteris_paribus_cutoff:
                ceteris_paribus_cutoff plot shows what would happen if cutoff
                for only one subgroup would change with others cutoffs constant.
                The plot shows also a minimum, where sum of parity loss of metrics
                is the lowest. Minimum only works if at some interval all metrics
                have non-nan scores.
                It accepts following keyword arguments:
                &#39;subgroup&#39; - necessary argument. It is name of subgroup from
                             protected attribute. Cutoff for this subgroup will
                             be changed.

                &#39;metrics&#39; - list of metrics to be plotted. The metrics are taken
                            from parity_loss attribute of the object.
                            Default is `[&#34;TPR&#34;, &#34;ACC&#34;, &#34;PPV&#34;, &#34;FPR&#34;, &#34;STP&#34;]`.

                &#39;grid_points&#39; - number of grid points (cutoff values) to calculate
                                metrics for. The points are distributed evenly.
                                Default is `101`.

        title : str, optional
            Title of the plot (default depends on the `type` attribute).
        show : bool, optional
            `True` shows the plot; `False` returns the plotly Figure object that can
            be edited or saved using the `write_image()` method (default is `True`).

        Returns
        -----------
        None or plotly.graph_objects.Figure
            Return figure that can be edited or saved. See `show` parameter.
        &#34;&#34;&#34;
        other_objects = None
        if objects is not None:
            other_objects = []
            if not isinstance(objects, (list, tuple)):
                objects = [objects]
            for obj in objects:
                _global_checks.global_check_object_class(obj, self.__class__)
                other_objects.append(obj)
            basic_checks.check_other_fairness_objects(self, other_objects)

        if type == &#39;fairness_check&#39;:
            fig = plot.plot_fairness_check_clf(self,
                                               other_objects=other_objects,
                                               title=title, **kwargs)

        elif type == &#34;metric_scores&#34;:
            fig = plot.plot_metric_scores(self,
                                          other_objects=other_objects,
                                          title=title,
                                          **kwargs)

        # names of plots may be changed
        elif type == &#39;stacked&#39;:
            fig = plot.plot_stacked(self,
                                    other_objects=other_objects,
                                    title=title,
                                    **kwargs)

        elif type == &#39;radar&#39;:
            fig = plot.plot_radar(self,
                                  other_objects=other_objects,
                                  title=title,
                                  **kwargs)

        elif type == &#39;performance_and_fairness&#39;:
            fig = plot.plot_performance_and_fairness(self,
                                                     other_objects=other_objects,
                                                     title=title,
                                                     **kwargs)

        elif type == &#39;heatmap&#39;:
            fig = plot.plot_heatmap(self,
                                    other_objects=other_objects,
                                    title=title,
                                    **kwargs)
        elif type == &#39;density&#39;:
            fig = plot.plot_density(self,
                                    other_objects=other_objects,
                                    title=title,
                                    **kwargs)

        elif type == &#39;ceteris_paribus_cutoff&#39;:
            fig = plot.plot_ceteris_paribus_cutoff(self,
                                                   other_objects=other_objects,
                                                   title=title,
                                                   **kwargs)

        else:
            raise ParameterCheckError(f&#34;plot type {type} not supported, try other types.&#34;)

        if show:
            fig.show(config=_theme.get_default_config())
        else:
            return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>dalex.fairness._basics._base_objects._FairnessObject</li>
<li>dalex.fairness._basics._base_objects._AbsObject</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dalex.fairness.GroupFairnessClassification.fairness_check"><code class="name flex">
<span>def <span class="ident">fairness_check</span></span>(<span>self, epsilon=None, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Check if classifier passes various fairness metrics</p>
<p>Fairness check is an easy way to check if the model is fair.
For that, this method uses 5 popular metrics of group fairness.
Model is considered to be fair if confusion matrix metrics are
close to each other. This arbitrary decision is based on epsilon,
which defaults to <code>0.8</code> (it matches the four-fifths 80% rule).</p>
<p>Methods in use: Equal opportunity, Predictive parity, Predictive equality,
Statistical parity and Accuracy equality.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Parameter defines acceptable fairness scores. The closer to <code>1</code> the
more strict the verdict is. If the ratio of certain unprivileged
and privileged subgroup is within the <code>(epsilon, 1/epsilon)</code> range,
then there is no discrimination in this metric and for this subgroups
(default is <code>0.8</code>, which is set during object initialization).</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Shows verbose text about potential problems
(e.g. <code>NaN</code> in model metrics that can cause misinterpretation).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None (prints console output)</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/_group_fairness/object.py#L37-L72" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def fairness_check(self, epsilon=None, verbose=True):
    &#34;&#34;&#34;Check if classifier passes various fairness metrics

    Fairness check is an easy way to check if the model is fair.
    For that, this method uses 5 popular metrics of group fairness.
    Model is considered to be fair if confusion matrix metrics are
    close to each other. This arbitrary decision is based on epsilon,
    which defaults to `0.8` (it matches the four-fifths 80% rule).

    Methods in use: Equal opportunity, Predictive parity, Predictive equality,
    Statistical parity and Accuracy equality.

    Parameters
    -----------
    epsilon : float, optional
        Parameter defines acceptable fairness scores. The closer to `1` the
        more strict the verdict is. If the ratio of certain unprivileged
        and privileged subgroup is within the `(epsilon, 1/epsilon)` range,
        then there is no discrimination in this metric and for this subgroups
        (default is `0.8`, which is set during object initialization).
    verbose : bool
        Shows verbose text about potential problems 
        (e.g. `NaN` in model metrics that can cause misinterpretation).

    Returns
    -----------
    None (prints console output)

    &#34;&#34;&#34;

    utils.universal_fairness_check(self,
                                   epsilon,
                                   verbose,
                                   num_for_not_fair=2,
                                   num_for_no_decision=1,
                                   metrics=utils.fairness_check_metrics())</code></pre>
</details>
</dd>
<dt id="dalex.fairness.GroupFairnessClassification.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, objects=None, type='fairness_check', title=None, show=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>objects</code></strong> :&ensp;<code>array_like</code> of <code><a title="dalex.fairness.GroupFairnessClassification" href="#dalex.fairness.GroupFairnessClassification">GroupFairnessClassification</a> objects</code></dt>
<dd>Additional objects to plot (default is <code>None</code>).</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>
<p>Type of the plot. Default is <code>'fairness_check'</code>.
When the type of plot is specified, user may provide additional
keyword arguments (<code>**kwargs</code>) which will be used in creating
plot of certain type. Below there is list of types:</p>
<ul>
<li>fairness_check:
fairness_check plot visualizes the fairness_check method
for one or more GroupFairnessClassification objects.
It accepts following keyword arguments:
'epsilon' - which denotes the decision
boundary (like in <code>fairness_check</code> method)</li>
<li>metric_scores:
metric_scores plot shows real values of metrics.
Each model displays values in each metric and each subgroup.
Vertical lines show metric score for privileged
subgroup and points connected with the lines
show scores for unprivileged subgroups.
This plot is simple and it does
not have additional keyword arguments.</li>
<li>stacked:
stacked plot shows cumulated parity loss from chosen
metrics. It stacks metrics on top of each other.
It accepts following keyword arguments:
'metrics' - list of metrics to be plotted. The metrics are taken
from parity_loss attribute of the object.
Default is <code>["TPR", "ACC", "PPV", "FPR", "STP"]</code>.</li>
<li>radar:
radar plot shows parity loss of provided metrics. It does it
in form of radar (spider) chart. The smaller the field of
figure the better.
It accepts following keyword arguments:
'metrics' - list of metrics to be plotted. The metrics are taken
from parity_loss attribute of the object.
Default is <code>["TPR", "ACC", "PPV", "FPR", "STP"]</code>.</li>
<li>performance_and_fairness:
performance_and_fairness plot shows relation between chosen
performance and fairness metrics. The fairness metric axis is
reversed, because the higher the model the less bias it has.
Thanks to that it is more intuitive to look at because
the best models are in top right corner.
It accepts following keyword arguments:
'fairness_metric' - single fairness metric to be plotted on Y axis.
The metric is taken from parity_loss attribute
of the object. The default is "TPR"
'performance_metric' - single performance metric. One of <code>{'recall',
'precision','accuracy','auc','f1'}</code>.
Metrics apart from 'auc' are
cutoff-sensitive. Default is "accuracy"</li>
<li>heatmap:
heatmap shows parity loss of metrics in form of heatmap. The less
parity loss model has, the more fair it is.
It accepts following keyword arguments:
'metrics' - list of metrics to be plotted. The metrics are taken
from parity_loss attribute of the object.
Default is 'all' which stands for all available metrics.</li>
<li>
<p>ceteris_paribus_cutoff:
ceteris_paribus_cutoff plot shows what would happen if cutoff
for only one subgroup would change with others cutoffs constant.
The plot shows also a minimum, where sum of parity loss of metrics
is the lowest. Minimum only works if at some interval all metrics
have non-nan scores.
It accepts following keyword arguments:
'subgroup' - necessary argument. It is name of subgroup from
protected attribute. Cutoff for this subgroup will
be changed.</p>
<p>'metrics' - list of metrics to be plotted. The metrics are taken
from parity_loss attribute of the object.
Default is <code>["TPR", "ACC", "PPV", "FPR", "STP"]</code>.</p>
<p>'grid_points' - number of grid points (cutoff values) to calculate
metrics for. The points are distributed evenly.
Default is <code>101</code>.</p>
</li>
</ul>
</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Title of the plot (default depends on the <code>type</code> attribute).</dd>
<dt><strong><code>show</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd><code>True</code> shows the plot; <code>False</code> returns the plotly Figure object that can
be edited or saved using the <code>write_image()</code> method (default is <code>True</code>).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code> or <code>plotly.graph_objects.Figure</code></dt>
<dd>Return figure that can be edited or saved. See <code>show</code> parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/_group_fairness/object.py#L74-L234" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot(self,
         objects=None,
         type=&#39;fairness_check&#39;,
         title=None,
         show=True,
         **kwargs):
    &#34;&#34;&#34;
    Parameters
    -----------
    objects : array_like of GroupFairnessClassification objects
        Additional objects to plot (default is `None`).
    type : str, optional
        Type of the plot. Default is `&#39;fairness_check&#39;`.
        When the type of plot is specified, user may provide additional
        keyword arguments (`**kwargs`) which will be used in creating
        plot of certain type. Below there is list of types:

        - fairness_check:
            fairness_check plot visualizes the fairness_check method
            for one or more GroupFairnessClassification objects.
            It accepts following keyword arguments:
             &#39;epsilon&#39; - which denotes the decision
                         boundary (like in `fairness_check` method)
        - metric_scores:
            metric_scores plot shows real values of metrics.
            Each model displays values in each metric and each subgroup.
            Vertical lines show metric score for privileged
            subgroup and points connected with the lines
            show scores for unprivileged subgroups.
            This plot is simple and it does
            not have additional keyword arguments.
        - stacked:
            stacked plot shows cumulated parity loss from chosen
            metrics. It stacks metrics on top of each other.
            It accepts following keyword arguments:
            &#39;metrics&#39; - list of metrics to be plotted. The metrics are taken
                        from parity_loss attribute of the object.
                        Default is `[&#34;TPR&#34;, &#34;ACC&#34;, &#34;PPV&#34;, &#34;FPR&#34;, &#34;STP&#34;]`.
        - radar:
            radar plot shows parity loss of provided metrics. It does it
            in form of radar (spider) chart. The smaller the field of
            figure the better.
            It accepts following keyword arguments:
            &#39;metrics&#39; - list of metrics to be plotted. The metrics are taken
                        from parity_loss attribute of the object.
                        Default is `[&#34;TPR&#34;, &#34;ACC&#34;, &#34;PPV&#34;, &#34;FPR&#34;, &#34;STP&#34;]`.
        - performance_and_fairness:
            performance_and_fairness plot shows relation between chosen
            performance and fairness metrics. The fairness metric axis is
            reversed, because the higher the model the less bias it has.
            Thanks to that it is more intuitive to look at because
            the best models are in top right corner.
            It accepts following keyword arguments:
            &#39;fairness_metric&#39; - single fairness metric to be plotted on Y axis.
                               The metric is taken from parity_loss attribute\
                               of the object. The default is &#34;TPR&#34;
            &#39;performance_metric&#39; - single performance metric. One of `{&#39;recall&#39;,
                                   &#39;precision&#39;,&#39;accuracy&#39;,&#39;auc&#39;,&#39;f1&#39;}`.
                                   Metrics apart from &#39;auc&#39; are
                                   cutoff-sensitive. Default is &#34;accuracy&#34;
        - heatmap:
            heatmap shows parity loss of metrics in form of heatmap. The less
            parity loss model has, the more fair it is.
            It accepts following keyword arguments:
            &#39;metrics&#39; - list of metrics to be plotted. The metrics are taken
                        from parity_loss attribute of the object.
                        Default is &#39;all&#39; which stands for all available metrics.
        - ceteris_paribus_cutoff:
            ceteris_paribus_cutoff plot shows what would happen if cutoff
            for only one subgroup would change with others cutoffs constant.
            The plot shows also a minimum, where sum of parity loss of metrics
            is the lowest. Minimum only works if at some interval all metrics
            have non-nan scores.
            It accepts following keyword arguments:
            &#39;subgroup&#39; - necessary argument. It is name of subgroup from
                         protected attribute. Cutoff for this subgroup will
                         be changed.

            &#39;metrics&#39; - list of metrics to be plotted. The metrics are taken
                        from parity_loss attribute of the object.
                        Default is `[&#34;TPR&#34;, &#34;ACC&#34;, &#34;PPV&#34;, &#34;FPR&#34;, &#34;STP&#34;]`.

            &#39;grid_points&#39; - number of grid points (cutoff values) to calculate
                            metrics for. The points are distributed evenly.
                            Default is `101`.

    title : str, optional
        Title of the plot (default depends on the `type` attribute).
    show : bool, optional
        `True` shows the plot; `False` returns the plotly Figure object that can
        be edited or saved using the `write_image()` method (default is `True`).

    Returns
    -----------
    None or plotly.graph_objects.Figure
        Return figure that can be edited or saved. See `show` parameter.
    &#34;&#34;&#34;
    other_objects = None
    if objects is not None:
        other_objects = []
        if not isinstance(objects, (list, tuple)):
            objects = [objects]
        for obj in objects:
            _global_checks.global_check_object_class(obj, self.__class__)
            other_objects.append(obj)
        basic_checks.check_other_fairness_objects(self, other_objects)

    if type == &#39;fairness_check&#39;:
        fig = plot.plot_fairness_check_clf(self,
                                           other_objects=other_objects,
                                           title=title, **kwargs)

    elif type == &#34;metric_scores&#34;:
        fig = plot.plot_metric_scores(self,
                                      other_objects=other_objects,
                                      title=title,
                                      **kwargs)

    # names of plots may be changed
    elif type == &#39;stacked&#39;:
        fig = plot.plot_stacked(self,
                                other_objects=other_objects,
                                title=title,
                                **kwargs)

    elif type == &#39;radar&#39;:
        fig = plot.plot_radar(self,
                              other_objects=other_objects,
                              title=title,
                              **kwargs)

    elif type == &#39;performance_and_fairness&#39;:
        fig = plot.plot_performance_and_fairness(self,
                                                 other_objects=other_objects,
                                                 title=title,
                                                 **kwargs)

    elif type == &#39;heatmap&#39;:
        fig = plot.plot_heatmap(self,
                                other_objects=other_objects,
                                title=title,
                                **kwargs)
    elif type == &#39;density&#39;:
        fig = plot.plot_density(self,
                                other_objects=other_objects,
                                title=title,
                                **kwargs)

    elif type == &#39;ceteris_paribus_cutoff&#39;:
        fig = plot.plot_ceteris_paribus_cutoff(self,
                                               other_objects=other_objects,
                                               title=title,
                                               **kwargs)

    else:
        raise ParameterCheckError(f&#34;plot type {type} not supported, try other types.&#34;)

    if show:
        fig.show(config=_theme.get_default_config())
    else:
        return fig</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dalex.fairness.GroupFairnessRegression"><code class="flex name class">
<span>class <span class="ident">GroupFairnessRegression</span></span>
<span>(</span><span>y, y_hat, protected, privileged, label, epsilon=0.8, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/_group_fairness/object.py#L237-L344" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class GroupFairnessRegression(_FairnessObject):

    def __init__(self, y, y_hat, protected, privileged, label, epsilon=0.8, verbose=False):

        super().__init__(y, y_hat, protected, privileged, verbose)
        checks.check_epsilon(epsilon)

        df_ratios = utils.calculate_regression_measures(y, y_hat, protected, privileged)

        self.result = df_ratios
        self.label = label
        self.epsilon = epsilon

    def fairness_check(self, epsilon=None, verbose=True):
        &#34;&#34;&#34;Check if classifier passes various fairness criteria

        Fairness check is an easy way to check if the model is fair.
        For that, this method uses 3 non-discrimination criteria.
        The approximations are made to check the conditional independence expressed
        in form of independence, separation and sufficiency.
        Model is considered to be fair if all criteria are met.
        This arbitrary decision is based on epsilon,
        which defaults to `0.8` (it matches the four-fifths 80% rule).

        Methods in use: Independence, Separation, Sufficiency.

        Parameters
        -----------
        epsilon : float, optional
            Parameter defines acceptable fairness scores. The closer to `1` the
            more strict the verdict is. If the ratio of certain unprivileged
            and privileged subgroup is within the `(epsilon, 1/epsilon)` range,
            then there is no discrimination in this metric and for this subgroups
            (default is `0.8`, which is set during object initialization).
        verbose : bool
            Shows verbose text about potential problems
            (e.g. `NaN` in model metrics that can cause misinterpretation).

        Returns
        -----------
        None (prints console output)

        &#34;&#34;&#34;

        utils.universal_fairness_check(self,
                                       epsilon,
                                       verbose,
                                       num_for_not_fair=1,
                                       num_for_no_decision=None,
                                       metrics=[&#39;independence&#39;, &#39;separation&#39;, &#39;sufficiency&#39;])

    def plot(self, objects=None, type=&#39;fairness_check&#39;, title=None, show=True, **kwargs):
        &#34;&#34;&#34;
        Parameters
        -----------
        objects : array_like of GroupFairnessRegression objects
            Additional objects to plot (default is `None`).
        type : str, optional
            Type of the plot. Default is `&#39;fairness_check&#39;`.
            When the type of plot is specified, user may provide additional
            keyword arguments (`**kwargs`) which will be used in creating
            plot of certain type. Below there is list of types:

            - fairness_check:
                fairness_check plot visualizes the fairness_check method
                for one or more GroupFairnessClassification objects.
                It accepts following keyword arguments:
                 &#39;epsilon&#39; - which denotes the decision
                             boundary (like in `fairness_check` method)

            - density:
                density plot visualizes the output of the model for each
                subgroup in form of violin plots with boxplots on top of them.
                It does not accept additional keyword arguments.
        title : str, optional
            Title of the plot (default depends on the `type` attribute).

        &#34;&#34;&#34;

        other_objects = None
        if objects is not None:
            other_objects = []
            if not isinstance(objects, (list, tuple)):
                objects = [objects]
            for obj in objects:
                _global_checks.global_check_object_class(obj, self.__class__)
                other_objects.append(obj)
            basic_checks.check_other_fairness_objects(self, other_objects)

        if type == &#39;density&#39;:
            fig = plot.plot_density(self,
                                    other_objects,
                                    title=title,
                                    **kwargs)

        elif type == &#39;fairness_check&#39;:
            fig = plot.plot_fairness_check_reg(self,
                                               other_objects=other_objects,
                                               title=title,
                                               **kwargs)

        else:
            raise ParameterCheckError(f&#34;plot type {type} not supported, try other types.&#34;)

        if show:
            fig.show(config=_theme.get_default_config())
        else:
            return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>dalex.fairness._basics._base_objects._FairnessObject</li>
<li>dalex.fairness._basics._base_objects._AbsObject</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dalex.fairness.GroupFairnessRegression.fairness_check"><code class="name flex">
<span>def <span class="ident">fairness_check</span></span>(<span>self, epsilon=None, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Check if classifier passes various fairness criteria</p>
<p>Fairness check is an easy way to check if the model is fair.
For that, this method uses 3 non-discrimination criteria.
The approximations are made to check the conditional independence expressed
in form of independence, separation and sufficiency.
Model is considered to be fair if all criteria are met.
This arbitrary decision is based on epsilon,
which defaults to <code>0.8</code> (it matches the four-fifths 80% rule).</p>
<p>Methods in use: Independence, Separation, Sufficiency.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Parameter defines acceptable fairness scores. The closer to <code>1</code> the
more strict the verdict is. If the ratio of certain unprivileged
and privileged subgroup is within the <code>(epsilon, 1/epsilon)</code> range,
then there is no discrimination in this metric and for this subgroups
(default is <code>0.8</code>, which is set during object initialization).</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Shows verbose text about potential problems
(e.g. <code>NaN</code> in model metrics that can cause misinterpretation).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None (prints console output)</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/_group_fairness/object.py#L250-L286" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def fairness_check(self, epsilon=None, verbose=True):
    &#34;&#34;&#34;Check if classifier passes various fairness criteria

    Fairness check is an easy way to check if the model is fair.
    For that, this method uses 3 non-discrimination criteria.
    The approximations are made to check the conditional independence expressed
    in form of independence, separation and sufficiency.
    Model is considered to be fair if all criteria are met.
    This arbitrary decision is based on epsilon,
    which defaults to `0.8` (it matches the four-fifths 80% rule).

    Methods in use: Independence, Separation, Sufficiency.

    Parameters
    -----------
    epsilon : float, optional
        Parameter defines acceptable fairness scores. The closer to `1` the
        more strict the verdict is. If the ratio of certain unprivileged
        and privileged subgroup is within the `(epsilon, 1/epsilon)` range,
        then there is no discrimination in this metric and for this subgroups
        (default is `0.8`, which is set during object initialization).
    verbose : bool
        Shows verbose text about potential problems
        (e.g. `NaN` in model metrics that can cause misinterpretation).

    Returns
    -----------
    None (prints console output)

    &#34;&#34;&#34;

    utils.universal_fairness_check(self,
                                   epsilon,
                                   verbose,
                                   num_for_not_fair=1,
                                   num_for_no_decision=None,
                                   metrics=[&#39;independence&#39;, &#39;separation&#39;, &#39;sufficiency&#39;])</code></pre>
</details>
</dd>
<dt id="dalex.fairness.GroupFairnessRegression.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, objects=None, type='fairness_check', title=None, show=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>objects</code></strong> :&ensp;<code>array_like</code> of <code><a title="dalex.fairness.GroupFairnessRegression" href="#dalex.fairness.GroupFairnessRegression">GroupFairnessRegression</a> objects</code></dt>
<dd>Additional objects to plot (default is <code>None</code>).</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>
<p>Type of the plot. Default is <code>'fairness_check'</code>.
When the type of plot is specified, user may provide additional
keyword arguments (<code>**kwargs</code>) which will be used in creating
plot of certain type. Below there is list of types:</p>
<ul>
<li>
<p>fairness_check:
fairness_check plot visualizes the fairness_check method
for one or more GroupFairnessClassification objects.
It accepts following keyword arguments:
'epsilon' - which denotes the decision
boundary (like in <code>fairness_check</code> method)</p>
</li>
<li>
<p>density:
density plot visualizes the output of the model for each
subgroup in form of violin plots with boxplots on top of them.
It does not accept additional keyword arguments.</p>
</li>
</ul>
</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Title of the plot (default depends on the <code>type</code> attribute).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/fairness/_group_fairness/object.py#L288-L344" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot(self, objects=None, type=&#39;fairness_check&#39;, title=None, show=True, **kwargs):
    &#34;&#34;&#34;
    Parameters
    -----------
    objects : array_like of GroupFairnessRegression objects
        Additional objects to plot (default is `None`).
    type : str, optional
        Type of the plot. Default is `&#39;fairness_check&#39;`.
        When the type of plot is specified, user may provide additional
        keyword arguments (`**kwargs`) which will be used in creating
        plot of certain type. Below there is list of types:

        - fairness_check:
            fairness_check plot visualizes the fairness_check method
            for one or more GroupFairnessClassification objects.
            It accepts following keyword arguments:
             &#39;epsilon&#39; - which denotes the decision
                         boundary (like in `fairness_check` method)

        - density:
            density plot visualizes the output of the model for each
            subgroup in form of violin plots with boxplots on top of them.
            It does not accept additional keyword arguments.
    title : str, optional
        Title of the plot (default depends on the `type` attribute).

    &#34;&#34;&#34;

    other_objects = None
    if objects is not None:
        other_objects = []
        if not isinstance(objects, (list, tuple)):
            objects = [objects]
        for obj in objects:
            _global_checks.global_check_object_class(obj, self.__class__)
            other_objects.append(obj)
        basic_checks.check_other_fairness_objects(self, other_objects)

    if type == &#39;density&#39;:
        fig = plot.plot_density(self,
                                other_objects,
                                title=title,
                                **kwargs)

    elif type == &#39;fairness_check&#39;:
        fig = plot.plot_fairness_check_reg(self,
                                           other_objects=other_objects,
                                           title=title,
                                           **kwargs)

    else:
        raise ParameterCheckError(f&#34;plot type {type} not supported, try other types.&#34;)

    if show:
        fig.show(config=_theme.get_default_config())
    else:
        return fig</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="dalex Home" href="https://dalex.drwhy.ai/">
<img src="https://raw.githubusercontent.com/ModelOriented/DALEX-docs/master/docs/misc/dalex_even.png" alt=""> dalex
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dalex" href="../index.html">dalex</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dalex.fairness.resample" href="#dalex.fairness.resample">resample</a></code></li>
<li><code><a title="dalex.fairness.reweight" href="#dalex.fairness.reweight">reweight</a></code></li>
<li><code><a title="dalex.fairness.roc_pivot" href="#dalex.fairness.roc_pivot">roc_pivot</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dalex.fairness.GroupFairnessClassification" href="#dalex.fairness.GroupFairnessClassification">GroupFairnessClassification</a></code></h4>
<ul class="">
<li><code><a title="dalex.fairness.GroupFairnessClassification.fairness_check" href="#dalex.fairness.GroupFairnessClassification.fairness_check">fairness_check</a></code></li>
<li><code><a title="dalex.fairness.GroupFairnessClassification.plot" href="#dalex.fairness.GroupFairnessClassification.plot">plot</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dalex.fairness.GroupFairnessRegression" href="#dalex.fairness.GroupFairnessRegression">GroupFairnessRegression</a></code></h4>
<ul class="">
<li><code><a title="dalex.fairness.GroupFairnessRegression.fairness_check" href="#dalex.fairness.GroupFairnessRegression.fairness_check">fairness_check</a></code></li>
<li><code><a title="dalex.fairness.GroupFairnessRegression.plot" href="#dalex.fairness.GroupFairnessRegression.plot">plot</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>