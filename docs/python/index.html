<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>dalex API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>dalex</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># dx.Explainer
# from dalex.datasets import load_*
from . import datasets
from ._explainer.object import Explainer

__version__ = &#39;0.3.0.9000&#39;

__all__ = [
    &#34;Explainer&#34;,
    &#34;dataset_level&#34;,
    &#34;instance_level&#34;,
    &#34;fairness&#34;,
    &#34;datasets&#34;
]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="dalex.dataset_level" href="dataset_level/index.html">dalex.dataset_level</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="dalex.datasets" href="datasets/index.html">dalex.datasets</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="dalex.fairness" href="fairness/index.html">dalex.fairness</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="dalex.instance_level" href="instance_level/index.html">dalex.instance_level</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="dalex.wrappers" href="wrappers/index.html">dalex.wrappers</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dalex.Explainer"><code class="flex name class">
<span>class <span class="ident">Explainer</span></span>
<span>(</span><span>model, data=None, y=None, predict_function=None, residual_function=None, weights=None, label=None, model_class=None, verbose=True, precalculate=True, model_type=None, model_info=None, colorize=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Create Model Explainer</p>
<p>Black-box models may have very different structures. This class creates a unified
representation of a model, which can be further processed by various explanations.
Methods of this class produce explanation objects, that contain the main result
attribute, and can be visualised using the plot method.</p>
<p>The <code>model</code> is the only required parameter, but most of the explanations require
that other parameters are provided (See <code>data</code>, <code>y</code>, <code>predict_function</code>, <code>model_type</code>).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>object</code></dt>
<dd>Model to be explained.</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.ndarray (2d)</code></dt>
<dd>Data which will be used to calculate the explanations. It shouldn't contain
the target column (See <code>y</code>).
NOTE: If target variable is present in the data, some of the functionalities may
not work properly.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code></dt>
<dd>Target variable with outputs / scores. It shall have the same length as <code>data</code>.</dd>
<dt><strong><code>predict_function</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>Function that takes two parameters (model, data) and returns a np.ndarray (1d)
with model predictions (default is predict method extracted from the model).
NOTE: This function needs to work with <code>data</code> as pd.DataFrame.</dd>
<dt><strong><code>residual_function</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>Function that takes three parameters (model, data, y) and returns a np.ndarray (1d)
with model residuals (default is a function constructed from <code>predict_function</code>).</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code>, optional</dt>
<dd>Sampling weights for observations in <code>data</code>. It shall have the same length as
<code>data</code> (default is None).</dd>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Model name to appear in result and plots
(default is last element of the class attribute extracted from the model).</dd>
<dt><strong><code>model_class</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Class of the model that is used e.g. to choose the <code>predict_function</code>
(default is the class attribute extracted from the model).
NOTE: Use if your model is wrapped with Pipeline.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Print diagnostic messages during the Explainer initialization (default is True).</dd>
<dt><strong><code>precalculate</code></strong> :&ensp;<code>bool</code></dt>
<dd>Calculate y_hat (predicted values) and residuals during the Explainer
initialization (default is True).</dd>
<dt><strong><code>model_type</code></strong> :&ensp;<code>{'regression', 'classification', None}</code></dt>
<dd>Model task type that is used e.g. in <code>model_performance</code> and <code>model_parts</code>
(default is try to extract the information from the model, else None).</dd>
<dt><strong><code>model_info</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Dict {'model_package', 'model_package_version', &hellip;} containing additional
information to be stored.</dd>
<dt><strong><code>colorize</code></strong> :&ensp;<code>TODO</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>object</code></dt>
<dd>A model to be explained.</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Data which will be used to calculate the explanations.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray (1d)</code></dt>
<dd>Target variable with outputs / scores.</dd>
<dt><strong><code>predict_function</code></strong> :&ensp;<code>function</code></dt>
<dd>Function that takes two arguments (model, data) and returns np.ndarray (1d)
with model predictions.</dd>
<dt><strong><code>y_hat</code></strong> :&ensp;<code>np.ndarray (1d)</code></dt>
<dd>Model predictions for <code>data</code>.</dd>
<dt><strong><code>residual_function</code></strong> :&ensp;<code>function</code></dt>
<dd>Function that takes three arguments (model, data, y) and returns np.ndarray (1d)
with model residuals.</dd>
<dt><strong><code>residuals</code></strong> :&ensp;<code>np.ndarray (1d)</code></dt>
<dd>Model residuals for <code>data</code>.</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>np.ndarray (1d)</code></dt>
<dd>Sampling weights for observations in <code>data</code>.</dd>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code></dt>
<dd>Model name to appear in result and plots.</dd>
<dt><strong><code>model_class</code></strong> :&ensp;<code>str</code></dt>
<dd>Class of the model.</dd>
<dt><strong><code>model_type</code></strong> :&ensp;<code>{'regression', 'classification', None}</code></dt>
<dd>Model task type.</dd>
<dt><strong><code>model_info</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dict {'model_package', 'model_package_version', &hellip;} containing additional
information.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p><a href="https://pbiecek.github.io/ema/dataSetsIntro.html#ExplainersTitanicPythonCode">https://pbiecek.github.io/ema/dataSetsIntro.html#ExplainersTitanicPythonCode</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Explainer:
    &#34;&#34;&#34; Create Model Explainer

    Black-box models may have very different structures. This class creates a unified
    representation of a model, which can be further processed by various explanations.
    Methods of this class produce explanation objects, that contain the main result
    attribute, and can be visualised using the plot method.

    The `model` is the only required parameter, but most of the explanations require
    that other parameters are provided (See `data`, `y`, `predict_function`, `model_type`).

    Parameters
    ----------
    model : object
        Model to be explained.
    data : pd.DataFrame or np.ndarray (2d)
        Data which will be used to calculate the explanations. It shouldn&#39;t contain
        the target column (See `y`).
        NOTE: If target variable is present in the data, some of the functionalities may
        not work properly.
    y : pd.Series or np.ndarray (1d)
        Target variable with outputs / scores. It shall have the same length as `data`.
    predict_function : function, optional
        Function that takes two parameters (model, data) and returns a np.ndarray (1d)
        with model predictions (default is predict method extracted from the model).
        NOTE: This function needs to work with `data` as pd.DataFrame.
    residual_function : function, optional
        Function that takes three parameters (model, data, y) and returns a np.ndarray (1d)
        with model residuals (default is a function constructed from `predict_function`).
    weights : pd.Series or np.ndarray (1d), optional
        Sampling weights for observations in `data`. It shall have the same length as
        `data` (default is None).
    label : str, optional
        Model name to appear in result and plots
        (default is last element of the class attribute extracted from the model).
    model_class : str, optional
        Class of the model that is used e.g. to choose the `predict_function`
        (default is the class attribute extracted from the model).
        NOTE: Use if your model is wrapped with Pipeline.
    verbose : bool
        Print diagnostic messages during the Explainer initialization (default is True).
    precalculate : bool
        Calculate y_hat (predicted values) and residuals during the Explainer
        initialization (default is True).
    model_type : {&#39;regression&#39;, &#39;classification&#39;, None}
        Model task type that is used e.g. in `model_performance` and `model_parts`
        (default is try to extract the information from the model, else None).
    model_info: dict, optional
        Dict {&#39;model_package&#39;, &#39;model_package_version&#39;, ...} containing additional
        information to be stored.
    colorize : TODO

    Attributes
    --------
    model : object
        A model to be explained.
    data : pd.DataFrame
        Data which will be used to calculate the explanations.
    y : np.ndarray (1d)
        Target variable with outputs / scores.
    predict_function : function
        Function that takes two arguments (model, data) and returns np.ndarray (1d)
        with model predictions.
    y_hat : np.ndarray (1d)
        Model predictions for `data`.
    residual_function : function
        Function that takes three arguments (model, data, y) and returns np.ndarray (1d)
        with model residuals.
    residuals : np.ndarray (1d)
        Model residuals for `data`.
    weights : np.ndarray (1d)
        Sampling weights for observations in `data`.
    label : str
        Model name to appear in result and plots.
    model_class : str
        Class of the model.
    model_type : {&#39;regression&#39;, &#39;classification&#39;, None}
        Model task type.
    model_info: dict
        Dict {&#39;model_package&#39;, &#39;model_package_version&#39;, ...} containing additional
        information.

    Notes
    --------
    https://pbiecek.github.io/ema/dataSetsIntro.html#ExplainersTitanicPythonCode

    &#34;&#34;&#34;

    def __init__(self,
                 model,
                 data=None,
                 y=None,
                 predict_function=None,
                 residual_function=None,
                 weights=None,
                 label=None,
                 model_class=None,
                 verbose=True,
                 precalculate=True,
                 model_type=None,
                 model_info=None,
                 colorize=True):

        verbose_cat(&#34;Preparation of a new explainer is initiated\n&#34;, verbose=verbose)

        # if requested, remove colors
        if not colorize:
            color_codes = {
                &#39;yellow_start&#39;: &#34;&#34;,
                &#39;yellow_end&#39;: &#34;&#34;,
                &#39;red_start&#39;: &#34;&#34;,
                &#39;red_end&#39;: &#34;&#34;,
                &#39;green_start&#39;: &#34;&#34;,
                &#39;green_end&#39;: &#34;&#34;}

        # REPORT: checks for data
        data = check_data(data, verbose)

        # REPORT: checks for y
        y = check_y(y, data, verbose)

        # REPORT: checks for weights
        weights = check_weights(weights, data, verbose)

        # REPORT: checks for model_class
        model_class, model_info_ = check_model_class(model_class, model, verbose)

        # REPORT: checks for label
        label, model_info_ = check_label(label, model_class, model_info_, verbose)

        # REPORT: checks for predict_function and model_type
        # these two are together only because of `yhat_exception_dict`
        predict_function, model_type, y_hat_, model_info_ = \
            check_predict_function_and_model_type(predict_function, model_type,
                                                  model, data, model_class, model_info_,
                                                  precalculate, verbose)

        # if data is specified then we may test predict_function
        # at this moment we have predict function

        # REPORT: checks for residual_function
        residual_function, residuals, model_info_ = check_residual_function(residual_function, predict_function,
                                                                            model, data, y,
                                                                            model_info_, precalculate, verbose)

        # REPORT: checks for model_info
        model_info = check_model_info(model_info, model_info_, verbose)

        # READY to create an explainer
        self.model = model
        self.data = data
        self.y = y
        self.predict_function = predict_function
        self.y_hat = y_hat_
        self.residual_function = residual_function
        self.residuals = residuals
        self.model_class = model_class
        self.label = label
        self.model_info = model_info
        self.weights = weights
        self.model_type = model_type

        verbose_cat(&#34;\nA new explainer has been created!&#34;, verbose=verbose)

    def predict(self, data):
        &#34;&#34;&#34;Make a prediction

        This function uses the `predict_function` attribute.

        Parameters
        ----------
        data : pd.DataFrame, np.ndarray 2d
            Data which will be used to make a prediction.

        Returns
        ----------
        np.ndarray (1d)
            Model predictions for given `data`.
        &#34;&#34;&#34;

        check_method_data(data)

        return self.predict_function(self.model, data)

    def residual(self, data, y):
        &#34;&#34;&#34;Calculate residuals

        This function uses the `residual_function` attribute.

        Parameters
        -----------
        data : pd.DataFrame
            Data which will be used to calculate residuals.
        y : pd.Series or np.ndarray (1d)
            Target variable which will be used to calculate residuals.

        Returns
        -----------
        np.ndarray (1d)
            Model residuals for given `data` and `y`.
        &#34;&#34;&#34;

        check_method_data(data)

        return self.residual_function(self.model, data, y)

    def predict_parts(self,
                      new_observation,
                      type=(&#39;break_down_interactions&#39;, &#39;break_down&#39;, &#39;shap&#39;, &#39;shap_wrapper&#39;),
                      order=None,
                      interaction_preference=1,
                      path=&#34;average&#34;,
                      B=25,
                      keep_distributions=False,
                      processes=1,
                      random_state=None,
                      **kwargs):
        &#34;&#34;&#34;Calculate instance level variable attributions as Break Down, Shapley Values or Shap Values

        Parameters
        -----------
        new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
            An observation for which a prediction needs to be explained.
        type : {&#39;break_down_interactions&#39;, &#39;break_down&#39;, &#39;shap&#39;, &#39;shap_wrapper}
            Type of variable attributions (default is &#39;break_down_interactions&#39;).
        order : list of int or str, optional
            Parameter specific for `break_down_interactions` and `break_down`. Use a fixed
            order of variables for attribution calculation. Use integer values  or string
            variable names (default is None, which means order by importance).
        interaction_preference : int, optional
            Parameter specific for `break_down_interactions` type. Specify which interactions
            will be present in an explanation. The larger the integer, the more frequently
            interactions will be presented (default is 1).
        path : list of int, optional
            Parameter specific for `shap`. If specified, then attributions for this path
            will be plotted (default is &#39;average&#39;, which plots attribution means for
            `B` random paths).
        B : int, optional
            Parameter specific for `shap`. Number of random paths to calculate
            variable attributions (default is 25).
        keep_distributions :  bool, optional
            Save the distribution of partial predictions (default is False).
        processes : int, optional
            Parameter specific for `shap`. Number of parallel processes to use in calculations.
            Iterated over `B` (default is 1, which means no parallel computation).
        random_state : int, optional
            Set seed for random number generator (default is random seed).
        kwargs :
            Used only for &#39;shap_wrapper&#39;. Pass `shap_explainer_type` to specify, which
            Explainer shall be used: {&#39;TreeExplainer&#39;, &#39;DeepExplainer&#39;, &#39;GradientExplainer&#39;,
            &#39;LinearExplainer&#39;, &#39;KernelExplainer&#39;} (default is None, which automatically
            chooses an Explainer to use).
            Also keyword arguments passed to one of the: shap.TreeExplainer.shap_values,
            shap.DeepExplainer.shap_values, shap.GradientExplainer.shap_values,
            shap.LinearExplainer.shap_values, shap.KernelExplainer.shap_values.
            See https://github.com/slundberg/shap

        Returns
        -----------
        BreakDown, Shap or ShapWrapper class object
            Explanation object containing the main result attribute and the plot method.
            Object class, its attributes, and the plot method depend on the `type` parameter.

        Notes
        --------
        https://pbiecek.github.io/ema/breakDown.html
        https://pbiecek.github.io/ema/iBreakDown.html
        https://pbiecek.github.io/ema/shapley.html
        https://github.com/slundberg/shap
        &#34;&#34;&#34;

        check_data_again(self.data)

        types = (&#39;break_down_interactions&#39;, &#39;break_down&#39;, &#39;shap&#39;, &#39;shap_wrapper&#39;)
        type = check_method_type(type, types)

        if type == &#39;break_down_interactions&#39; or type == &#39;break_down&#39;:
            predict_parts_ = BreakDown(
                type=type,
                keep_distributions=keep_distributions,
                order=order,
                interaction_preference=interaction_preference
            )
        elif type == &#39;shap&#39;:
            predict_parts_ = Shap(
                keep_distributions=keep_distributions,
                path=path,
                B=B,
                processes=processes,
                random_state=random_state
            )
        elif type == &#39;shap_wrapper&#39;:
            _global_checks.global_check_import(&#39;shap&#39;, &#39;SHAP explanations&#39;)
            predict_parts_ = ShapWrapper(&#39;predict_parts&#39;)

        predict_parts_.fit(self, new_observation, **kwargs)

        return predict_parts_

    def predict_profile(self,
                        new_observation,
                        type=(&#39;ceteris_paribus&#39;,),
                        y=None,
                        variables=None,
                        grid_points=101,
                        variable_splits=None,
                        variable_splits_type=&#39;uniform&#39;,
                        variable_splits_with_obs=True,
                        processes=1,
                        verbose=True):
        &#34;&#34;&#34;Calculate instance level variable profiles as Ceteris Paribus

        Parameters
        -----------
        new_observation : pd.DataFrame or np.ndarray or pd.Series
            Observations for which predictions need to be explained.
        type : {&#39;ceteris_paribus&#39;, TODO: &#39;oscilations&#39;}
            Type of variable profiles (default is &#39;ceteris_paribus&#39;).
        y : pd.Series or np.ndarray (1d), optional
            Target variable with the same length as `new_observation`.
        variables : str or array_like of str, optional
            Variables for which the profiles will be calculated
            (default is None, which means all of the variables).
        grid_points : int, optional
            Maximum number of points for profile calculations (default is 101).
            NOTE: The final number of points may be lower than `grid_points`,
            eg. if there is not enough unique values for a given variable.
        variable_splits : dict of lists, optional
            Split points for variables e.g. {&#39;x&#39;: [0, 0.2, 0.5, 0.8, 1], &#39;y&#39;: [&#39;a&#39;, &#39;b&#39;]}
            (default is None, which means that they will be calculated using one of
            `variable_splits_type` and the `data` attribute).
        variable_splits_type : {&#39;uniform&#39;, &#39;quantiles&#39;}, optional
            Way of calculating `variable_splits`. Set &#39;quantiles&#39; for percentiles.
            (default is &#39;uniform&#39;, which means uniform grid of points).
        variable_splits_with_obs: bool, optional
            Add variable values of `new_observation` data to the `variable_splits`
            (default is True).
        processes : int, optional
            Number of parallel processes to use in calculations. Iterated over `variables`
            (default is 1, which means no parallel computation).
        verbose : bool, optional
            Print tqdm progress bar (default is True).

        Returns
        -----------
        CeterisParibus class object
            Explanation object containing the main result attribute and the plot method.

        Notes
        --------
        https://pbiecek.github.io/ema/ceterisParibus.html
        &#34;&#34;&#34;

        check_data_again(self.data)

        types = (&#39;ceteris_paribus&#39;,)
        type = check_method_type(type, types)

        if type == &#39;ceteris_paribus&#39;:
            predict_profile_ = CeterisParibus(
                variables=variables,
                grid_points=grid_points,
                variable_splits=variable_splits,
                variable_splits_type=variable_splits_type,
                variable_splits_with_obs=variable_splits_with_obs,
                processes=processes
            )

        predict_profile_.fit(self, new_observation, y, verbose)

        return predict_profile_

    def predict_surrogate(self, new_observation, type=&#39;lime&#39;, **kwargs):
        &#34;&#34;&#34;Wrapper for surrogate model explanations

        This function uses the lime package to create the model explanation.
        See https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular

        Parameters
        -----------
        new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
            An observation for which a prediction needs to be explained.
        type : {&#39;lime&#39;}
            Type of explanation method
            (default is &#39;lime&#39;, which uses the lime package to create an explanation).
        kwargs :
            Keyword arguments passed to the lime.lime_tabular.LimeTabularExplainer object
            and the LimeTabularExplainer.explain_instance method. Exceptions are:
            `training_data`, `mode`, `data_row` and `predict_fn`. Other parameters:
            https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular

        Returns
        -----------
        lime.explanation.Explanation
            Explanation object.

        Notes
        -----------
        https://github.com/marcotcr/lime
        &#34;&#34;&#34;

        check_data_again(self.data)

        if type == &#39;lime&#39;:
            _global_checks.global_check_import(&#39;lime&#39;, &#39;LIME explanations&#39;)
            from lime.lime_tabular import LimeTabularExplainer
            new_observation = check_new_observation_lime(new_observation)

            explainer_dict, explanation_dict = unpack_kwargs_lime(self, new_observation, **kwargs)
            lime_tabular_explainer = LimeTabularExplainer(**explainer_dict)
            explanation = lime_tabular_explainer.explain_instance(**explanation_dict)

            return explanation

    def model_performance(self,
                          model_type=None,
                          cutoff=0.5):
        &#34;&#34;&#34;Calculate dataset level model performance measures

        Parameters
        -----------
        model_type : {&#39;regression&#39;, &#39;classification&#39;, None}
            Model task type that is used to choose the proper performance measures
            (default is None, which means try to extract from the `model_type` attribute).
        cutoff : float, optional
            Cutoff for predictions in classification models. Needed for measures like
            recall, precision, acc, f1 (default is 0.5).

        Returns
        -----------
        ModelPerformance class object
            Explanation object containing the main result attribute and the plot method.

        Notes
        --------
        https://pbiecek.github.io/ema/modelPerformance.html
        &#34;&#34;&#34;

        check_data_again(self.data)
        check_y_again(self.y)

        if model_type is None and self.model_type is None:
            raise TypeError(&#34;if self.model_type is None, then model_type must be not None&#34;)
        elif model_type is None:
            model_type = self.model_type

        model_performance_ = ModelPerformance(
            model_type=model_type,
            cutoff=cutoff
        )
        model_performance_.fit(self)

        return model_performance_

    def model_parts(self,
                    loss_function=None,
                    type=(&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;, &#39;shap_wrapper&#39;),
                    N=1000,
                    B=10,
                    variables=None,
                    variable_groups=None,
                    keep_raw_permutations=True,
                    processes=1,
                    random_state=None,
                    **kwargs):

        &#34;&#34;&#34;Calculate dataset level variable importance

        Parameters
        -----------
        loss_function : {&#39;rmse&#39;, &#39;1-auc&#39;, &#39;mse&#39;, &#39;mae&#39;, &#39;mad&#39;} or function, optional
            If string, then such loss function will be used to assess variable importance
            (default is &#39;rmse&#39; or `1-auc`, depends on `model_type` attribute).
        type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;, &#39;shap_wrapper&#39;}, optional
            Type of transformation that will be applied to dropout loss.
            (default is &#39;variable_importance&#39;, which is Permutational Variable Importance).
        N : int, optional
            Number of observations that will be sampled from the `data` attribute before
            the calculation of variable importance. None means all `data` (default is 1000).
        B : int, optional
            Number of permutation rounds to perform on each variable (default is 10).
        variables : array_like of str, optional
            Variables for which the importance will be calculated
            (default is None, which means all of the variables).
            NOTE: Ignored if `variable_groups` is not None.
        variable_groups : dict of lists, optional
            Group the variables to calculate their joint variable importance
            e.g. {&#39;X&#39;: [&#39;x1&#39;, &#39;x2&#39;], &#39;Y&#39;: [&#39;y1&#39;, &#39;y2&#39;]} (default is None).
        keep_raw_permutations: bool, optional
            Save results for all permutation rounds (default is True).
        processes : int, optional
            Number of parallel processes to use in calculations. Iterated over `B`
            (default is 1, which means no parallel computation).
        random_state : int, optional
            Set seed for random number generator (default is random seed).
        kwargs :
            Used only for &#39;shap_wrapper&#39;. Pass `shap_explainer_type` to specify, which
            Explainer shall be used: {&#39;TreeExplainer&#39;, &#39;DeepExplainer&#39;, &#39;GradientExplainer&#39;,
            &#39;LinearExplainer&#39;, &#39;KernelExplainer&#39;}.
            Also keyword arguments passed to one of the: shap.TreeExplainer.shap_values,
            shap.DeepExplainer.shap_values, shap.GradientExplainer.shap_values,
            shap.LinearExplainer.shap_values, shap.KernelExplainer.shap_values.
            See https://github.com/slundberg/shap

        Returns
        -----------
        VariableImportance or ShapWrapper class object
            Explanation object containing the main result attribute and the plot method.
            Object class, its attributes, and the plot method depend on the `type` parameter.

        Notes
        --------
        https://pbiecek.github.io/ema/featureImportance.html
        https://github.com/slundberg/shap
        &#34;&#34;&#34;

        check_data_again(self.data)

        types = (&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;, &#39;shap_wrapper&#39;)
        aliases = {&#39;permutational&#39;: &#39;variable_importance&#39;, &#39;feature_importance&#39;: &#39;variable_importance&#39;}
        type = check_method_type(type, types, aliases)

        loss_function = check_method_loss_function(self, loss_function)

        if type != &#39;shap_wrapper&#39;:
            check_y_again(self.y)

            model_parts_ = VariableImportance(
                loss_function=loss_function,
                type=type,
                N=N,
                B=B,
                variables=variables,
                variable_groups=variable_groups,
                processes=processes,
                random_state=random_state,
                keep_raw_permutations=keep_raw_permutations,
            )
            model_parts_.fit(self)
        elif type == &#39;shap_wrapper&#39;:
            _global_checks.global_check_import(&#39;shap&#39;, &#39;SHAP explanations&#39;)
            model_parts_ = ShapWrapper(&#39;model_parts&#39;)
            if N is None:
                N = self.data.shape[0]
            else:
                N = min(N, self.data.shape[0])

            sampled_rows = np.random.choice(np.arange(N), N, replace=False)
            sampled_data = self.data.iloc[sampled_rows, :]

            model_parts_.fit(self, sampled_data, **kwargs)

        return model_parts_

    def model_profile(self,
                      type=(&#39;partial&#39;, &#39;accumulated&#39;, &#39;conditional&#39;),
                      N=300,
                      variables=None,
                      variable_type=&#39;numerical&#39;,
                      groups=None,
                      span=0.25,
                      grid_points=101,
                      variable_splits=None,
                      variable_splits_type=&#39;uniform&#39;,
                      center=True,
                      processes=1,
                      random_state=None,
                      verbose=True):

        &#34;&#34;&#34;Calculate dataset level variable profiles as Partial or Accumulated Dependence

        Parameters
        -----------
        type : {&#39;partial&#39;, &#39;accumulated&#39;, &#39;conditional&#39;}
            Type of model profiles (default is &#39;partial&#39; for Partial Dependence Profiles).
        N : int, optional
            Number of observations that will be sampled from the `data` attribute before
            the calculation of variable profiles. None means all `data` (default is 300).
        variables : str or array_like of str, optional
            Variables for which the profiles will be calculated
            (default is None, which means all of the variables).
        variable_type : {&#39;numerical&#39;, &#39;categorical&#39;}
            Calculate the profiles for numerical or categorical variables
            (default is &#39;numerical&#39;).
        groups : str or array_like of str, optional
            Names of categorical variables that will be used for profile grouping
            (default is None, which means no grouping).
        span : float, optional
            Smoothing coefficient used as sd for gaussian kernel (default is 0.25).
        grid_points : int, optional
            Maximum number of points for profile calculations (default is 101).
            NOTE: The final number of points may be lower than `grid_points`,
            eg. if there is not enough unique values for a given variable.
        variable_splits : dict of lists, optional
            Split points for variables e.g. {&#39;x&#39;: [0, 0.2, 0.5, 0.8, 1], &#39;y&#39;: [&#39;a&#39;, &#39;b&#39;]}
            (default is None, which means that they will be distributed uniformly).
        variable_splits_type : {&#39;uniform&#39;, &#39;quantiles&#39;}, optional
            Way of calculating `variable_splits`. Set &#39;quantiles&#39; for percentiles.
            (default is &#39;uniform&#39;, which means uniform grid of points).
        center : bool, optional
            Theoretically Accumulated Profiles start at 0, but are centered to compare
            them with Partial Dependence Profiles (default is True, which means center
            around the average y_hat calculated on the data sample).
        processes : int, optional
            Number of parallel processes to use in calculations. Iterated over `variables`
            (default is 1, which means no parallel computation).
        random_state : int, optional
            Set seed for random number generator (default is random seed).
        verbose : bool, optional
            Print tqdm progress bar (default is True).

        Returns
        -----------
        AggregatedProfiles class object
            Explanation object containing the main result attribute and the plot method.

        Notes
        --------
        https://pbiecek.github.io/ema/partialDependenceProfiles.html
        https://pbiecek.github.io/ema/accumulatedLocalProfiles.html
        &#34;&#34;&#34;

        check_data_again(self.data)

        types = (&#39;partial&#39;, &#39;accumulated&#39;, &#39;conditional&#39;)
        aliases = {&#39;pdp&#39;: &#39;partial&#39;, &#39;ale&#39;: &#39;accumulated&#39;}
        type = check_method_type(type, types, aliases)

        if N is None:
            N = self.data.shape[0]
        else:
            N = min(N, self.data.shape[0])

        if random_state is not None:
            np.random.seed(random_state)

        I = np.random.choice(np.arange(N), N, replace=False)

        ceteris_paribus = CeterisParibus(grid_points=grid_points,
                                         variables=variables,
                                         variable_splits=variable_splits,
                                         variable_splits_type=variable_splits_type,
                                         processes=processes)
        _y = self.y[I] if self.y is not None else self.y
        ceteris_paribus.fit(self, self.data.iloc[I, :], y=_y, verbose=verbose)

        model_profile_ = AggregatedProfiles(
            type=type,
            variables=variables,
            variable_type=variable_type,
            groups=groups,
            span=span,
            center=center,
            random_state=random_state
        )

        model_profile_.fit(ceteris_paribus, verbose)

        return model_profile_

    def model_diagnostics(self,
                          variables=None):
        &#34;&#34;&#34;Calculate dataset level residuals diagnostics

        Parameters
        -----------
        variables : str or array_like of str, optional
            Variables for which the data will be calculated
            (default is None, which means all of the variables).

        Returns
        -----------
        ResidualDiagnostics class object
            Explanation object containing the main result attribute and the plot method.

        Notes
        --------
        https://pbiecek.github.io/ema/residualDiagnostic.html
        &#34;&#34;&#34;

        check_data_again(self.data)
        check_y_again(self.y)

        residual_diagnostics_ = ResidualDiagnostics(
            variables=variables
        )
        residual_diagnostics_.fit(self)

        return residual_diagnostics_

    def model_surrogate(self,
                        type=(&#39;tree&#39;, &#39;linear&#39;),
                        max_vars=5,
                        max_depth=3,
                        **kwargs):
        &#34;&#34;&#34;Create a surrogate interpretable model from the black-box model

        This method uses the scikit-learn package to create a surrogate
        interpretable model (e.g. decision tree) from the black-box model.
        It aims to use the most important features and add a plot method to
        the model, so that it can be easily interpreted. See Notes section
        for references.

        Parameters
        -----------
        type : {&#39;tree&#39;, &#39;linear&#39;}
            Type of a surrogate model. This can be a decision tree or a linear model
            (default is &#39;tree&#39;).
        max_vars : int, optional
            Maximum number of variables that will be used in surrogate model training.
            These are the most important variables to the black-box model (default is 5).
        max_depth : int, optional
            The maximum depth of the tree. If None, then nodes are expanded until all
            leaves are pure or until all leaves contain less than min_samples_split
            samples (default is 3 for interpretable plot).
        kwargs :
            Keyword arguments passed to one of the: sklearn.tree.DecisionTreeClassifier,
            sklearn.tree.DecisionTreeRegressor, sklearn.linear_model.LogisticRegression,
            sklearn.linear_model.LinearRegression


        Returns
        -----------
        sklearn.tree.DecisionTreeClassifier, sklearn.tree.DecisionTreeRegressor,
        sklearn.linear_model.LogisticRegression, sklearn.linear_model.LinearRegression
            A surrogate model with additional:
                - `plot` method
                - `performance` attribute
                - `feature_names` attribute
                - `class_names` attribute

        Notes
        -----------
        https://christophm.github.io/interpretable-ml-book/global.html
        https://github.com/scikit-learn/scikit-learn
        &#34;&#34;&#34;

        _global_checks.global_check_import(&#39;scikit-learn&#39;, &#39;surrogate models&#39;)
        check_data_again(self.data)

        types = (&#39;tree&#39;, &#39;linear&#39;)
        type = check_method_type(type, types)

        surrogate_model = create_surrogate_model(explainer=self,
                                                 type=type,
                                                 max_vars=max_vars,
                                                 max_depth=max_depth,
                                                 **kwargs)

        return surrogate_model

    def model_fairness(self, protected, privileged, cutoff=0.5, **kwargs):
        &#34;&#34;&#34;Creates a dataset level fairness explanation that enables bias detection

        This method returns a GroupFairnessClassification object that for now
        supports only classification models. GroupFairnessClassification object
        works as a wrapper of the protected attribute and the Explainer from which
        `y` and `y_hat` attributes were extracted. Along with an information about
        privileged subgroup (value in the `protected` parameter), those 3 vectors
        create triplet (y, y_hat, protected) which is a base for all further
        fairness calculations and visualizations.

        Parameters
        -----------
        protected : np.ndarray (1d)
            Vector, preferably 1-dimensional np.ndarray containing strings,
            which denotes the membership to a subgroup. It doesn&#39;t have to be binary.
            It doesn&#39;t need to be in data. It is sometimes suggested not to use
            sensitive attributes in modelling, but still check model bias for them.
            NOTE: List and pd.Series are also supported, however if provided
            they will be transformed into a (1d) np.ndarray with dtype &#39;U&#39;.
        privileged : str
            Subgroup that is suspected to have the most privilege.
            It needs to be a string present in `protected`.
        cutoff : float or dict, optional
            Threshold for probabilistic output of a classifier.
            It might be:
                float - same for all subgroups from `protected`
                dict - individually adjusted for each subgroup
                       (must have values from `protected` as keys).
        kwargs :
            Keyword arguments. It supports `verbose`, which is a boolean
            value telling if additional output should be printed
            (True) or not (False, default).

        Returns
        -----------
        GroupFairnessClassification class object (a subclass of _FairnessObject)
            Explanation object containing the main result attribute and the plot method.
            It has the following main attributes:
                result : pd.DataFrame
                    Scaled `metric_scores`. The scaling is performed by
                    dividing all metric scores by scores of the privileged subgroup.
                metric_scores : pd.DataFrame
                    Raw metric scores for each subgroup.
                parity_loss : pd.Series
                    It is a summarised `result`. From each metric (column) a logarithm
                    is calculated, then the absolute value is taken and summarised.
                    Therefore, for metric M:
                        `parity_loss` is sum(abs(log(M_i / M_privileged)))
                         where M_i is the metric score for subgroup i.
                label : str
                    `label` attribute from the Explainer object.
                     Labels must be unique when plotting.
                cutoff : dict
                    A float value for each subgroup (key in dict).

        Notes
        -----------
        Verma, S. &amp; Rubin, J. (2018) https://fairware.cs.umass.edu/papers/Verma.pdf
        Zafar, M.B., et al. (2017) https://arxiv.org/pdf/1610.08452.pdf
        Hardt, M., et al. (2016) https://arxiv.org/pdf/1610.02413.pdf
        &#34;&#34;&#34;

        if self.model_type != &#39;classification&#39;:
            raise ValueError(
                &#34;model_fairness for now supports only classification models.&#34;
                &#34;Explainer attribute &#39;model_type&#39; must be &#39;classification&#39;&#34;)

        fobject = GroupFairnessClassification(y=self.y,
                                              y_hat=self.y_hat,
                                              protected=protected,
                                              privileged=privileged,
                                              cutoff=cutoff,
                                              label=self.label,
                                              **kwargs)

        return fobject

    def dumps(self, *args, **kwargs):
        &#34;&#34;&#34;Return the pickled representation (bytes object) of the Explainer

        This method uses the pickle package. See
        https://docs.python.org/3/library/pickle.html#pickle.dumps

        NOTE: local functions and lambdas cannot be pickled.
        Attribute `residual_function` by default contains lambda; thus,
        if not provided by the user, it will be dropped before the dump.

        Parameters
        -----------
        args :
            Positional arguments passed to the pickle.dumps function
        kwargs :
            Keyword arguments passed to the pickle.dumps function

        Returns
        -----------
        bytes object
        &#34;&#34;&#34;

        from copy import deepcopy
        to_dump = deepcopy(self)
        to_dump = check_if_local_and_lambda(to_dump)

        import pickle
        return pickle.dumps(to_dump, *args, **kwargs)

    def dump(self, file, *args, **kwargs):
        &#34;&#34;&#34;Write the pickled representation of the Explainer to the file (pickle)

        This method uses the pickle package. See
        https://docs.python.org/3/library/pickle.html#pickle.dump

        NOTE: local functions and lambdas cannot be pickled.
        Attribute `residual_function` by default contains lambda; thus,
        if not provided by the user, it will be dropped before the dump.

        Parameters
        -----------
        file :
            A file object opened for binary writing, or an io.BytesIO instance.
        args :
            Positional arguments passed to the pickle.dump function
        kwargs :
            Keyword arguments passed to the pickle.dump function
        &#34;&#34;&#34;

        from copy import deepcopy
        to_dump = deepcopy(self)
        to_dump = check_if_local_and_lambda(to_dump)

        import pickle
        return pickle.dump(to_dump, file, *args, **kwargs)

    @staticmethod
    def loads(data, use_defaults=True, *args, **kwargs):
        &#34;&#34;&#34;Load the Explainer from the pickled representation (bytes object)

        This method uses the pickle package. See
        https://docs.python.org/3/library/pickle.html#pickle.loads

        NOTE: local functions and lambdas cannot be pickled.
        If use_defaults is set to True, then dropped functions are set to defaults.

        Parameters
        -----------
        data : bytes object
            Binary representation of the Explainer.
        use_defaults : bool
            Replace empty `predict_function` and `residual_function` with default
            values like in Explainer initialization (default is True).
        args :
            Positional arguments passed to the pickle.loads function
        kwargs :
            Keyword arguments passed to the pickle.loads function

        Returns
        -----------
        Explainer object
        &#34;&#34;&#34;

        import pickle
        exp = pickle.loads(data, *args, **kwargs)

        if use_defaults:
            exp = check_if_empty_fields(exp)

        return exp

    @staticmethod
    def load(file, use_defaults=True, *args, **kwargs):
        &#34;&#34;&#34;Read the pickled representation of the Explainer from the file (pickle)

        This method uses the pickle package. See
        https://docs.python.org/3/library/pickle.html#pickle.load

        NOTE: local functions and lambdas cannot be pickled.
        If use_defaults is set to True, then dropped functions are set to defaults.

        Parameters
        -----------
        file :
            A binary file object opened for reading, or an io.BytesIO object.
        use_defaults : bool
            Replace empty `predict_function` and `residual_function` with default
            values like in Explainer initialization (default is True).
        args :
            Positional arguments passed to the pickle.load function
        kwargs :
            Keyword arguments passed to the pickle.load function

        Returns
        -----------
        Explainer object
        &#34;&#34;&#34;

        import pickle
        exp = pickle.load(file, *args, **kwargs)

        if use_defaults:
            exp = check_if_empty_fields(exp)

        return exp</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="dalex.Explainer.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>file, use_defaults=True, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Read the pickled representation of the Explainer from the file (pickle)</p>
<p>This method uses the pickle package. See
<a href="https://docs.python.org/3/library/pickle.html#pickle.load">https://docs.python.org/3/library/pickle.html#pickle.load</a></p>
<p>NOTE: local functions and lambdas cannot be pickled.
If use_defaults is set to True, then dropped functions are set to defaults.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>file :</dt>
<dt>A binary file object opened for reading, or an io.BytesIO object.</dt>
<dt><strong><code>use_defaults</code></strong> :&ensp;<code>bool</code></dt>
<dd>Replace empty <code>predict_function</code> and <code>residual_function</code> with default
values like in Explainer initialization (default is True).</dd>
</dl>
<p>args :
Positional arguments passed to the pickle.load function
kwargs :
Keyword arguments passed to the pickle.load function</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="dalex.Explainer" href="#dalex.Explainer">Explainer</a> object</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def load(file, use_defaults=True, *args, **kwargs):
    &#34;&#34;&#34;Read the pickled representation of the Explainer from the file (pickle)

    This method uses the pickle package. See
    https://docs.python.org/3/library/pickle.html#pickle.load

    NOTE: local functions and lambdas cannot be pickled.
    If use_defaults is set to True, then dropped functions are set to defaults.

    Parameters
    -----------
    file :
        A binary file object opened for reading, or an io.BytesIO object.
    use_defaults : bool
        Replace empty `predict_function` and `residual_function` with default
        values like in Explainer initialization (default is True).
    args :
        Positional arguments passed to the pickle.load function
    kwargs :
        Keyword arguments passed to the pickle.load function

    Returns
    -----------
    Explainer object
    &#34;&#34;&#34;

    import pickle
    exp = pickle.load(file, *args, **kwargs)

    if use_defaults:
        exp = check_if_empty_fields(exp)

    return exp</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.loads"><code class="name flex">
<span>def <span class="ident">loads</span></span>(<span>data, use_defaults=True, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Load the Explainer from the pickled representation (bytes object)</p>
<p>This method uses the pickle package. See
<a href="https://docs.python.org/3/library/pickle.html#pickle.loads">https://docs.python.org/3/library/pickle.html#pickle.loads</a></p>
<p>NOTE: local functions and lambdas cannot be pickled.
If use_defaults is set to True, then dropped functions are set to defaults.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>bytes object</code></dt>
<dd>Binary representation of the Explainer.</dd>
<dt><strong><code>use_defaults</code></strong> :&ensp;<code>bool</code></dt>
<dd>Replace empty <code>predict_function</code> and <code>residual_function</code> with default
values like in Explainer initialization (default is True).</dd>
</dl>
<p>args :
Positional arguments passed to the pickle.loads function
kwargs :
Keyword arguments passed to the pickle.loads function</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="dalex.Explainer" href="#dalex.Explainer">Explainer</a> object</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def loads(data, use_defaults=True, *args, **kwargs):
    &#34;&#34;&#34;Load the Explainer from the pickled representation (bytes object)

    This method uses the pickle package. See
    https://docs.python.org/3/library/pickle.html#pickle.loads

    NOTE: local functions and lambdas cannot be pickled.
    If use_defaults is set to True, then dropped functions are set to defaults.

    Parameters
    -----------
    data : bytes object
        Binary representation of the Explainer.
    use_defaults : bool
        Replace empty `predict_function` and `residual_function` with default
        values like in Explainer initialization (default is True).
    args :
        Positional arguments passed to the pickle.loads function
    kwargs :
        Keyword arguments passed to the pickle.loads function

    Returns
    -----------
    Explainer object
    &#34;&#34;&#34;

    import pickle
    exp = pickle.loads(data, *args, **kwargs)

    if use_defaults:
        exp = check_if_empty_fields(exp)

    return exp</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="dalex.Explainer.dump"><code class="name flex">
<span>def <span class="ident">dump</span></span>(<span>self, file, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Write the pickled representation of the Explainer to the file (pickle)</p>
<p>This method uses the pickle package. See
<a href="https://docs.python.org/3/library/pickle.html#pickle.dump">https://docs.python.org/3/library/pickle.html#pickle.dump</a></p>
<p>NOTE: local functions and lambdas cannot be pickled.
Attribute <code>residual_function</code> by default contains lambda; thus,
if not provided by the user, it will be dropped before the dump.</p>
<h2 id="parameters">Parameters</h2>
<p>file :
A file object opened for binary writing, or an io.BytesIO instance.
args :
Positional arguments passed to the pickle.dump function
kwargs :
Keyword arguments passed to the pickle.dump function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump(self, file, *args, **kwargs):
    &#34;&#34;&#34;Write the pickled representation of the Explainer to the file (pickle)

    This method uses the pickle package. See
    https://docs.python.org/3/library/pickle.html#pickle.dump

    NOTE: local functions and lambdas cannot be pickled.
    Attribute `residual_function` by default contains lambda; thus,
    if not provided by the user, it will be dropped before the dump.

    Parameters
    -----------
    file :
        A file object opened for binary writing, or an io.BytesIO instance.
    args :
        Positional arguments passed to the pickle.dump function
    kwargs :
        Keyword arguments passed to the pickle.dump function
    &#34;&#34;&#34;

    from copy import deepcopy
    to_dump = deepcopy(self)
    to_dump = check_if_local_and_lambda(to_dump)

    import pickle
    return pickle.dump(to_dump, file, *args, **kwargs)</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.dumps"><code class="name flex">
<span>def <span class="ident">dumps</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the pickled representation (bytes object) of the Explainer</p>
<p>This method uses the pickle package. See
<a href="https://docs.python.org/3/library/pickle.html#pickle.dumps">https://docs.python.org/3/library/pickle.html#pickle.dumps</a></p>
<p>NOTE: local functions and lambdas cannot be pickled.
Attribute <code>residual_function</code> by default contains lambda; thus,
if not provided by the user, it will be dropped before the dump.</p>
<h2 id="parameters">Parameters</h2>
<p>args :
Positional arguments passed to the pickle.dumps function
kwargs :
Keyword arguments passed to the pickle.dumps function</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bytes object</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dumps(self, *args, **kwargs):
    &#34;&#34;&#34;Return the pickled representation (bytes object) of the Explainer

    This method uses the pickle package. See
    https://docs.python.org/3/library/pickle.html#pickle.dumps

    NOTE: local functions and lambdas cannot be pickled.
    Attribute `residual_function` by default contains lambda; thus,
    if not provided by the user, it will be dropped before the dump.

    Parameters
    -----------
    args :
        Positional arguments passed to the pickle.dumps function
    kwargs :
        Keyword arguments passed to the pickle.dumps function

    Returns
    -----------
    bytes object
    &#34;&#34;&#34;

    from copy import deepcopy
    to_dump = deepcopy(self)
    to_dump = check_if_local_and_lambda(to_dump)

    import pickle
    return pickle.dumps(to_dump, *args, **kwargs)</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.model_diagnostics"><code class="name flex">
<span>def <span class="ident">model_diagnostics</span></span>(<span>self, variables=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate dataset level residuals diagnostics</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>variables</code></strong> :&ensp;<code>str</code> or <code>array_like</code> of <code>str</code>, optional</dt>
<dd>Variables for which the data will be calculated
(default is None, which means all of the variables).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ResidualDiagnostics class object</code></dt>
<dd>Explanation object containing the main result attribute and the plot method.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p><a href="https://pbiecek.github.io/ema/residualDiagnostic.html">https://pbiecek.github.io/ema/residualDiagnostic.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_diagnostics(self,
                      variables=None):
    &#34;&#34;&#34;Calculate dataset level residuals diagnostics

    Parameters
    -----------
    variables : str or array_like of str, optional
        Variables for which the data will be calculated
        (default is None, which means all of the variables).

    Returns
    -----------
    ResidualDiagnostics class object
        Explanation object containing the main result attribute and the plot method.

    Notes
    --------
    https://pbiecek.github.io/ema/residualDiagnostic.html
    &#34;&#34;&#34;

    check_data_again(self.data)
    check_y_again(self.y)

    residual_diagnostics_ = ResidualDiagnostics(
        variables=variables
    )
    residual_diagnostics_.fit(self)

    return residual_diagnostics_</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.model_fairness"><code class="name flex">
<span>def <span class="ident">model_fairness</span></span>(<span>self, protected, privileged, cutoff=0.5, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a dataset level fairness explanation that enables bias detection</p>
<p>This method returns a GroupFairnessClassification object that for now
supports only classification models. GroupFairnessClassification object
works as a wrapper of the protected attribute and the Explainer from which
<code>y</code> and <code>y_hat</code> attributes were extracted. Along with an information about
privileged subgroup (value in the <code>protected</code> parameter), those 3 vectors
create triplet (y, y_hat, protected) which is a base for all further
fairness calculations and visualizations.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>protected</code></strong> :&ensp;<code>np.ndarray (1d)</code></dt>
<dd>Vector, preferably 1-dimensional np.ndarray containing strings,
which denotes the membership to a subgroup. It doesn't have to be binary.
It doesn't need to be in data. It is sometimes suggested not to use
sensitive attributes in modelling, but still check model bias for them.
NOTE: List and pd.Series are also supported, however if provided
they will be transformed into a (1d) np.ndarray with dtype 'U'.</dd>
<dt><strong><code>privileged</code></strong> :&ensp;<code>str</code></dt>
<dd>Subgroup that is suspected to have the most privilege.
It needs to be a string present in <code>protected</code>.</dd>
<dt><strong><code>cutoff</code></strong> :&ensp;<code>float</code> or <code>dict</code>, optional</dt>
<dd>Threshold for probabilistic output of a classifier.
It might be:
float - same for all subgroups from <code>protected</code>
dict - individually adjusted for each subgroup
(must have values from <code>protected</code> as keys).</dd>
</dl>
<p>kwargs :
Keyword arguments. It supports <code>verbose</code>, which is a boolean
value telling if additional output should be printed
(True) or not (False, default).</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>GroupFairnessClassification class object (a subclass</code> of <code>_FairnessObject)</code></dt>
<dd>Explanation object containing the main result attribute and the plot method.
It has the following main attributes:
result : pd.DataFrame
Scaled <code>metric_scores</code>. The scaling is performed by
dividing all metric scores by scores of the privileged subgroup.
metric_scores : pd.DataFrame
Raw metric scores for each subgroup.
parity_loss : pd.Series
It is a summarised <code>result</code>. From each metric (column) a logarithm
is calculated, then the absolute value is taken and summarised.
Therefore, for metric M:
<code>parity_loss</code> is sum(abs(log(M_i / M_privileged)))
where M_i is the metric score for subgroup i.
label : str
<code>label</code> attribute from the Explainer object.
Labels must be unique when plotting.
cutoff : dict
A float value for each subgroup (key in dict).</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Verma, S. &amp; Rubin, J. (2018) <a href="https://fairware.cs.umass.edu/papers/Verma.pdf">https://fairware.cs.umass.edu/papers/Verma.pdf</a>
Zafar, M.B., et al. (2017) <a href="https://arxiv.org/pdf/1610.08452.pdf">https://arxiv.org/pdf/1610.08452.pdf</a>
Hardt, M., et al. (2016) <a href="https://arxiv.org/pdf/1610.02413.pdf">https://arxiv.org/pdf/1610.02413.pdf</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_fairness(self, protected, privileged, cutoff=0.5, **kwargs):
    &#34;&#34;&#34;Creates a dataset level fairness explanation that enables bias detection

    This method returns a GroupFairnessClassification object that for now
    supports only classification models. GroupFairnessClassification object
    works as a wrapper of the protected attribute and the Explainer from which
    `y` and `y_hat` attributes were extracted. Along with an information about
    privileged subgroup (value in the `protected` parameter), those 3 vectors
    create triplet (y, y_hat, protected) which is a base for all further
    fairness calculations and visualizations.

    Parameters
    -----------
    protected : np.ndarray (1d)
        Vector, preferably 1-dimensional np.ndarray containing strings,
        which denotes the membership to a subgroup. It doesn&#39;t have to be binary.
        It doesn&#39;t need to be in data. It is sometimes suggested not to use
        sensitive attributes in modelling, but still check model bias for them.
        NOTE: List and pd.Series are also supported, however if provided
        they will be transformed into a (1d) np.ndarray with dtype &#39;U&#39;.
    privileged : str
        Subgroup that is suspected to have the most privilege.
        It needs to be a string present in `protected`.
    cutoff : float or dict, optional
        Threshold for probabilistic output of a classifier.
        It might be:
            float - same for all subgroups from `protected`
            dict - individually adjusted for each subgroup
                   (must have values from `protected` as keys).
    kwargs :
        Keyword arguments. It supports `verbose`, which is a boolean
        value telling if additional output should be printed
        (True) or not (False, default).

    Returns
    -----------
    GroupFairnessClassification class object (a subclass of _FairnessObject)
        Explanation object containing the main result attribute and the plot method.
        It has the following main attributes:
            result : pd.DataFrame
                Scaled `metric_scores`. The scaling is performed by
                dividing all metric scores by scores of the privileged subgroup.
            metric_scores : pd.DataFrame
                Raw metric scores for each subgroup.
            parity_loss : pd.Series
                It is a summarised `result`. From each metric (column) a logarithm
                is calculated, then the absolute value is taken and summarised.
                Therefore, for metric M:
                    `parity_loss` is sum(abs(log(M_i / M_privileged)))
                     where M_i is the metric score for subgroup i.
            label : str
                `label` attribute from the Explainer object.
                 Labels must be unique when plotting.
            cutoff : dict
                A float value for each subgroup (key in dict).

    Notes
    -----------
    Verma, S. &amp; Rubin, J. (2018) https://fairware.cs.umass.edu/papers/Verma.pdf
    Zafar, M.B., et al. (2017) https://arxiv.org/pdf/1610.08452.pdf
    Hardt, M., et al. (2016) https://arxiv.org/pdf/1610.02413.pdf
    &#34;&#34;&#34;

    if self.model_type != &#39;classification&#39;:
        raise ValueError(
            &#34;model_fairness for now supports only classification models.&#34;
            &#34;Explainer attribute &#39;model_type&#39; must be &#39;classification&#39;&#34;)

    fobject = GroupFairnessClassification(y=self.y,
                                          y_hat=self.y_hat,
                                          protected=protected,
                                          privileged=privileged,
                                          cutoff=cutoff,
                                          label=self.label,
                                          **kwargs)

    return fobject</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.model_parts"><code class="name flex">
<span>def <span class="ident">model_parts</span></span>(<span>self, loss_function=None, type=('variable_importance', 'ratio', 'difference', 'shap_wrapper'), N=1000, B=10, variables=None, variable_groups=None, keep_raw_permutations=True, processes=1, random_state=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate dataset level variable importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>{'rmse', '1-auc', 'mse', 'mae', 'mad'}</code> or <code>function</code>, optional</dt>
<dd>If string, then such loss function will be used to assess variable importance
(default is 'rmse' or <code>1-auc</code>, depends on <code>model_type</code> attribute).</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'variable_importance', 'ratio', 'difference', 'shap_wrapper'}</code>, optional</dt>
<dd>Type of transformation that will be applied to dropout loss.
(default is 'variable_importance', which is Permutational Variable Importance).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>data</code> attribute before
the calculation of variable importance. None means all <code>data</code> (default is 1000).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of permutation rounds to perform on each variable (default is 10).</dd>
<dt><strong><code>variables</code></strong> :&ensp;<code>array_like</code> of <code>str</code>, optional</dt>
<dd>Variables for which the importance will be calculated
(default is None, which means all of the variables).
NOTE: Ignored if <code>variable_groups</code> is not None.</dd>
<dt><strong><code>variable_groups</code></strong> :&ensp;<code>dict</code> of <code>lists</code>, optional</dt>
<dd>Group the variables to calculate their joint variable importance
e.g. {'X': ['x1', 'x2'], 'Y': ['y1', 'y2']} (default is None).</dd>
<dt><strong><code>keep_raw_permutations</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Save results for all permutation rounds (default is True).</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>
(default is 1, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<p>kwargs :
Used only for 'shap_wrapper'. Pass <code>shap_explainer_type</code> to specify, which
Explainer shall be used: {'TreeExplainer', 'DeepExplainer', 'GradientExplainer',
'LinearExplainer', 'KernelExplainer'}.
Also keyword arguments passed to one of the: shap.TreeExplainer.shap_values,
shap.DeepExplainer.shap_values, shap.GradientExplainer.shap_values,
shap.LinearExplainer.shap_values, shap.KernelExplainer.shap_values.
See <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>VariableImportance</code> or <code>ShapWrapper class object</code></dt>
<dd>Explanation object containing the main result attribute and the plot method.
Object class, its attributes, and the plot method depend on the <code>type</code> parameter.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p><a href="https://pbiecek.github.io/ema/featureImportance.html">https://pbiecek.github.io/ema/featureImportance.html</a>
<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_parts(self,
                loss_function=None,
                type=(&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;, &#39;shap_wrapper&#39;),
                N=1000,
                B=10,
                variables=None,
                variable_groups=None,
                keep_raw_permutations=True,
                processes=1,
                random_state=None,
                **kwargs):

    &#34;&#34;&#34;Calculate dataset level variable importance

    Parameters
    -----------
    loss_function : {&#39;rmse&#39;, &#39;1-auc&#39;, &#39;mse&#39;, &#39;mae&#39;, &#39;mad&#39;} or function, optional
        If string, then such loss function will be used to assess variable importance
        (default is &#39;rmse&#39; or `1-auc`, depends on `model_type` attribute).
    type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;, &#39;shap_wrapper&#39;}, optional
        Type of transformation that will be applied to dropout loss.
        (default is &#39;variable_importance&#39;, which is Permutational Variable Importance).
    N : int, optional
        Number of observations that will be sampled from the `data` attribute before
        the calculation of variable importance. None means all `data` (default is 1000).
    B : int, optional
        Number of permutation rounds to perform on each variable (default is 10).
    variables : array_like of str, optional
        Variables for which the importance will be calculated
        (default is None, which means all of the variables).
        NOTE: Ignored if `variable_groups` is not None.
    variable_groups : dict of lists, optional
        Group the variables to calculate their joint variable importance
        e.g. {&#39;X&#39;: [&#39;x1&#39;, &#39;x2&#39;], &#39;Y&#39;: [&#39;y1&#39;, &#39;y2&#39;]} (default is None).
    keep_raw_permutations: bool, optional
        Save results for all permutation rounds (default is True).
    processes : int, optional
        Number of parallel processes to use in calculations. Iterated over `B`
        (default is 1, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).
    kwargs :
        Used only for &#39;shap_wrapper&#39;. Pass `shap_explainer_type` to specify, which
        Explainer shall be used: {&#39;TreeExplainer&#39;, &#39;DeepExplainer&#39;, &#39;GradientExplainer&#39;,
        &#39;LinearExplainer&#39;, &#39;KernelExplainer&#39;}.
        Also keyword arguments passed to one of the: shap.TreeExplainer.shap_values,
        shap.DeepExplainer.shap_values, shap.GradientExplainer.shap_values,
        shap.LinearExplainer.shap_values, shap.KernelExplainer.shap_values.
        See https://github.com/slundberg/shap

    Returns
    -----------
    VariableImportance or ShapWrapper class object
        Explanation object containing the main result attribute and the plot method.
        Object class, its attributes, and the plot method depend on the `type` parameter.

    Notes
    --------
    https://pbiecek.github.io/ema/featureImportance.html
    https://github.com/slundberg/shap
    &#34;&#34;&#34;

    check_data_again(self.data)

    types = (&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;, &#39;shap_wrapper&#39;)
    aliases = {&#39;permutational&#39;: &#39;variable_importance&#39;, &#39;feature_importance&#39;: &#39;variable_importance&#39;}
    type = check_method_type(type, types, aliases)

    loss_function = check_method_loss_function(self, loss_function)

    if type != &#39;shap_wrapper&#39;:
        check_y_again(self.y)

        model_parts_ = VariableImportance(
            loss_function=loss_function,
            type=type,
            N=N,
            B=B,
            variables=variables,
            variable_groups=variable_groups,
            processes=processes,
            random_state=random_state,
            keep_raw_permutations=keep_raw_permutations,
        )
        model_parts_.fit(self)
    elif type == &#39;shap_wrapper&#39;:
        _global_checks.global_check_import(&#39;shap&#39;, &#39;SHAP explanations&#39;)
        model_parts_ = ShapWrapper(&#39;model_parts&#39;)
        if N is None:
            N = self.data.shape[0]
        else:
            N = min(N, self.data.shape[0])

        sampled_rows = np.random.choice(np.arange(N), N, replace=False)
        sampled_data = self.data.iloc[sampled_rows, :]

        model_parts_.fit(self, sampled_data, **kwargs)

    return model_parts_</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.model_performance"><code class="name flex">
<span>def <span class="ident">model_performance</span></span>(<span>self, model_type=None, cutoff=0.5)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate dataset level model performance measures</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_type</code></strong> :&ensp;<code>{'regression', 'classification', None}</code></dt>
<dd>Model task type that is used to choose the proper performance measures
(default is None, which means try to extract from the <code>model_type</code> attribute).</dd>
<dt><strong><code>cutoff</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Cutoff for predictions in classification models. Needed for measures like
recall, precision, acc, f1 (default is 0.5).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ModelPerformance class object</code></dt>
<dd>Explanation object containing the main result attribute and the plot method.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p><a href="https://pbiecek.github.io/ema/modelPerformance.html">https://pbiecek.github.io/ema/modelPerformance.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_performance(self,
                      model_type=None,
                      cutoff=0.5):
    &#34;&#34;&#34;Calculate dataset level model performance measures

    Parameters
    -----------
    model_type : {&#39;regression&#39;, &#39;classification&#39;, None}
        Model task type that is used to choose the proper performance measures
        (default is None, which means try to extract from the `model_type` attribute).
    cutoff : float, optional
        Cutoff for predictions in classification models. Needed for measures like
        recall, precision, acc, f1 (default is 0.5).

    Returns
    -----------
    ModelPerformance class object
        Explanation object containing the main result attribute and the plot method.

    Notes
    --------
    https://pbiecek.github.io/ema/modelPerformance.html
    &#34;&#34;&#34;

    check_data_again(self.data)
    check_y_again(self.y)

    if model_type is None and self.model_type is None:
        raise TypeError(&#34;if self.model_type is None, then model_type must be not None&#34;)
    elif model_type is None:
        model_type = self.model_type

    model_performance_ = ModelPerformance(
        model_type=model_type,
        cutoff=cutoff
    )
    model_performance_.fit(self)

    return model_performance_</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.model_profile"><code class="name flex">
<span>def <span class="ident">model_profile</span></span>(<span>self, type=('partial', 'accumulated', 'conditional'), N=300, variables=None, variable_type='numerical', groups=None, span=0.25, grid_points=101, variable_splits=None, variable_splits_type='uniform', center=True, processes=1, random_state=None, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate dataset level variable profiles as Partial or Accumulated Dependence</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type</code></strong> :&ensp;<code>{'partial', 'accumulated', 'conditional'}</code></dt>
<dd>Type of model profiles (default is 'partial' for Partial Dependence Profiles).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>data</code> attribute before
the calculation of variable profiles. None means all <code>data</code> (default is 300).</dd>
<dt><strong><code>variables</code></strong> :&ensp;<code>str</code> or <code>array_like</code> of <code>str</code>, optional</dt>
<dd>Variables for which the profiles will be calculated
(default is None, which means all of the variables).</dd>
<dt><strong><code>variable_type</code></strong> :&ensp;<code>{'numerical', 'categorical'}</code></dt>
<dd>Calculate the profiles for numerical or categorical variables
(default is 'numerical').</dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>str</code> or <code>array_like</code> of <code>str</code>, optional</dt>
<dd>Names of categorical variables that will be used for profile grouping
(default is None, which means no grouping).</dd>
<dt><strong><code>span</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Smoothing coefficient used as sd for gaussian kernel (default is 0.25).</dd>
<dt><strong><code>grid_points</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of points for profile calculations (default is 101).
NOTE: The final number of points may be lower than <code>grid_points</code>,
eg. if there is not enough unique values for a given variable.</dd>
<dt><strong><code>variable_splits</code></strong> :&ensp;<code>dict</code> of <code>lists</code>, optional</dt>
<dd>Split points for variables e.g. {'x': [0, 0.2, 0.5, 0.8, 1], 'y': ['a', 'b']}
(default is None, which means that they will be distributed uniformly).</dd>
<dt><strong><code>variable_splits_type</code></strong> :&ensp;<code>{'uniform', 'quantiles'}</code>, optional</dt>
<dd>Way of calculating <code>variable_splits</code>. Set 'quantiles' for percentiles.
(default is 'uniform', which means uniform grid of points).</dd>
<dt><strong><code>center</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Theoretically Accumulated Profiles start at 0, but are centered to compare
them with Partial Dependence Profiles (default is True, which means center
around the average y_hat calculated on the data sample).</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>variables</code>
(default is 1, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Print tqdm progress bar (default is True).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>AggregatedProfiles class object</code></dt>
<dd>Explanation object containing the main result attribute and the plot method.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p><a href="https://pbiecek.github.io/ema/partialDependenceProfiles.html">https://pbiecek.github.io/ema/partialDependenceProfiles.html</a>
<a href="https://pbiecek.github.io/ema/accumulatedLocalProfiles.html">https://pbiecek.github.io/ema/accumulatedLocalProfiles.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_profile(self,
                  type=(&#39;partial&#39;, &#39;accumulated&#39;, &#39;conditional&#39;),
                  N=300,
                  variables=None,
                  variable_type=&#39;numerical&#39;,
                  groups=None,
                  span=0.25,
                  grid_points=101,
                  variable_splits=None,
                  variable_splits_type=&#39;uniform&#39;,
                  center=True,
                  processes=1,
                  random_state=None,
                  verbose=True):

    &#34;&#34;&#34;Calculate dataset level variable profiles as Partial or Accumulated Dependence

    Parameters
    -----------
    type : {&#39;partial&#39;, &#39;accumulated&#39;, &#39;conditional&#39;}
        Type of model profiles (default is &#39;partial&#39; for Partial Dependence Profiles).
    N : int, optional
        Number of observations that will be sampled from the `data` attribute before
        the calculation of variable profiles. None means all `data` (default is 300).
    variables : str or array_like of str, optional
        Variables for which the profiles will be calculated
        (default is None, which means all of the variables).
    variable_type : {&#39;numerical&#39;, &#39;categorical&#39;}
        Calculate the profiles for numerical or categorical variables
        (default is &#39;numerical&#39;).
    groups : str or array_like of str, optional
        Names of categorical variables that will be used for profile grouping
        (default is None, which means no grouping).
    span : float, optional
        Smoothing coefficient used as sd for gaussian kernel (default is 0.25).
    grid_points : int, optional
        Maximum number of points for profile calculations (default is 101).
        NOTE: The final number of points may be lower than `grid_points`,
        eg. if there is not enough unique values for a given variable.
    variable_splits : dict of lists, optional
        Split points for variables e.g. {&#39;x&#39;: [0, 0.2, 0.5, 0.8, 1], &#39;y&#39;: [&#39;a&#39;, &#39;b&#39;]}
        (default is None, which means that they will be distributed uniformly).
    variable_splits_type : {&#39;uniform&#39;, &#39;quantiles&#39;}, optional
        Way of calculating `variable_splits`. Set &#39;quantiles&#39; for percentiles.
        (default is &#39;uniform&#39;, which means uniform grid of points).
    center : bool, optional
        Theoretically Accumulated Profiles start at 0, but are centered to compare
        them with Partial Dependence Profiles (default is True, which means center
        around the average y_hat calculated on the data sample).
    processes : int, optional
        Number of parallel processes to use in calculations. Iterated over `variables`
        (default is 1, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).
    verbose : bool, optional
        Print tqdm progress bar (default is True).

    Returns
    -----------
    AggregatedProfiles class object
        Explanation object containing the main result attribute and the plot method.

    Notes
    --------
    https://pbiecek.github.io/ema/partialDependenceProfiles.html
    https://pbiecek.github.io/ema/accumulatedLocalProfiles.html
    &#34;&#34;&#34;

    check_data_again(self.data)

    types = (&#39;partial&#39;, &#39;accumulated&#39;, &#39;conditional&#39;)
    aliases = {&#39;pdp&#39;: &#39;partial&#39;, &#39;ale&#39;: &#39;accumulated&#39;}
    type = check_method_type(type, types, aliases)

    if N is None:
        N = self.data.shape[0]
    else:
        N = min(N, self.data.shape[0])

    if random_state is not None:
        np.random.seed(random_state)

    I = np.random.choice(np.arange(N), N, replace=False)

    ceteris_paribus = CeterisParibus(grid_points=grid_points,
                                     variables=variables,
                                     variable_splits=variable_splits,
                                     variable_splits_type=variable_splits_type,
                                     processes=processes)
    _y = self.y[I] if self.y is not None else self.y
    ceteris_paribus.fit(self, self.data.iloc[I, :], y=_y, verbose=verbose)

    model_profile_ = AggregatedProfiles(
        type=type,
        variables=variables,
        variable_type=variable_type,
        groups=groups,
        span=span,
        center=center,
        random_state=random_state
    )

    model_profile_.fit(ceteris_paribus, verbose)

    return model_profile_</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.model_surrogate"><code class="name flex">
<span>def <span class="ident">model_surrogate</span></span>(<span>self, type=('tree', 'linear'), max_vars=5, max_depth=3, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a surrogate interpretable model from the black-box model</p>
<p>This method uses the scikit-learn package to create a surrogate
interpretable model (e.g. decision tree) from the black-box model.
It aims to use the most important features and add a plot method to
the model, so that it can be easily interpreted. See Notes section
for references.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type</code></strong> :&ensp;<code>{'tree', 'linear'}</code></dt>
<dd>Type of a surrogate model. This can be a decision tree or a linear model
(default is 'tree').</dd>
<dt><strong><code>max_vars</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of variables that will be used in surrogate model training.
These are the most important variables to the black-box model (default is 5).</dd>
<dt><strong><code>max_depth</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until all
leaves are pure or until all leaves contain less than min_samples_split
samples (default is 3 for interpretable plot).</dd>
</dl>
<p>kwargs :
Keyword arguments passed to one of the: sklearn.tree.DecisionTreeClassifier,
sklearn.tree.DecisionTreeRegressor, sklearn.linear_model.LogisticRegression,
sklearn.linear_model.LinearRegression</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>sklearn.tree.DecisionTreeClassifier, sklearn.tree.DecisionTreeRegressor,</code></dt>
<dd>&nbsp;</dd>
<dt><code>sklearn.linear_model.LogisticRegression, sklearn.linear_model.LinearRegression</code></dt>
<dd>A surrogate model with additional:
- <code>plot</code> method
- <code>performance</code> attribute
- <code>feature_names</code> attribute
- <code>class_names</code> attribute</dd>
</dl>
<h2 id="notes">Notes</h2>
<p><a href="https://christophm.github.io/interpretable-ml-book/global.html">https://christophm.github.io/interpretable-ml-book/global.html</a>
<a href="https://github.com/scikit-learn/scikit-learn">https://github.com/scikit-learn/scikit-learn</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_surrogate(self,
                    type=(&#39;tree&#39;, &#39;linear&#39;),
                    max_vars=5,
                    max_depth=3,
                    **kwargs):
    &#34;&#34;&#34;Create a surrogate interpretable model from the black-box model

    This method uses the scikit-learn package to create a surrogate
    interpretable model (e.g. decision tree) from the black-box model.
    It aims to use the most important features and add a plot method to
    the model, so that it can be easily interpreted. See Notes section
    for references.

    Parameters
    -----------
    type : {&#39;tree&#39;, &#39;linear&#39;}
        Type of a surrogate model. This can be a decision tree or a linear model
        (default is &#39;tree&#39;).
    max_vars : int, optional
        Maximum number of variables that will be used in surrogate model training.
        These are the most important variables to the black-box model (default is 5).
    max_depth : int, optional
        The maximum depth of the tree. If None, then nodes are expanded until all
        leaves are pure or until all leaves contain less than min_samples_split
        samples (default is 3 for interpretable plot).
    kwargs :
        Keyword arguments passed to one of the: sklearn.tree.DecisionTreeClassifier,
        sklearn.tree.DecisionTreeRegressor, sklearn.linear_model.LogisticRegression,
        sklearn.linear_model.LinearRegression


    Returns
    -----------
    sklearn.tree.DecisionTreeClassifier, sklearn.tree.DecisionTreeRegressor,
    sklearn.linear_model.LogisticRegression, sklearn.linear_model.LinearRegression
        A surrogate model with additional:
            - `plot` method
            - `performance` attribute
            - `feature_names` attribute
            - `class_names` attribute

    Notes
    -----------
    https://christophm.github.io/interpretable-ml-book/global.html
    https://github.com/scikit-learn/scikit-learn
    &#34;&#34;&#34;

    _global_checks.global_check_import(&#39;scikit-learn&#39;, &#39;surrogate models&#39;)
    check_data_again(self.data)

    types = (&#39;tree&#39;, &#39;linear&#39;)
    type = check_method_type(type, types)

    surrogate_model = create_surrogate_model(explainer=self,
                                             type=type,
                                             max_vars=max_vars,
                                             max_depth=max_depth,
                                             **kwargs)

    return surrogate_model</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a prediction</p>
<p>This function uses the <code>predict_function</code> attribute.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pd.DataFrame, np.ndarray 2d</code></dt>
<dd>Data which will be used to make a prediction.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray (1d)</code></dt>
<dd>Model predictions for given <code>data</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, data):
    &#34;&#34;&#34;Make a prediction

    This function uses the `predict_function` attribute.

    Parameters
    ----------
    data : pd.DataFrame, np.ndarray 2d
        Data which will be used to make a prediction.

    Returns
    ----------
    np.ndarray (1d)
        Model predictions for given `data`.
    &#34;&#34;&#34;

    check_method_data(data)

    return self.predict_function(self.model, data)</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.predict_parts"><code class="name flex">
<span>def <span class="ident">predict_parts</span></span>(<span>self, new_observation, type=('break_down_interactions', 'break_down', 'shap', 'shap_wrapper'), order=None, interaction_preference=1, path='average', B=25, keep_distributions=False, processes=1, random_state=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate instance level variable attributions as Break Down, Shapley Values or Shap Values</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>new_observation</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code> or <code>pd.DataFrame (1,p)</code></dt>
<dd>An observation for which a prediction needs to be explained.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'break_down_interactions', 'break_down', 'shap', 'shap_wrapper}</code></dt>
<dd>Type of variable attributions (default is 'break_down_interactions').</dd>
<dt><strong><code>order</code></strong> :&ensp;<code>list</code> of <code>int</code> or <code>str</code>, optional</dt>
<dd>Parameter specific for <code>break_down_interactions</code> and <code>break_down</code>. Use a fixed
order of variables for attribution calculation. Use integer values
or string
variable names (default is None, which means order by importance).</dd>
<dt><strong><code>interaction_preference</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>break_down_interactions</code> type. Specify which interactions
will be present in an explanation. The larger the integer, the more frequently
interactions will be presented (default is 1).</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>list</code> of <code>int</code>, optional</dt>
<dd>Parameter specific for <code>shap</code>. If specified, then attributions for this path
will be plotted (default is 'average', which plots attribution means for
<code>B</code> random paths).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>shap</code>. Number of random paths to calculate
variable attributions (default is 25).</dd>
<dt><strong><code>keep_distributions</code></strong> :&ensp;<code> bool</code>, optional</dt>
<dd>Save the distribution of partial predictions (default is False).</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>shap</code>. Number of parallel processes to use in calculations.
Iterated over <code>B</code> (default is 1, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<p>kwargs :
Used only for 'shap_wrapper'. Pass <code>shap_explainer_type</code> to specify, which
Explainer shall be used: {'TreeExplainer', 'DeepExplainer', 'GradientExplainer',
'LinearExplainer', 'KernelExplainer'} (default is None, which automatically
chooses an Explainer to use).
Also keyword arguments passed to one of the: shap.TreeExplainer.shap_values,
shap.DeepExplainer.shap_values, shap.GradientExplainer.shap_values,
shap.LinearExplainer.shap_values, shap.KernelExplainer.shap_values.
See <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BreakDown, Shap</code> or <code>ShapWrapper class object</code></dt>
<dd>Explanation object containing the main result attribute and the plot method.
Object class, its attributes, and the plot method depend on the <code>type</code> parameter.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p><a href="https://pbiecek.github.io/ema/breakDown.html">https://pbiecek.github.io/ema/breakDown.html</a>
<a href="https://pbiecek.github.io/ema/iBreakDown.html">https://pbiecek.github.io/ema/iBreakDown.html</a>
<a href="https://pbiecek.github.io/ema/shapley.html">https://pbiecek.github.io/ema/shapley.html</a>
<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_parts(self,
                  new_observation,
                  type=(&#39;break_down_interactions&#39;, &#39;break_down&#39;, &#39;shap&#39;, &#39;shap_wrapper&#39;),
                  order=None,
                  interaction_preference=1,
                  path=&#34;average&#34;,
                  B=25,
                  keep_distributions=False,
                  processes=1,
                  random_state=None,
                  **kwargs):
    &#34;&#34;&#34;Calculate instance level variable attributions as Break Down, Shapley Values or Shap Values

    Parameters
    -----------
    new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
        An observation for which a prediction needs to be explained.
    type : {&#39;break_down_interactions&#39;, &#39;break_down&#39;, &#39;shap&#39;, &#39;shap_wrapper}
        Type of variable attributions (default is &#39;break_down_interactions&#39;).
    order : list of int or str, optional
        Parameter specific for `break_down_interactions` and `break_down`. Use a fixed
        order of variables for attribution calculation. Use integer values  or string
        variable names (default is None, which means order by importance).
    interaction_preference : int, optional
        Parameter specific for `break_down_interactions` type. Specify which interactions
        will be present in an explanation. The larger the integer, the more frequently
        interactions will be presented (default is 1).
    path : list of int, optional
        Parameter specific for `shap`. If specified, then attributions for this path
        will be plotted (default is &#39;average&#39;, which plots attribution means for
        `B` random paths).
    B : int, optional
        Parameter specific for `shap`. Number of random paths to calculate
        variable attributions (default is 25).
    keep_distributions :  bool, optional
        Save the distribution of partial predictions (default is False).
    processes : int, optional
        Parameter specific for `shap`. Number of parallel processes to use in calculations.
        Iterated over `B` (default is 1, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).
    kwargs :
        Used only for &#39;shap_wrapper&#39;. Pass `shap_explainer_type` to specify, which
        Explainer shall be used: {&#39;TreeExplainer&#39;, &#39;DeepExplainer&#39;, &#39;GradientExplainer&#39;,
        &#39;LinearExplainer&#39;, &#39;KernelExplainer&#39;} (default is None, which automatically
        chooses an Explainer to use).
        Also keyword arguments passed to one of the: shap.TreeExplainer.shap_values,
        shap.DeepExplainer.shap_values, shap.GradientExplainer.shap_values,
        shap.LinearExplainer.shap_values, shap.KernelExplainer.shap_values.
        See https://github.com/slundberg/shap

    Returns
    -----------
    BreakDown, Shap or ShapWrapper class object
        Explanation object containing the main result attribute and the plot method.
        Object class, its attributes, and the plot method depend on the `type` parameter.

    Notes
    --------
    https://pbiecek.github.io/ema/breakDown.html
    https://pbiecek.github.io/ema/iBreakDown.html
    https://pbiecek.github.io/ema/shapley.html
    https://github.com/slundberg/shap
    &#34;&#34;&#34;

    check_data_again(self.data)

    types = (&#39;break_down_interactions&#39;, &#39;break_down&#39;, &#39;shap&#39;, &#39;shap_wrapper&#39;)
    type = check_method_type(type, types)

    if type == &#39;break_down_interactions&#39; or type == &#39;break_down&#39;:
        predict_parts_ = BreakDown(
            type=type,
            keep_distributions=keep_distributions,
            order=order,
            interaction_preference=interaction_preference
        )
    elif type == &#39;shap&#39;:
        predict_parts_ = Shap(
            keep_distributions=keep_distributions,
            path=path,
            B=B,
            processes=processes,
            random_state=random_state
        )
    elif type == &#39;shap_wrapper&#39;:
        _global_checks.global_check_import(&#39;shap&#39;, &#39;SHAP explanations&#39;)
        predict_parts_ = ShapWrapper(&#39;predict_parts&#39;)

    predict_parts_.fit(self, new_observation, **kwargs)

    return predict_parts_</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.predict_profile"><code class="name flex">
<span>def <span class="ident">predict_profile</span></span>(<span>self, new_observation, type=('ceteris_paribus',), y=None, variables=None, grid_points=101, variable_splits=None, variable_splits_type='uniform', variable_splits_with_obs=True, processes=1, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate instance level variable profiles as Ceteris Paribus</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>new_observation</code></strong> :&ensp;<code>pd.DataFrame</code> or <code>np.ndarray</code> or <code>pd.Series</code></dt>
<dd>Observations for which predictions need to be explained.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'ceteris_paribus', TODO: 'oscilations'}</code></dt>
<dd>Type of variable profiles (default is 'ceteris_paribus').</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code>, optional</dt>
<dd>Target variable with the same length as <code>new_observation</code>.</dd>
<dt><strong><code>variables</code></strong> :&ensp;<code>str</code> or <code>array_like</code> of <code>str</code>, optional</dt>
<dd>Variables for which the profiles will be calculated
(default is None, which means all of the variables).</dd>
<dt><strong><code>grid_points</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of points for profile calculations (default is 101).
NOTE: The final number of points may be lower than <code>grid_points</code>,
eg. if there is not enough unique values for a given variable.</dd>
<dt><strong><code>variable_splits</code></strong> :&ensp;<code>dict</code> of <code>lists</code>, optional</dt>
<dd>Split points for variables e.g. {'x': [0, 0.2, 0.5, 0.8, 1], 'y': ['a', 'b']}
(default is None, which means that they will be calculated using one of
<code>variable_splits_type</code> and the <code>data</code> attribute).</dd>
<dt><strong><code>variable_splits_type</code></strong> :&ensp;<code>{'uniform', 'quantiles'}</code>, optional</dt>
<dd>Way of calculating <code>variable_splits</code>. Set 'quantiles' for percentiles.
(default is 'uniform', which means uniform grid of points).</dd>
<dt><strong><code>variable_splits_with_obs</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Add variable values of <code>new_observation</code> data to the <code>variable_splits</code>
(default is True).</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>variables</code>
(default is 1, which means no parallel computation).</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Print tqdm progress bar (default is True).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>CeterisParibus class object</code></dt>
<dd>Explanation object containing the main result attribute and the plot method.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p><a href="https://pbiecek.github.io/ema/ceterisParibus.html">https://pbiecek.github.io/ema/ceterisParibus.html</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_profile(self,
                    new_observation,
                    type=(&#39;ceteris_paribus&#39;,),
                    y=None,
                    variables=None,
                    grid_points=101,
                    variable_splits=None,
                    variable_splits_type=&#39;uniform&#39;,
                    variable_splits_with_obs=True,
                    processes=1,
                    verbose=True):
    &#34;&#34;&#34;Calculate instance level variable profiles as Ceteris Paribus

    Parameters
    -----------
    new_observation : pd.DataFrame or np.ndarray or pd.Series
        Observations for which predictions need to be explained.
    type : {&#39;ceteris_paribus&#39;, TODO: &#39;oscilations&#39;}
        Type of variable profiles (default is &#39;ceteris_paribus&#39;).
    y : pd.Series or np.ndarray (1d), optional
        Target variable with the same length as `new_observation`.
    variables : str or array_like of str, optional
        Variables for which the profiles will be calculated
        (default is None, which means all of the variables).
    grid_points : int, optional
        Maximum number of points for profile calculations (default is 101).
        NOTE: The final number of points may be lower than `grid_points`,
        eg. if there is not enough unique values for a given variable.
    variable_splits : dict of lists, optional
        Split points for variables e.g. {&#39;x&#39;: [0, 0.2, 0.5, 0.8, 1], &#39;y&#39;: [&#39;a&#39;, &#39;b&#39;]}
        (default is None, which means that they will be calculated using one of
        `variable_splits_type` and the `data` attribute).
    variable_splits_type : {&#39;uniform&#39;, &#39;quantiles&#39;}, optional
        Way of calculating `variable_splits`. Set &#39;quantiles&#39; for percentiles.
        (default is &#39;uniform&#39;, which means uniform grid of points).
    variable_splits_with_obs: bool, optional
        Add variable values of `new_observation` data to the `variable_splits`
        (default is True).
    processes : int, optional
        Number of parallel processes to use in calculations. Iterated over `variables`
        (default is 1, which means no parallel computation).
    verbose : bool, optional
        Print tqdm progress bar (default is True).

    Returns
    -----------
    CeterisParibus class object
        Explanation object containing the main result attribute and the plot method.

    Notes
    --------
    https://pbiecek.github.io/ema/ceterisParibus.html
    &#34;&#34;&#34;

    check_data_again(self.data)

    types = (&#39;ceteris_paribus&#39;,)
    type = check_method_type(type, types)

    if type == &#39;ceteris_paribus&#39;:
        predict_profile_ = CeterisParibus(
            variables=variables,
            grid_points=grid_points,
            variable_splits=variable_splits,
            variable_splits_type=variable_splits_type,
            variable_splits_with_obs=variable_splits_with_obs,
            processes=processes
        )

    predict_profile_.fit(self, new_observation, y, verbose)

    return predict_profile_</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.predict_surrogate"><code class="name flex">
<span>def <span class="ident">predict_surrogate</span></span>(<span>self, new_observation, type='lime', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for surrogate model explanations</p>
<p>This function uses the lime package to create the model explanation.
See <a href="https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular">https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>new_observation</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code> or <code>pd.DataFrame (1,p)</code></dt>
<dd>An observation for which a prediction needs to be explained.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'lime'}</code></dt>
<dd>Type of explanation method
(default is 'lime', which uses the lime package to create an explanation).</dd>
</dl>
<p>kwargs :
Keyword arguments passed to the lime.lime_tabular.LimeTabularExplainer object
and the LimeTabularExplainer.explain_instance method. Exceptions are:
<code>training_data</code>, <code>mode</code>, <code>data_row</code> and <code>predict_fn</code>. Other parameters:
<a href="https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular">https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular</a></p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>lime.explanation.Explanation</code></dt>
<dd>Explanation object.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p><a href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_surrogate(self, new_observation, type=&#39;lime&#39;, **kwargs):
    &#34;&#34;&#34;Wrapper for surrogate model explanations

    This function uses the lime package to create the model explanation.
    See https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular

    Parameters
    -----------
    new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
        An observation for which a prediction needs to be explained.
    type : {&#39;lime&#39;}
        Type of explanation method
        (default is &#39;lime&#39;, which uses the lime package to create an explanation).
    kwargs :
        Keyword arguments passed to the lime.lime_tabular.LimeTabularExplainer object
        and the LimeTabularExplainer.explain_instance method. Exceptions are:
        `training_data`, `mode`, `data_row` and `predict_fn`. Other parameters:
        https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular

    Returns
    -----------
    lime.explanation.Explanation
        Explanation object.

    Notes
    -----------
    https://github.com/marcotcr/lime
    &#34;&#34;&#34;

    check_data_again(self.data)

    if type == &#39;lime&#39;:
        _global_checks.global_check_import(&#39;lime&#39;, &#39;LIME explanations&#39;)
        from lime.lime_tabular import LimeTabularExplainer
        new_observation = check_new_observation_lime(new_observation)

        explainer_dict, explanation_dict = unpack_kwargs_lime(self, new_observation, **kwargs)
        lime_tabular_explainer = LimeTabularExplainer(**explainer_dict)
        explanation = lime_tabular_explainer.explain_instance(**explanation_dict)

        return explanation</code></pre>
</details>
</dd>
<dt id="dalex.Explainer.residual"><code class="name flex">
<span>def <span class="ident">residual</span></span>(<span>self, data, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate residuals</p>
<p>This function uses the <code>residual_function</code> attribute.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Data which will be used to calculate residuals.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code></dt>
<dd>Target variable which will be used to calculate residuals.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray (1d)</code></dt>
<dd>Model residuals for given <code>data</code> and <code>y</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def residual(self, data, y):
    &#34;&#34;&#34;Calculate residuals

    This function uses the `residual_function` attribute.

    Parameters
    -----------
    data : pd.DataFrame
        Data which will be used to calculate residuals.
    y : pd.Series or np.ndarray (1d)
        Target variable which will be used to calculate residuals.

    Returns
    -----------
    np.ndarray (1d)
        Model residuals for given `data` and `y`.
    &#34;&#34;&#34;

    check_method_data(data)

    return self.residual_function(self.model, data, y)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="dalex.dataset_level" href="dataset_level/index.html">dalex.dataset_level</a></code></li>
<li><code><a title="dalex.datasets" href="datasets/index.html">dalex.datasets</a></code></li>
<li><code><a title="dalex.fairness" href="fairness/index.html">dalex.fairness</a></code></li>
<li><code><a title="dalex.instance_level" href="instance_level/index.html">dalex.instance_level</a></code></li>
<li><code><a title="dalex.wrappers" href="wrappers/index.html">dalex.wrappers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dalex.Explainer" href="#dalex.Explainer">Explainer</a></code></h4>
<ul class="two-column">
<li><code><a title="dalex.Explainer.dump" href="#dalex.Explainer.dump">dump</a></code></li>
<li><code><a title="dalex.Explainer.dumps" href="#dalex.Explainer.dumps">dumps</a></code></li>
<li><code><a title="dalex.Explainer.load" href="#dalex.Explainer.load">load</a></code></li>
<li><code><a title="dalex.Explainer.loads" href="#dalex.Explainer.loads">loads</a></code></li>
<li><code><a title="dalex.Explainer.model_diagnostics" href="#dalex.Explainer.model_diagnostics">model_diagnostics</a></code></li>
<li><code><a title="dalex.Explainer.model_fairness" href="#dalex.Explainer.model_fairness">model_fairness</a></code></li>
<li><code><a title="dalex.Explainer.model_parts" href="#dalex.Explainer.model_parts">model_parts</a></code></li>
<li><code><a title="dalex.Explainer.model_performance" href="#dalex.Explainer.model_performance">model_performance</a></code></li>
<li><code><a title="dalex.Explainer.model_profile" href="#dalex.Explainer.model_profile">model_profile</a></code></li>
<li><code><a title="dalex.Explainer.model_surrogate" href="#dalex.Explainer.model_surrogate">model_surrogate</a></code></li>
<li><code><a title="dalex.Explainer.predict" href="#dalex.Explainer.predict">predict</a></code></li>
<li><code><a title="dalex.Explainer.predict_parts" href="#dalex.Explainer.predict_parts">predict_parts</a></code></li>
<li><code><a title="dalex.Explainer.predict_profile" href="#dalex.Explainer.predict_profile">predict_profile</a></code></li>
<li><code><a title="dalex.Explainer.predict_surrogate" href="#dalex.Explainer.predict_surrogate">predict_surrogate</a></code></li>
<li><code><a title="dalex.Explainer.residual" href="#dalex.Explainer.residual">residual</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>