<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="DALEX: Descriptive mAchine Learning EXplanations" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Do not trust a black-box model. Unless it explains itself." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Przemysław Biecek" />

<meta name="date" content="2018-04-12" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Do not trust a black-box model. Unless it explains itself.">

<title>DALEX: Descriptive mAchine Learning EXplanations</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li class="has-sub"><a href="1-1-motivation.html#motivation"><span class="toc-section-number">1.1</span> Motivation</a><ul>
<li><a href="1-1-motivation.html#why-dalex"><span class="toc-section-number">1.1.1</span> Why DALEX?</a></li>
<li><a href="1-1-motivation.html#to-validate"><span class="toc-section-number">1.1.2</span> To validate</a></li>
<li><a href="1-1-motivation.html#to-understand"><span class="toc-section-number">1.1.3</span> To understand</a></li>
<li><a href="1-1-motivation.html#to-improve"><span class="toc-section-number">1.1.4</span> To improve</a></li>
</ul></li>
<li><a href="1-2-trivia.html#trivia"><span class="toc-section-number">1.2</span> Trivia</a></li>
</ul></li>
<li class="has-sub"><a href="2-architecture.html#architecture"><span class="toc-section-number">2</span> Architecture of DALEX</a><ul>
<li><a href="2-1-explainFunction.html#explainFunction"><span class="toc-section-number">2.1</span> The <code>explain()</code> function</a></li>
<li class="has-sub"><a href="2-2-useCaseApartmetns.html#useCaseApartmetns"><span class="toc-section-number">2.2</span> Use case: Regression. Apartment prices in Warsaw</a><ul>
<li><a href="2-2-useCaseApartmetns.html#model-1-linear-regression"><span class="toc-section-number">2.2.1</span> Model 1: Linear regression</a></li>
<li><a href="2-2-useCaseApartmetns.html#model-2-random-forest"><span class="toc-section-number">2.2.2</span> Model 2: Random forest</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="3-modelUnderstanding.html#modelUnderstanding"><span class="toc-section-number">3</span> Model understanding</a><ul>
<li><a href="3-1-modelPerformance.html#modelPerformance"><span class="toc-section-number">3.1</span> Model performance</a></li>
<li class="has-sub"><a href="3-2-featureImportance.html#featureImportance"><span class="toc-section-number">3.2</span> Feature importance</a><ul>
<li><a href="3-2-featureImportance.html#modelAgnostic"><span class="toc-section-number">3.2.1</span> Model agnostic</a></li>
<li><a href="3-2-featureImportance.html#modelSpecific"><span class="toc-section-number">3.2.2</span> Model specific</a></li>
</ul></li>
<li class="has-sub"><a href="3-3-variableResponse.html#variableResponse"><span class="toc-section-number">3.3</span> Variable response</a><ul>
<li><a href="3-3-variableResponse.html#pdpchapter"><span class="toc-section-number">3.3.1</span> Partial Dependence Plot</a></li>
<li><a href="3-3-variableResponse.html#accumulatedLocalEffects"><span class="toc-section-number">3.3.2</span> Accumulated Local Effects Plot</a></li>
<li><a href="3-3-variableResponse.html#mergingPathPlot"><span class="toc-section-number">3.3.3</span> Mering Path Plot</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="4-predictionUnderstanding.html#predictionUnderstanding"><span class="toc-section-number">4</span> Prediction understanding</a><ul>
<li><a href="4-1-outlierDetection.html#outlierDetection"><span class="toc-section-number">4.1</span> Outlier detection</a></li>
<li><a href="4-2-predictionBreakdown.html#predictionBreakdown"><span class="toc-section-number">4.2</span> Prediction breakDown</a></li>
</ul></li>
<li><a href="5-epilogue.html#epilogue"><span class="toc-section-number">5</span> Epilogue</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="motivation" class="section level2">
<h2><span class="header-section-number">1.1</span> Motivation</h2>
<p><em>Machine Learning</em> is a vague name. There is some <em>learning</em> and some <em>machines</em>, but what the heck is going on? What does it really mean? Is it possible that the meaning of this term evolves over time?</p>
<ul>
<li>A few years ago I would say that the term refers to <em>machines learning from humans</em>. In the supervised learning problems, a human being creates a labeled dataset and machines are tuned/trained to predict correct labels from data.</li>
<li>Recently we have more and more examples of <em>machines that are learning from other machines</em>. Self-playing neural nets like AlphaGo Zero <span class="citation">(Silver et al. <label for="tufte-mn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle">2017<span class="marginnote">Silver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, et al. 2017. “Mastering the Game of Go Without Human Knowledge.” <em>Nature</em> 550 (7676):354–59. <a href="https://doi.org/10.1038/nature24270" class="uri">https://doi.org/10.1038/nature24270</a>.</span>)</span> learn from themselves with blazing speed. Humans are involved in designing the learning environment but the labeling turns out to be very expensive or not feasible, and we are looking for other ways to learn from partial labels, fuzzy labels, or no labels at all.</li>
<li>I could imagine that in close future <em>humans will learn from machines</em>. Well trained black-boxes may teach us how to be better at playing Go, how to be better at reading PET images (Positron-Emission Tomography images), or how to be better at diagnosing patients.</li>
</ul>
<p>As the human supervision over learning is decreasing over time, the understanding of black-boxes is more important. To make this future possible, we need tools that extract useful information from black-box models.</p>
<p>DALEX is <em>the tool</em> for this.</p>
<div id="why-dalex" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Why DALEX?</h3>
<p>In recent years we have been observing an increasing interest in tools for knowledge extraction from complex machine learning models, see <span class="citation">(Štrumbelj and Kononenko <label for="tufte-mn-4" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle">2011<span class="marginnote">Štrumbelj, Erik, and Igor Kononenko. 2011. “A General Method for Visualizing and Explaining Black-Box Regression Models.” In <em>Proceedings of the 10th International Conference on Adaptive and Natural Computing Algorithms - Volume Part Ii</em>, 21–30. ICANNGA’11. Berlin, Heidelberg: Springer-Verlag. <a href="http://dl.acm.org/citation.cfm?id=1997005.1997009" class="uri">http://dl.acm.org/citation.cfm?id=1997005.1997009</a>.</span>)</span>, <span class="citation">(Tzeng and Ma <label for="tufte-mn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle">2005<span class="marginnote">Tzeng, F. Y., and K. L. Ma. 2005. “Opening the Black Box - Data Driven Visualization of Neural Networks.” In <em>VIS 05. IEEE Visualization, 2005.</em>, 383–90. <a href="https://doi.org/10.1109/VISUAL.2005.1532820" class="uri">https://doi.org/10.1109/VISUAL.2005.1532820</a>.</span>)</span>, <span class="citation">(Puri et al. <label for="tufte-mn-6" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle">2017<span class="marginnote">Puri, N., P. Gupta, P. Agarwal, S. Verma, and B. Krishnamurthy. 2017. “MAGIX: Model Agnostic Globally Interpretable Explanations.” <em>ArXiv E-Prints</em>, June.</span>)</span>, <span class="citation">(Zeiler and Fergus <label for="tufte-mn-7" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">2014<span class="marginnote">Zeiler, Matthew D., and Rob Fergus. 2014. “Visualizing and Understanding Convolutional Networks.” In <em>Computer Vision – Eccv 2014</em>, edited by David Fleet, Tomas Pajdla, Bernt Schiele, and TinneEditors Tuytelaars, 818–33. Springer International Publishing.</span>)</span>.</p>
<p>There are some very useful R packages that may be used for knowledge extraction from R models, see for example <code>pdp</code> <span class="citation">(Greenwell <label for="tufte-mn-8" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle">2017<span class="marginnote">Greenwell, Brandon M. 2017. “Pdp: An R Package for Constructing Partial Dependence Plots.” <em>The R Journal</em> 9 (1):421–36. <a href="https://journal.r-project.org/archive/2017/RJ-2017-016/index.html" class="uri">https://journal.r-project.org/archive/2017/RJ-2017-016/index.html</a>.</span>)</span>, <code>ALEPlot</code> <span class="citation">(Apley <label for="tufte-mn-9" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle">2017<span class="marginnote">Apley, Dan. 2017. <em>ALEPlot: Accumulated Local Effects (Ale) Plots and Partial Dependence (Pd) Plots</em>. <a href="https://CRAN.R-project.org/package=ALEPlot" class="uri">https://CRAN.R-project.org/package=ALEPlot</a>.</span>)</span>, <code>randomForestExplainer</code> <span class="citation">(Paluszynska and Biecek <label for="tufte-mn-10" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-10" class="margin-toggle">2017<span class="marginnote">Paluszynska, Aleksandra, and Przemyslaw Biecek. 2017. <em>RandomForestExplainer: A Set of Tools to Understand What Is Happening Inside a Random Forest</em>. <a href="https://github.com/MI2DataLab/randomForestExplainer" class="uri">https://github.com/MI2DataLab/randomForestExplainer</a>.</span>)</span>, <code>xgboostExplainer</code> <span class="citation">(Foster <label for="tufte-mn-11" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-11" class="margin-toggle">2017<span class="marginnote">Foster, David. 2017. <em>XgboostExplainer: An R Package That Makes Xgboost Models Fully Interpretable</em>. <a href="https://github.com/AppliedDataSciencePartners/xgboostExplainer/" class="uri">https://github.com/AppliedDataSciencePartners/xgboostExplainer/</a>.</span>)</span>, <code>live</code> <span class="citation">(Staniak and Biecek <label for="tufte-mn-12" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-12" class="margin-toggle">2017<span class="marginnote">Staniak, Mateusz, and Przemyslaw Biecek. 2017. <em>Live: Local Interpretable (Model-Agnostic) Visual Explanations</em>. <a href="https://github.com/MI2DataLab/live" class="uri">https://github.com/MI2DataLab/live</a>.</span>)</span> and others.</p>
<p>Do we need yet another R package to better understand ML models? I think so. There are some features available in the DALEX package which make it unique.</p>
<ul>
<li>Scope. DALEX is a wrapper for a large number of very good tools / model explainers. It offers a wide range of state-of-the-art techniques for model exploration. Some of these techniques are more useful for understanding model predictions; other techniques are more handy for understanding model structure.</li>
<li>Consistency. DALEX offers a consistent grammar across various techniques for model explanation. It’s a wrapper that smoothes differences across different R packages.</li>
<li>Model agnostic. DALEX explainers are model agnostic. One can use them for linear models, tree ensembles, or other structures, hence we are not limited to any particular family of black-box models.</li>
<li>Model comparisons. One can learn a lot from a single black-box model, but one can learn much more by contrasting models with different structures, like linear models with ensembles of trees. All DALEX explainers support model comparisons.</li>
<li>Visual consistency. Each DALEX explainer can be plotted with the generic <code>plot()</code> function. These visual explanations are based on <code>ggplot2</code> <span class="citation">(Wickham <label for="tufte-mn-13" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-13" class="margin-toggle">2009<span class="marginnote">Wickham, Hadley. 2009. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. <a href="http://ggplot2.org" class="uri">http://ggplot2.org</a>.</span>)</span> package, which generates elegant, customizable, and consistent graphs.</li>
</ul>
<p>Chapter <a href="2-architecture.html#architecture">2</a> presents the overall architecture of the DALEX package. Chapter <a href="3-modelUnderstanding.html#modelUnderstanding">3</a> presents explainers that explore global model performance and variable importance of feature effects. Chapter <a href="4-predictionUnderstanding.html#predictionUnderstanding">4</a> presents explainers that explore feature attribution for single predictions of validation of a model prediction’s reliability.</p>
<p>In this document we focus on three primary use-cases for DALEX explainers.</p>
</div>
<div id="to-validate" class="section level3">
<h3><span class="header-section-number">1.1.2</span> To validate</h3>
<p>Explainers presented in Section <a href="3-1-modelPerformance.html#modelPerformance">3.1</a> help in understanding model performance and comparing performance of different models.</p>
<p>Explainers presented in Section <a href="4-1-outlierDetection.html#outlierDetection">4.1</a> help to identify outliers or observations with particularly large residuals.</p>
<p>Explainers presented in Section <a href="4-2-predictionBreakdown.html#predictionBreakdown">4.2</a> help to understand which key features influence model predictions.</p>
</div>
<div id="to-understand" class="section level3">
<h3><span class="header-section-number">1.1.3</span> To understand</h3>
<p>Explainers presented in Section <a href="3-2-featureImportance.html#featureImportance">3.2</a> help to understand which variables are the most important in the model. Explainers presented in Section <a href="4-2-predictionBreakdown.html#predictionBreakdown">4.2</a> help to understand which features influence single predictions. They are useful in identifying key influencers behind the black-box.</p>
<p>Explainers presented in Section <a href="3-3-variableResponse.html#variableResponse">3.3</a> help to understand how particular features affect model prediction.</p>
</div>
<div id="to-improve" class="section level3">
<h3><span class="header-section-number">1.1.4</span> To improve</h3>
<p>Explainers presented in Section <a href="3-3-variableResponse.html#variableResponse">3.3</a> help to perform feature engineering based on model conditional responses.</p>
<p>Explainers presented in Section <a href="4-2-predictionBreakdown.html#predictionBreakdown">4.2</a> help to understand which variables result in incorrect model decisions. These explainers are useful in identifying and correcting biases in the training data.</p>
</div>
</div>
<p style="text-align: center;">
<a href="index.html"><button class="btn btn-default">Previous</button></a>
<a href="https://github.com/pbiecek/DALEX/edit/master/index.Rmd"><button class="btn btn-default">Edit</button></a>
<a href="1-2-trivia.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
