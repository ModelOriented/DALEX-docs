Welcome to the Explanatory Model Analysis podcast.
This podcast is about various techniques for explainable artificial intelligence. 

This episode is an introduction to the topic of visualization, exploration and explanation of machine learning models. I will show why this topic is important and how it can be used to build better predictive models.

This and other episodes along with complementary materials are available at tiny.cc/DALEX.
If you have any question or comment related to this topic please let me know. 
You will find my contact details at the bottom of the page and on the final screen.



Why the Explanatory Model Analysis is important?

Predictive models have been developed for years. So you're surely asking yourself why now so much attention is being paid to techniques for model exploration. There are several reasons for this.


Models are getting more complex.

The availability of large data sets and fast computers allow you to train more and more complex models. 
So complex that they cannot be analyzed by looking directly at model parameters.  
It is common to work with models that have thousands or millions of parameters.
We need tools to analyze these huge models.

Right to explanation.

Predictive models decide which ads we see on social media, which books, films or music are recommended for us.
Automated models win elections, drive cars, diagnose patients and decide who gets and who doesn't get credit.
The increasing automation is increasingly affecting our daily lives. 
This has led to changes in the law to protect citizens' civil rights.  Many countries are introducing so-called 'Right to Explanation' into their legal systems. This is the right to an explanation for a decision that has been made automatically by an algorithm.
We need tools to provide such explanations.

Ethical issues.

The analysis of machine learning models in some cases has shown that the models lead to biased decisions. 
Like models for recidivism that discriminate on the basis of skin colour or models for credit scoring that discriminate on the basis of age or gender.
Usually, this bias is learned from historical data. In complex models, it is difficult to monitor if the bias occurs and how strong it is.
We need tools to identify or correct such biases.

Model debugging.

The more complex the model, the harder it is to understand the reasons leading to wrong decisions. And we can't fix what we don't understand.
We need tools to support post-hoc analysis of wrong predictions.

Trust and model human interaction.

Machine learning models are often designed to support human decision-making. For the human operator, it is necessary to trust and understand the reasons behind the model recommendation. 
We need tools for better communication between the model and the human.

Explanatory Model Analysis for models is what Exploratory Data Analysis for data.



Three approaches to explainability.

The need to understand how models work is not new. Tools for model diagnostics have been accompanying model development for years.
We can distinguish three major approaches to the topic of model explainability.

Interpretable by design.

One approach is to use only models that have a simple structure and can be directly understood. For example models with a small number of variables or shallow decision trees or methods based on decision rules. This approach usually is time-consuming and requires expert knowledge for the selection and engineering of variables. A simple structure of a model allows for direct analysis.

For complex models, we cannot analyze individual parameters.  For some types of models, special tools have been developed to analyze and visualize their structures. This approach is called a model-specific. Their functionality is based on exploring the model structure. Examples of such tools are diagnostic plots for linear models, node statistics for random forest models or integrated gradients for deep neural networks. 

In this podcast, we focus on the model-agnostic approach. In the model-agnostic approach we do not assume anything about the internal structure of the model. It is analysed on the black box principle. We only explore the relationship between the input and output of the model without caring about its internal structure. Such methods can be applied to any model and also allows to compare different models.


How EMA may improve the model development?

EMA methods can be applied in any part of the model development cycle.
A typical process of building a predictive model is based on three steps repeated. Deepen the understanding of data, prepare a new version of the models and assess the quality of the model. 

In each of these stages, we can use EMA methods. 
Pre-modelling applications are aimed at extracting new knowledge from the model. 
In-modelling applications are aimed at debugging models and fixing their mispredictions. 
Post-modelling applications are aimed at explaining model predictions.


The pyramid for Explanatory Model Analysis

The methods we will discuss in this podcast are arranged in a pyramid-like ontology.

We will discuss two classes of methods. 
The first one focuses on the analysis of the model result for a single observation. 
These methods are called instance-level explanations or local explanations. 
For example, for a model that estimates odds of default in credit scoring, the instance level analysis focuses on explaining the model's behaviour for one particular client. 

The second group of methods is focused on the analysis of the model's behaviour on the whole data set. We are not interested in any particular prediction, but in average model behaviour.
Such tools are called dataset level explanations or global explanations. 
For example, for a model that estimates odds of default in credit scoring, the model level analysis focuses on explaining the model's behaviour for a selected population of customers. 



The explanatory pyramid is composed of levels. In each subsequent one, we explore more and more deeply the behaviour of the model.

In the first level are raw model predictions and model statistics, which can be described by single numbers like model performance measures.

The second level is related to the analysis of model parts. We want to understand which variables have the greatest influence on model behaviour.

The third level describes the profile of model behaviour depending on the values of individual variables.

The fourth level corresponds to the residual diagnostics. We want to understand how good is the model fit to the analyzed data.


The methods listed in this pyramid are described in detail in the ebook Exploratory Model Analysis, available at github.com/pbiecek/ema. 
In this ebook, for each method you will find the intuition, methodological details and examples for different classification and regression models.

The consecutive episodes in this podcast show how to perform EMA analysis using the DALEX library for R or Python. Subsequent episodes are divided into two tracks, one with examples for the R program and the second for Python. 

The list of all episodes is available on the website tiny.cc/DALEX.
If you have any question or comment related to this topic please let me know. 












Welcome to the second episode of Explanatory Model Analysis podcast.
This podcast is about various techniques for explainable artificial intelligence. 
In this episode, I will overview the workflow of model exploration with R and introduce the DALEX package.

If you are interested in examples for Python or if you are looking for a specific XAI method you will find other episodes at tiny.cc/DALEX webpage.

Before we jump into the R code, let's see the overall architecture for the DrWhy AI family.
In this podcast, we describe only tools that are model agnostic. This means that we want them to work for models with different structures and even for models created in different programming languages. 
Such tools work on the principle of black box exploration. They don't have to know the internal structure of the model. They just need a function to calculate predictions for some perturbed data.
The problem that we need to overcome is the lack of coherence between different libraries in the way they calculate predictions.
There are many libraries in R and Python to train predictive models. But different libraries have different interfaces.
To handle different models in a uniform way we need a universal adapter that will create a uniform interface around any model. This is the role of the explain function from the DALEX package.
The result of this function is a new object of the class explainer.
It doesn't matter what library we used to build the model. The explain function creates a consistent interface for performing predictions on the model.

All functions in DALEX package and other packages from the DrWhy AI family can work on explainer objects without worrying about the original structure of the model.




Let's see an example for this workflow. 
We'll use the dataset for the Sinking of the Titanic. This is a popular dataset for the binary classification task. We have seven variables. Three qualitative like class or gener and four quantitative like age and fare. Based on them we want to predict the chances of surviving the Titanic disaster.

In the DALEX package, we have a titanic imputed data set with preprocessed data for 2207 people. In this example, we use a random forest model implemented in the ranger package. But the tools described and the workflow work for any model. 


Let's start with typical use of explain function.
The first argument is the model. This is the only mandatory argument. Useful optional arguments are the data to validate, the corresponding values of the target variable and the label - the name of the model.

Depending on the class of the model, the explain() function extracts the relevant information from the model and creates an adapter with a uniform interface.

It may happen that the explain function will not recognize the model class and will not know how to calculate predictions for it. In such situations, the predict function argument can be used to directly indicate how to calculate predictions for a particular model. 


Each explainer has the same fields that can be used by other functions to explore the encapsulated model.
Model is stored in the slot model.
The model info slot contains information about the name and version of the package in which the model was built.
The slot data stores validation data.
The slot prediction function has a function that calculate numeric scores for the embedded model.
These predictions are stored in the y hat slot and the corresponding residuals are in the residuals slot.




Such adapter can be used in any other function in the DALEX package.
The general workflow is following.
Each function transforms an explainer object into an explanation object. And then the explanation can be visualised with the generic plot function.
These functions are discussed in detail in the following episodes.



The instance-level explanations are described in the first part of this podcast.

Episodes 3 and 4 are devoted to the decomposition of one prediction into parts that can be assigned to specific variables. This can be done using the predict parts function.

Episode 5 is devoted to the model response profile for selected variables calculated with Ceteris Paribus principle. This profile can be examined using the predict profile function.

The dataset level explanations are described in the second part of the podcast.

Episode 6 is devoted to the assessment of the predictive power of a model. The model performance function can be used to calculate and plot different performance measures.

Episode 7 is devoted to the analysis of the importance of variables in the model. The model parts function can be used for calculation and visualisation of variable importance.

Episode 8 is devoted to the analysis of the average response profile of the model for selected variables. This response may be calculated by the model profile function.

Episode 9 is devoted to the residual diagnostics. 



The results of each explanation can be drawn with the plot function. You can also use this function to compare explanations for two or more models. This functionality is especially useful when you need to select one model out of a few candidate models. 

Let's show it with an example. For the titanic data, we build a second model using rms package. This time it is a logistic regression model with splines for age. Using the explain function we build a wrapper around the model.

We can now plot the results for both models in a single chart.
We use the model performance function to draw ROC curves for both models. Similarly, we can draw two LIFT curves. In this case, the random forest model seems to be better on average.
Now we use the 'model profile' function to draw 'partial dependence curves' for both models. Thanks to this we can compare them in detail by tracking the model's behaviour in reaction to selected variables.


We can also compare models trained in different languages. Using DALEXtra package we can cross-compare a model trained in R with a model trained in s-k-learn in Python with a model trained in H-2-O. We can also compare models across different frameworks like mlr, caret, keras or parsnip.


Adapters built with explain function can be used in any package from DrWhy AI family, such as ingredients, iBreakdown, auditor or drifter.




Let's look at two examples.

The modelDown package automatically creates a static html page for one or more models with essential dataset level explanations. 
The page is built automatically. In subsequent sections, it has information such as the importance of variables, description of models performance and model response profiles to individual variables.
The modelDown package works for models of any structure, because of the uniform interface created by the explain function.
Such generated HTML pages can be used to document the models as well as allow for easy model archivisation.


Another interesting solution from DrWhy family is the modelStudio package. 
It automates the Explanatory Analysis of Machine Learning predictive models. 
Generate advanced interactive and animated model explanations in the form of a serverless HTML site with only one line of code.
The main function computes various model explanations and produces an interactive, customisable dashboard made with D3 js. It consists of multiple panels for plots with their short descriptions. 
With this dashboard, we can cross-compare plots for data exploration and model exploration at the level of one observation or the whole data set.


The central point of all these solutions is the explain function. To work in a model agnostic way we need to have adapters with a uniform interface regardless of which tool was used for model training.
In the following episodes, we will show in detail how to build individual explanations from such an adapter.

The list of all episodes is available on the website tiny.cc/DALEX.
If you have any question or comment related to this topic please let me know. 














This is the Explanatory Model Analysis podcast about various techniques for explainable artificial intelligence. I describe methods for visualization, exploration and explanation of machine learning models.


This episode is about the Shapley values and Break-Down with interaction.
I will show you how to use these methods with the DALEX package for the R language.
If you are interested in examples for Python or other XAI methods you can find other episodes at tiny.cc/DALEX webpage.

Before we jump into the R code, let me summarise how these methods are different from Break Down method presented in Episode 3.


All these methods are ‘model agnostic’ and 'instance-specific'.  It means that they work for any model without assuming anything of its internal structure. And they explain the model result for a single observation. 

The Break-Down method is order specific. It means that for the different ordering of variables we may get different explanations.
In default settings a greedy heuristic is used to identify the best order.

Shapley values are defined as an average of variable attributions across all possible orderings. The number of orderings may be large but we may estimate these values based on some random orderings.
This way differences between different orderings will be average out.


The method 'Break down with interactions' identifies pairs of variables which are the source of the differences across orderings. It seeks for interactions between pairs of variables.  Such interactions are then presented on waterfall plots. 


Let's see how to calculate these different methods in R.
In this video, we are focused on the software. If you want to learn more about the methods go to the ebook at pbiecek.github.io/ema.



As in the previous episode, I will use the data for Singkinf of RMS Titanic and a classification random forest model trained with the ranger function. 
The model is trained to predict the chances of survival based on avaliable variables like gender, age and class. 


All three methods described in this episode work on the instance level. 
As the instance of interest we use Henry, a 15-years old boy from 2nd class.


The 'predict parts' function by default calculates break down attributions. 
There is an interaction of fare and class variables for Henry.
Depending on the order one of these variables has a positive effect while the other has negative.

If you want to calculate Shapley values, set the argument type to 'shap'. Optionally, set the B argument to specify the number of random orderings that shall be used for calculations.


The result is a data frame with the attributions for different orderings. It is best to visualise this result with the generic plot function. For each variable, the Shapley values are shown with coloured bars. Additionally, boxplots show how the attributions for individual variables look for different orderings. In this model, we have interactions of fare and class, so the boxplots for this pair are very wide.

If we want to draw Shapley values without boxplots then in plot function we should set the argument 'show boxplots' to FALSE.


This is the gg-plot object so you can use other gg-plot-2 functions to modify the plot. Like 'ggtitle' to add a title or `theme` to change the skin.


To identify interactions, the predict parts function must set the type argument for 'break down interactions'.

The result is also a data frame. As in previous cases, we can visualise it with the generic plot function.




The general workflow for the predict part function is summarised in this sequence diagram. 
Depending on the type argument, the function calculates break down profiles, break down with interaction or Shapley values.
The result can be further processed with the generic plot function.


Find more examples and more details in the Exploratory Model Analysis ebook.


This episode is about the Ceteris Paribus profiles.


In this episode, I will show you how to use the Ceteris Paribus profiles implemented in the DALEX package for the R language.
If you are interested in examples for Python or other XAI methods you can find other episodes at tiny.cc/DALEX webpage.

Before we jump into the R code, let me remind you what Ceteris Paribus profiles are for. Like Break Down and Shapley values, it's a ‘model agnostic’ and 'instance-specific' method.  It means that it works for any model without assuming anything of its internal structure. And it explains the result of the model for a single observation. 

Ceteris paribus is a Latin phrase meaning 'other things held constant' or 'all else unchanged'. CP profiles show how the model response would change if values for a single variable is changed for the instance of interest. 

This technique may be used for sensitivity analysis or to what-if model analysis.

In this slide, we have an example for a single passenger, the 8-year-old boy from the first class. The model prediction for him is 0.466. 
Keeping all other variables unchanged, increasing the age lowers the expected chance of survival. 
Changing the gender to a woman would increase the predicted chances of survival. 
For such a young age a change of class would not change chances of survival. 


It is important to remember that we need a causal model to do causal inference for real-world phenomena. CP profiles are about the behaviour of the model, which may not be transferred to the behaviour of the analysed process.


Let's see how to do such ceteris paribus profiles in R.

This video is focused on the software. If you want to learn more about the method find more details on pbiecek.github.io/ema


As in other episodes, I will use the titanic data and a classification random forest model trained with the ranger library. 
Note that the ceteris-paribus method is model agnostic, so you can try the same workflow with any other model you wish.


Different models have different structures. We use the explain function from the DALEX package to create a model adapter with unified A-P-I. You can find more details about this function in Episode 2.



The ceteris paribus profiles work on the instance level. So we need an observation for which we can generate the explanation.
Here we create a data frame with a single row that corresponds to an eight years old boy from 1st class.

The model prediction for this boy is 0.465.


Once we have the model and the instance of interest, we are ready for the ceteris paribus method.

Here we use the `predict profile` function from the `DALEX` package.
The name of the function means that we analyse response profiles for a prediction.

In the basic settings, this function expects two arguments. The explainer and one or more observations.
The result is a data frame with profiles for each variable used in the model. 
One can limit the number of variables for which profiles are calculated with the optional argument `variables`.


A plot is easier to read than a table with numbers. You can use the generic plot function to visualise the model response. Each variable is presented in a different panel.

Many variables can obfuscate the plot.  The argument variables can be used to specify which variables shall be presented.

This is the gg-plot object so you can use ggplot2 functions to change the plot. For example 'ggtitle' to add a title or `theme` to change the skin.


ggplot2 does not allow to draw continuous and categorical variables simultaneously. By default, only continuous variables are drawn. To plot categorical variables, you have to list them directly in the argument variables.


By default, the 'predict profile' function calculates the model response for a small grid of points. If you want a more detailed profile, you may manually specify the grid of points in which the profiles are to be calculated. In this case, you should indicate a list with grids of points using the argument 'variable splits'.
For a random forest model, the response is a step function. But you need a very dense grid of points to see these steps.



The 'predict profile' function calculates profiles for one or more observations. In this example, we calculate and plot profiles for John and Mary. 
If the model was additive, the ceteris paribus profiles for observations would be parallel. 
But the random forest model is not additive.  
For John the higher is the age the lower are model predictions.
For Mary, regardless of the age, the predictions of the model are very high.


The ceteris paribus profiles can be used to compare different predictive models.
To show this functionality we will use the lrm function from the rms package to build a logistic regression model with splines. Then we transform this model with the predict profile function and visualise the profile with the plot function.
We see that the logistic regression model for John gives much higher predictions. The response curve of the model is also much smoother, although in shape similar to a random forest.



The general workflow with the Ceteris-paribus method is summarised in this sequence diagram. 
Once you have a model you need to create a model adapter.
To calculate ceteris paribus profiles you can use the 'predict profile' function.
The result can be visualised with the generic plot function.







Unverified black box model is the path to the failure. Opaqueness leads to distrust. Distrust leads to ignoration. Ignoration leads to rejection.

The DALEX package xrays any model and helps to explore and explain its behaviour, helps to understand how complex models are working. The main function explain creates a wrapper around a predictive model. Wrapped models may then be explored and compared with a collection of local and global explainers. Recent developents from the area of Interpretable Machine Learning / eXplainable Artificial Intelligence.

The philosophy behind DALEX explanations is described in the Explanatory Model Analysis e-book. The DALEX package is a part of DrWhy.AI universe.







In this episode, I will show you how to use the Break-Down method implemented in the lately published DALEX package for the Python language.
If you are interested in examples for R or other XAI methods you can find other episodes at tiny.cc/DALEX webpage.

Before we jump into the Python code, let me remind you what Break Down is for. 




Let's see how to do such break-down in Python.


We will drop rows with missing values to simplify the model and encode target variable to 0s and 1s, where 1 is `survived`. 


Due to categorical variables in the dataset, we need to include some preprocessing in our model. We split preprocessing into two parts, first one hot encode categorical variables, second scale numerical ones. Then we combine this in pipeline with actual model - gradient boosting machine. All used in construction algorithms are taken from the scikit-learn package. 


The model is trained to predict the chances of survival based on eight variables. 

Here we construct the Explainer object from dalex package.

Here we create a pandas data frame with a single row that corresponds to an eight years old boy from 1st class. Note that this has to be a pandas data frame, not series or numpy array.

The model prediction for this boy is 0.81.


Here we use the `predict parts` method of the Explainer object.

In the basic settings, this method expects two arguments. The new observation and the type of attribution. By default, the type is equal to `i break down` that will be covered in the future episodes. However, we will put type as `break down`.
The result is a newly constructed object of the `BreakDown` class with field called `result` which is a data frame with attributions for each variable in the model.



To calculate break down attributions you can use the predict_parts method with the type = break down.
The result can be further processed with the plot method.
