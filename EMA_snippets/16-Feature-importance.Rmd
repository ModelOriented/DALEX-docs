---
title: "Shapley Additive Explanations (SHAP) for Instance Level Attributions"
subtitle: "Code snippets for R "
author: "Przemys≈Çaw Biecek"
date: "for DALEX 1.0"
output: 
  tint::tintHtml:
    toc: true
link-citations: yes
---

```{r setup, include=FALSE}
library(tint)
set.seed(1313)
```

In this section, we use an `DALEX::predict_parts()` function for calculation of Shapley Values.
Note that there are also other R packages that offer similar functionality, like `shapper` which is a wrapper for the Python library `SHAP`, and `iml`.

If you want learn more about Shapley Values read https://pbiecek.github.io/ema/shapley.html.

# Prepare data

In this example we will use the titanic data.
It has few variables that are easy to understand.
The `titanic_imputed` dataset from `DALEX` package has imputed missing values.
Find more about the data https://pbiecek.github.io/ema/dataSetsIntro.html#TitanicDataset

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(titanic_imputed)
```

# Train a model

Here we use `ranger` library to train a classification random forest model.

```{r, warning=FALSE, message=FALSE}
library("ranger")
titanic_rf <- ranger(survived ~ class + gender + age + sibsp + 
         parch + fare + embarked, data = titanic_imputed,
         probability = TRUE,
         classification = TRUE)
titanic_rf
```

# Prepare an explainer

Different models have different structures.
We use `DALEX::explain()` function to create an uniform interface for the model 

```{r, warning=FALSE, message=FALSE}
library("DALEX")
titanic_ex <- explain(titanic_rf,
                data  = titanic_imputed,
                y     = titanic_imputed$survived,
                label = "Regression Forest for Titanic")
```

# Prepare an instance

For instance level explanations we need an observation for which we can generate explanation.

Let's create a data.frame with a single row that corresponds to 8 years old boy from 1st class.

```{r, warning=FALSE, message=FALSE}
johny_d <- data.frame(
            class = factor("1st", levels = c("1st", "2nd", "3rd", "deck crew",
                        "engineering crew", "restaurant staff", "victualling crew")),
            gender = factor("male", levels = c("female", "male")),
            age = 8,
            sibsp = 0,
            parch = 0,
            fare = 72,
            embarked = factor("Southampton", levels = c("Belfast",
                        "Cherbourg","Queenstown","Southampton")))
```

The predicted survival for `johny_d` is

```{r, warning=FALSE, message=FALSE}
predict(titanic_ex, johny_d)
```

# Shapley values with `predict_parts()` 

The `DALEX::predict_parts()` function calculates the variable attributions for a selected model and the instance of interest. 

For Shapley values this function requires four arguments: 

* the model explainer, 
* the data frame with the instance of interest, 
* the method for calculation of variable attribution, here `shap` and
* number of random orders that are used for Shap calculations. By default `B = 25`.

The resulting object is a data frame with variable contributions computed for every `B` ordering. Applying the generic function `plot()` to the object constructs the plot that includes the Shapley values and the corresponding box-plots.

```{r, warning=FALSE, message=FALSE}
shap_johny <- variable_attribution(titanic_ex, 
                                   johny_d, 
                                   type = "shap",
                                   B = 25)
shap_johny
```

To obtain a plot with only Shapley values, we can use the generic `plot()` function.

```{r, warning=FALSE, message=FALSE}
plot(shap_johny) 
```

To obtain a plot with only Shapley values, we can use the `show_boxplots=FALSE` argument in the `plot()` function call.

```{r, warning=FALSE, message=FALSE}
plot(shap_johny, show_boxplots = FALSE) 
```

It is the `ggplot2` object, so one can modify it as any other `ggplot2` object.

```{r, warning=FALSE, message=FALSE}
library("ggplot2")
plot(shap_johny, show_boxplots = FALSE) +
  ggtitle("Shapley values for Johny D") +
  theme(panel.grid = element_blank())
```



















```{r load_models_VI, warning=FALSE, message=FALSE, echo=FALSE}
source("models/models_titanic.R")
```

# Variable's Importance {#featureImportance}

## Introduction {#featureImportanceIntro}

In this chapter, we present methods that are useful for the evaluation of an explanatory variable importance. The methods may be applied for several purposes.
  
* Model simplification: variables that do not influence model's predictions may be excluded from the model. 
* Model exploration: comparison of a variable's importance in different models may help in discovering interrelations between the variables.Also, ordering of variables in function of their importance is helpful in deciding in what order should we perform further model exploration. 
* Domain-knowledge-based model validation: identification of the most important variables may be helpful in assessing the validity of the model based on the domain knowledge. 
* Knowledge generation: identification of the most important variables may lead to discovery of new factors involved in a particular mechanism.

The methods for assessment of variable importance can be divided, in general, into two groups: model-specific and model-agnostic.

For models like linear models, random forest, and many others, there are methods of assessing of variable importance that exploit particular elements of the structure of the model. These are model-specific methods. For instance, for linear models, one can use the value of the normalized regression coefficient or its corresponding p-value as the variable-importance measure. For tree-based ensembles, such a measure may be based on the use of a particular variable in particular trees (see, e.g., `XgboostExplainer` [@xgboostExplainer] for gradient boosting and `RandomForestExplainer` [@randomForestExplainer] for random forest).

In this book we focus on model-agnostic methods. These methods do not assume anything about the model structure. Therefore, they can be applied to any predictive model or ensemble of models. Moreover, and perhaps even more importantly, they allow comparing variable importance between models with different structures.

## Intuition {#featureImportanceIntuition}

We focus on the method described in more detail in [@variableImportancePermutations]. The main idea is to measure how much the model fit decreases if the effect of a selected explanatory variable or of a group of variables is removed. The effect is removed by means of perturbations like resampling from an empirical distribution of just permutation of the values of the variable. 

The idea is in some sense borrowed from variable important measure proposed by \@ref{randomForestBreiman} for random forest. If a variable is important, then after permutation of this variable we expect that the model performance will be lower. The larger drop in the performance, the more important is the variable.

Despite the simplicity of definition, the permutation variable importance is a very powerful model agnostic tool for model exploration. Values of permutation variable importance may be compared between different structures of models. This property is discussed in detail in the section *Pros and Cons*.

## Method {#featureImportanceMethod}


Consider a set of $n$ observations for a set of $p$ explanatory variables. Denote by  $\widetilde{y}=(f(x_1),\ldots,f(x_n))$ the vector of predictions for model $f()$ for all the observations. Let $y$ denote the vector of observed values of the dependent variable $Y$. 

Let $\mathcal L(\widetilde{y}, y)$ be a loss function that quantifies goodness of fit of model $f()$ based on $\widetilde{y}$ and $y$. For instance, $\mathcal L$ may be the value of likelihood. Consider the following algorithm:

1. Compute $L = \mathcal L(\widetilde{y}, y)$, i.e., the value of the loss function for the original data. 
2. For each explanatory variable $X^j$ included in the model, do steps 3-6.
3. Replace vector $x^j$ of observed values of $X^j$ by vector $x^{*j}$ of resampled or permuted values.
4. Calculate model predictions $\widetilde{y}^{*j}$ for the modified data, $\widetilde{y}^{*j} = f(x^{*j})$.
5. Calculate the value of the model performance for the modified data: 
$$
L^{*j} = \mathcal L(\widetilde{y}^{*j}, y)
$$
6. Quantify the importance of explanatory variable $x^j$ by calculating $vip_{Diff}(x^j) = L^{*j} - L$ or $vip_{Ratio}(x^j) = L^{*j} / L$, where $L$ is the value of the loss function for the original data. 


Note that the use of resampling or permuting data in Step 3 involves randomness. Thus, the results of the procedure may depend on the actual configuration of resampled/permuted values. Hence, it is advisable to repeat the procedure several times. In this way, the uncertainty related to the calculated variable-importance values can be assessed.

The calculations in Step 6 ``normalize'' the value of the variable importance measure with respect to $L$. However, given that $L$ is a constant, the normalization has no effect on the ranking of variables according to $vip_{Diff}(x^j)$ or $vip_{Ratio}(x^j)$. Thus, in practice, often the values of $L^{*j}$ are simply used to quantify variable's importance.

## Example: Titanic data {#featureImportanceTitanic}

In this section, we illustrate the use of the permutation-based variable-importance method by applying it to the random forest model for the Titanic data (see Section \@ref(model-titanic-rf)). Recall that the goal is to predict survival probability of passengers based on their sex, age, cityplace of embarkment, class in which they travelled, fare, and the number of persons they travelled with. 

Figure \@ref(fig:TitanicRFFeatImp) shows the values of loss function measured as $1-AUC^{*j}$ after permuting, in turn, each of the variables included in the model. Additionally, the plot indicates the value of $L$ by the vertical dashed line at the left-hand-side of the plot. Length of the bar span between $L$ and $L^{*j}=1-AUC^{*j}$ and correspond to the variable importance.


```{r titanic1, warning=FALSE, message=FALSE, echo=FALSE}
library("randomForest")
library("DALEX")
library("dplyr")
```
  
```{r TitanicRFFeatImp, warning=FALSE, message=FALSE, echo=FALSE, fig.width=5, fig.height=2.5, fig.cap="Each interval presents the difference between the loss function for the original data (vertical dashed line at the left) and for the data with permuted observation for a particular variable.", out.width = '70%'}
library("ingredients")
fi_rf <- feature_importance(explain_titanic_rf, loss_function = loss_one_minus_auc, B = 1) 
plot(fi_rf[-2,], show_boxplots = FALSE) + 
  ggtitle("Permutational variable importance") + 
  ylab("Loss function: 1-AUC") + ylim(0.1,0.35)
```

The plot in Figure \@ref(fig:TitanicRFFeatImp) suggests that the most important variable in the model is gender. This agrees with the conclusions drawn in the exploratory analysis presented in Section \@ref(exploration-titanic). The next three important variables are class of the travel (first-class patients had a higher chance of survival), age (children had a higher chance of survival), and fare (owners of more expensive tickets had a higher chance of survival).

To take into account the uncertainty related to the use of permutations, we can consider computing the average values of $L^{*j}$ over a set of, say, 10 permutations. The plot in Figure \@ref(fig:TitanicRFFeatImp10) presents the average values.  The only remarkable difference, as compared to Figure \@ref(fig:TitanicRFFeatImp), is the change in the ordering of the `sibsp` and `parch` variables.

```{r TitanicRFFeatImp10, warning=FALSE, message=FALSE, echo=FALSE, fig.width=5, fig.height=2.5, fig.cap="Average variable importance based on 10 permutations.", out.width = '70%'}
fi_rf <- feature_importance(explain_titanic_rf, loss_function = loss_one_minus_auc, B = 10) 
plot(fi_rf[-2,]) + 
  ggtitle("Permutation-based variable-importance") + 
  ylab("Loss function: 1-AUC") + ylim(0.1,0.35)
```

The plots similar to those presented in Figures  \@ref(fig:TitanicRFFeatImp) and  \@ref(fig:TitanicRFFeatImp10) are useful for comparisons of variable importance for different models. 
Figure \@ref(fig:TitanicFeatImp) presents the single-permutation results for the random forest, gradient boosting (see Section \@ref(model-titanic-gbm)), and logistic regression (see Section \@ref(model-titanic-lmr)) models. The best result, in terms of the smallest value of the goodness-of-fit function $L$, are obtained for the random forest model. Note, however, that this model includes more variables than the other two. For instance, variable `fare`, which is highly correlated with the travel class, is not important neither in the gradient boosting nor in the logistic regression model, but is important in the random forest model. 

The plots in Figure \@ref(fig:TitanicFeatImp) indicate that `gender` is the most important variable in all three models, followed by `class`.  

```{r TitanicFeatImp, warning=FALSE, message=FALSE, echo=FALSE, fig.width=5, fig.height=6, fig.cap="Variable importance for the random forest, gradient boosting, and logistic regression models for the Titanic data.", out.width = '70%'}
fi_rf <- feature_importance(explain_titanic_rf)
fi_gbm <- feature_importance(explain_titanic_gbm)
fi_glm <- feature_importance(explain_titanic_lmr)

plot(fi_rf, fi_gbm, fi_glm) + ggtitle("Variable Importance")
```

## Pros and cons {#featureImportanceProsCons}

Permutation variable importance offer a model-agnostic approach to assessment of the influence of each variable on model performance. The approach offers several advantages. The plots are easy to understand. They are compact, all most important variables are presented in a single plot. 

Permutation variable importance is expressed in a terms of model performance and can be compared between models. In different models the same variable may have different importance scores and comparison of such scores may lead to interesting insights. For example if variables are correlated then models like random forest are expected to spread importance across every variable while in regularized regression models coefficients for one correlated variable may dominate over coefficients for other variables.  

The same approach can be used to measure importance of a single explanatory variable or a group of variables. The latter is useful for aspects - groups of variables that are complementary or are related to a similar concept. For example in the Titanic example the `fare` and `class` variables are linked with wealth of a passenger. Instead of calculation of effects of each variable independently we may calculate effect of both variables by permutation of both.

The disadvantage of this measure comes from the randomness behind permutations. For different permutations we may get different results. Also different choices of model performance measure, like Precision, Accuracy, AUC, lead to different numeric values of variable importance. And last disadvantage is related with the data used for assessment of model performance. Different importance values may be obtained on training and testing data.


## Code snippets for R {#featureImportanceR}

For illustration, We will use the random forest model for the apartment prices data (see Section \@ref(model-Apartments-rf)).
 
Let's recover a regression model for prediction of apartment prices.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library("DALEX")
library("randomForest")
set.seed(59)
explainer_rf <- archivist::aread("pbiecek/models/6ed54")
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library("DALEX")
library("randomForest")
set.seed(59)
model_rf <- randomForest(m2.price ~ construction.year + surface + floor + 
                           no.rooms + district, data = apartments)
explainer_rf <- explain(model_rf, data = apartmentsTest[,2:6], 
            y = apartmentsTest$m2.price, verbose = FALSE)
```

A popular loss function for regression model is the root mean square loss.

$$
  L(y, \tilde y) = \sqrt{\frac1n \sum_{i=1}^n (y_i - \tilde y_i)^2}
$$
  
It is implemented in the `DALEX` package in the function `loss_root_mean_square`. The initial loss function $L$ for this model is

```{r, warning=FALSE, message=FALSE}
loss_root_mean_square(
  predict(model_rf, apartmentsTest), 
  apartmentsTest$m2.price
)
```

Let's calculate variable importance for root mean square loss with the `model_parts` function.

```{r, warning=FALSE, message=FALSE}
vip <- model_parts(explainer_rf, 
            loss_function = loss_root_mean_square)
vip
```

On a diagnostic plot is useful to present variable importance with boxplots that show results for different permutations.

```{r featureImportanceUnoPlot, fig.cap="Permutation variable importance calculated as root mean square loss for random forest model for apartments data.",  warning=FALSE, message=FALSE, fig.width=8, fig.height=3}
library("ggplot2")
plot(vip) +
  ggtitle("Permutation variable importance", "")
```

### Models comparison

Variable importance plots are very useful tool for model comparison. In the section \@ref(ApartmentDataset) we have trained three models  on `apartments` dataset. 
These were models with different structures to make the comparison more interesting.
Random Forest model [@R-randomForest] (elastic but biased), Support Vector Machines model [@R-e1071] (large variance on boundaries) and Linear Model (stable but not very elastic). 

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library("DALEX")
model_lm <- lm(m2.price ~ construction.year + surface + floor + 
                      no.rooms + district, data = apartments)

library("randomForest")
set.seed(59)
model_rf <- randomForest(m2.price ~ construction.year + surface + floor + 
                      no.rooms + district, data = apartments)

library("e1071")
model_svm <- svm(m2.price ~ construction.year + surface + floor + 
                         no.rooms + district, data = apartments)


explainer_lm <- explain(model_lm, data = apartmentsTest[,2:6], 
                       y = apartmentsTest$m2.price, verbose = FALSE)
explainer_rf <- explain(model_rf, data = apartmentsTest[,2:6], 
                       y = apartmentsTest$m2.price, verbose = FALSE)
explainer_svm <- explain(model_svm, data = apartmentsTest[,2:6], 
                       y = apartmentsTest$m2.price, verbose = FALSE)
```

Let's calculate permutation variable importance with root mean square error for these three models.

```{r, warning=FALSE, message=FALSE}
vip_lm <- variable_importance(explainer_lm, 
            loss_function = loss_root_mean_square)
vip_lm
vip_rf <- variable_importance(explainer_rf, 
            loss_function = loss_root_mean_square)
vip_rf
vip_svm <- variable_importance(explainer_svm, 
            loss_function = loss_root_mean_square)
vip_svm
```

Now we can plot variable importance for all three models on a single plot.
Intervals start in a different values, thus we can read that loss for SVM model is the lowest.

When we compare other variables it looks like in all models the `district` is the most important feature followed by `surface` and `floor`. 

```{r featureImportanceTriPlot, fig.cap="Permutation variable importance on apartments data for Random forest, Support vector model and Linear model.",  warning=FALSE, message=FALSE, fig.width=8, fig.height=6}
library("ggplot2")
plot(vip_rf, vip_svm, vip_lm) +
  ggtitle("Permutation variable importance", "")
```

There is interesting difference between linear model and others in the way how important is the `construction.year`. For linear model this variable is not importance, while for remaining two models there is some importance.

In the next chapter we will see how this is possible.

