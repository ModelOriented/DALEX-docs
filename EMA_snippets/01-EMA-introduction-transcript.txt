Welcome to the Explanatory Model Analysis podcast.
This podcast is about various techniques for explainable artificial intelligence. 

This episode is an introduction to the topic of visualization, exploration and explanation of machine learning models. I will show why this topic is important and how it can be used to build better predictive models.

This and other episodes along with complementary materials are available at tiny.cc/DALEX.
If you have any question or comment related to this topic please let me know. 
You will find my contact details at the bottom of the page and on the final screen.



Why the Explanatory Model Analysis is important?

Predictive models have been developed for years. So you're surely asking yourself why now so much attention is being paid to techniques for model exploration. There are several reasons for this.


Models are getting more complex.

The availability of large data sets and fast computers allow you to train more and more complex models. 
So complex that they cannot be analyzed by looking directly at model parameters.  
It is common to work with models that have thousands or millions of parameters.
We need tools to analyze these huge models.

Right to explanation.

Predictive models decide which ads we see on social media, which books, films or music are recommended for us.
Automated models win elections, drive cars, diagnose patients and decide who gets and who doesn't get credit.
The increasing automation is increasingly affecting our daily lives. 
This has led to changes in the law to protect citizens' civil rights.  Many countries are introducing so-called "Right to Explanation" into their legal systems. This is the right to an explanation for a decision that has been made automatically by an algorithm.
We need tools to provide such explanations.

Ethical issues.

The analysis of machine learning models in some cases has shown that the models lead to biased decisions. 
Like models for recidivism that discriminate on the basis of skin colour or models for credit scoring that discriminate on the basis of age or gender.
Usually, this bias is learned from historical data. In complex models, it is difficult to monitor if the bias occurs and how strong it is.
We need tools to identify or correct such biases.

Model debugging.

The more complex the model, the harder it is to understand the reasons leading to wrong decisions. And we can't fix what we don't understand.
We need tools to support post-hoc analysis of wrong predictions.

Trust and model human interaction.

Machine learning models are often designed to support human decision-making. For the human operator, it is necessary to trust and understand the reasons behind the model recommendation. 
We need tools for better communication between the model and the human.

Explanatory Model Analysis for models is what Exploratory Data Analysis for data.



Three approaches to explainability.

The need to understand how models work is not new. Tools for model diagnostics have been accompanying model development for years.
We can distinguish three major approaches to the topic of model explainability.

Interpretable by design.

One approach is to use only models that have a simple structure and can be directly understood. For example models with a small number of variables or shallow decision trees or methods based on decision rules. This approach usually is time-consuming and requires expert knowledge for the selection and engineering of variables. A simple structure of a model allows for direct analysis.

For complex models, we cannot analyze individual parameters.  For some types of models, special tools have been developed to analyze and visualize their structures. This approach is called a model-specific. Their functionality is based on exploring the model structure. Examples of such tools are diagnostic plots for linear models, node statistics for random forest models or integrated gradients for deep neural networks. 

In this podcast, we focus on the model-agnostic approach. In the model-agnostic approach we do not assume anything about the internal structure of the model. It is analysed on the black box principle. We only explore the relationship between the input and output of the model without caring about its internal structure. Such methods can be applied to any model and also allows to compare different models.


How EMA may improve the model development?

EMA methods can be applied in any part of the model development cycle.
A typical process of building a predictive model is based on three steps repeated. Deepen the understanding of data, prepare a new version of the models and assess the quality of the model. 

In each of these stages, we can use EMA methods. 
Pre-modelling applications are aimed at extracting new knowledge from the model. 
In-modelling applications are aimed at debugging models and fixing their mispredictions. 
Post-modelling applications are aimed at explaining model predictions.


The pyramid for Explanatory Model Analysis

The methods we will discuss in this podcast are arranged in a pyramid-like ontology.

We will discuss two classes of methods. 
The first one focuses on the analysis of the model result for a single observation. 
These methods are called instance-level explanations or local explanations. 
For example, for a model that estimates odds of default in credit scoring, the instance level analysis focuses on explaining the model's behaviour for one particular client. 

The second group of methods is focused on the analysis of the model's behaviour on the whole data set. We are not interested in any particular prediction, but in average model behaviour.
Such tools are called dataset level explanations or global explanations. 
For example, for a model that estimates odds of default in credit scoring, the model level analysis focuses on explaining the model's behaviour for a selected population of customers. 



The explanatory pyramid is composed of levels. In each subsequent one, we explore more and more deeply the behaviour of the model.

In the first level are raw model predictions and model statistics, which can be described by single numbers like model performance measures.
The second level is related to the analysis of model parts. We want to understand which variables have the greatest influence on model behaviour.
The third level describes the profile of model behaviour depending on the values of individual variables.
The fourth level corresponds to the residual diagnostics. We want to understand how good is the model fit to the analyzed data.


The methods listed in this pyramid are described in detail in the ebook Exploratory Model Analysis, available at github.com/pbiecek/ema. 
In this ebook, for each method you will find the intuition, methodological details and examples for different classification and regression models.

The consecutive episodes in this podcast show how to perform EMA analysis using the DALEX library for R or Python. Subsequent episodes are divided into two tracks, one with examples for the R program and the second for Python. 

The list of all episodes is available on the website tiny.cc/DALEX.
If you have any question or comment related to this topic please let me know. 


