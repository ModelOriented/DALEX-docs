This is the Explanatory Model Analysis podcast about various techniques for explainable artificial intelligence. I describe methods for visualization, exploration and explanation of machine learning models.

This episode is about the Ceteris Paribus profiles.


In this episode, I will show you how to use the Ceteris Paribus profiles implemented in the DALEX package for the Python language.
If you are interested in examples for R or other XAI methods you can find other episodes at tiny.cc/DALEX webpage.

Before we jump into the Python code, let me remind you what Ceteris Paribus profiles are for. Like Break Down and Shapley values, it's a ‘model agnostic’ and 'instance-specific' method.  It means that it works for any model without assuming anything of its internal structure. And it explains the result of the model for a single observation. 

Ceteris paribus is a Latin phrase meaning 'other things held constant' or 'all else unchanged'. CP profiles show how the model response would change if values for a single variable is changed for the instance of interest. 

This technique may be used for sensitivity analysis or to what-if model analysis.

In this slide, we have an example for a single passenger, the 8-year-old boy from the first class. The model prediction for him is 0.64. 
Keeping all other variables unchanged, increasing the age lowers the expected chance of survival. 
Changing the gender to a woman would increase the predicted chances of survival. 
For such a young age a change of class would not change chances of survival. 


It is important to remember that we need a causal model to do causal inference for real-world phenomena. CP profiles are about the behaviour of the model, which may not be transferred to the behaviour of the analysed process.


Let's see how to do such ceteris paribus profiles in Python.

This video is focused on the software. If you want to learn more about the method find more details on pbiecek.github.io/ema


As in other episodes, I will use the titanic data and a classification random forest model trained with scikit-learn.
Note that the ceteris-paribus method is model agnostic, so you can try the same workflow with any other model you wish.


Different models have different structures. We use the Explainer constructor from DALEX package to create a model adapter with unified A-P-I. You can find more details about this function in Episode 2.



The ceteris paribus profiles work on the instance level. So we need an observation for which we can generate the explanation.
Here we create a data frame with a single row that corresponds to an eight years old boy from 1st class.

The model prediction for this boy is 0.64.


Once we have the model and the instance of interest, we are ready for the ceteris paribus method.

Here we use the explainer's `predict_profile` method.
The name of the function means that we analyse response profiles for a prediction.

In the basic settings, this method expects only one argument. Pandas Data Frame with one or more observations.
The result is a Ceteris Paribus object containing data frame with profiles for each variable used in the model. 
One can limit the number of variables for which profiles are calculated with the optional argument `variables`.


A plot is easier to read than a table with numbers. You can use the `plot` method of the Ceteris Paribus object to visualise the model response. Each variable is presented in a different panel.

Many variables can obfuscate the plot.  The argument variables can be used to specify which variables shall be presented.

This is the plotly object so you can use plotly functions to change the plot or simply download it. Set parameter `show` to `False` in order to receive the plotly object.


There are only plotted numerical variables by default. You can change it simply setting parameter `variable_type` to `categorical` or just listing categorical features.


By default, the 'predict profile' method calculates the model response for a small grid of points. If you want a more detailed profile, you may manually specify the grid of points in which the profiles are to be calculated. In this case, you should indicate a dictionary with lists of grids of points using the argument 'variable splits'.
For a random forest model, the response is a step function. But you need a very dense grid of points to see these steps.



The predict_profile method calculates profiles for one or more observations. In this example, we calculate and plot profiles for John and Mary. 
If the model was additive, the ceteris paribus profiles for observations would be parallel. 
But the random forest model is not additive.  
For John the higher is the age the lower are model predictions.
For Mary, regardless of the age, the predictions of the model are very high.


The ceteris paribus profiles can be used to compare different predictive models.
To show this functionality we will use the Logistic Regression model from scikit-learn package. We create another explainer and with the predict profile  and plot methods, we can visualise it.
We see that the logistic regression model for John gives much higher predictions. The response curve of the model is also much smoother, although in trend similar to a random forest.



The general workflow with the Ceteris-paribus method is summarised in this sequence diagram. 
Once you have a model you need to create a model adapter.
To calculate ceteris paribus profiles you can use the predict_profile method.
The result can be visualised with the plot method.
