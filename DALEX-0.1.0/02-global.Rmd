# Model understanding  {#modelUnderstanding}

In this chapter we introduce three groups of explainers that can be used to boost our understanding of black-box models.

* Section \@ref(modelPerformance) presents explainers for model performance. A single number may be misleading when we need to compare complex models. In this section you will also find plots that give more information about model performance in a consistent form.
* Section \@ref(featureImportance) presents explainers for variable importance. Knowing which variables are important allows us to validate the model and increase our understanding of the domain.
* Section \@ref(variableResponse) presents explainers for variable effect.  You may find in it plots that summarize the relation between model response and particular variables.

All explainers are illustrated on the basis of two models fitted to the `apartments` data.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
apartments_lm_model <- lm(m2.price ~ construction.year + surface + floor + 
                      no.rooms + district, data = apartments)
library("randomForest")
set.seed(59)
apartments_rf_model <- randomForest(m2.price ~ construction.year + surface + floor + 
                      no.rooms + district, data = apartments)
```

First we need to prepare wrappers for these models. They are in `explainer_lm` and `explainer_rf` objects.

```{r, warning=FALSE, message=FALSE}
explainer_lm <- explain(apartments_lm_model, 
                       data = apartmentsTest[,2:6], y = apartmentsTest$m2.price)
explainer_rf <- explain(apartments_rf_model, 
                       data = apartmentsTest[,2:6], y = apartmentsTest$m2.price)
```

## Model performance {#modelPerformance}

As you may remember from the previous chapter, the [root mean square](https://en.wikipedia.org/wiki/Root_mean_square) of residuals is identical for both considered models. Does it mean that these models are equally good?

```{r, warning=FALSE, message=FALSE}
predicted_mi2_lm <- predict(apartments_lm_model, apartmentsTest)
sqrt(mean((predicted_mi2_lm - apartmentsTest$m2.price)^2))

predicted_mi2_rf <- predict(apartments_rf_model, apartmentsTest)
sqrt(mean((predicted_mi2_rf - apartmentsTest$m2.price)^2))
```

Function `model_performance()` calculates predictions and residuals for validation dataset `apartmentsTest`.

Generic function `print()` returns quantiles for residuals.

```{r}
mp_lm <- model_performance(explainer_lm)
mp_rf <- model_performance(explainer_rf)
mp_lm
mp_rf
```

The generic `plot()` function shows reversed empirical cumulative distribution function for absolute values from residuals. This function presents a fraction of residuals larger than `x`. The figure below shows that majority of residuals for the random forest is smaller than residuals for the linear model, yet the small fraction of very large residuals affects the root mean square.


```{r global_explain_ecdf, fig.cap="Comparison of residuals for linear model and random forest"}
plot(mp_lm, mp_rf)
```

Use the `geom = "boxplot"` parameter for the generic `plot()` function to get an alternative comparison of residuals. The red dot stands for the root mean square.

```{r global_explain_boxplot, fig.height=2.5, fig.cap="Comparison of residuals for linear model and random forest"}
plot(mp_lm, mp_rf, geom = "boxplot")
```


## Feature importance {#featureImportance}

Explainers presented in this section are designed to better understand which variables are important.

Some models, such as linear regression or random forest, have a build-in *model specific* methods to calculate and visualize variable importance. They will be presented in Section \@ref(modelSpecific).

Section \@ref(modelAgnostic) presents a model agnostic approach on the basis of permutations. The advantage of this approach is that different models can be compared within a single setup.


### Model agnostic {#modelAgnostic}

Model agnostic variable importance is calculated by means of permutations. 
We simply substract the loss function calculated for validation dataset with permuted values for a single variable from  the loss function calculated for validation dataset. This concept and some extensions are described in [@variableImportancePermutations].

This method is implemented in the `variable_importance()` function. The loss function is calculated for:

* the original validation `data`. It is an estimate of a model performance and will be denoted as `_full_model_`,
* validation data with resampled `y` labels. It is a kind of *worst case* loss when model are compared against random labels. It will be denoted as `_baseline_`,
* validation data with single variable being resampled. It tells us how much is gone from the model performance after the selected variable is blinded.

Let's see how this function works for a random forest model. 

```{r}
vi_rf <- variable_importance(explainer_rf, loss_function = loss_root_mean_square)
vi_rf
```

<p><span class="marginnote">Here the `loss_root_mean_square()` function is defined as square root from averaged squared differences between labels and model predictions.
</span>
The same method may be applied to a linear model. Since we are using the same loss function and the same method for variable permutations, the losses calculated with both methods can be directly compared.</p>


```{r}
vi_lm <- variable_importance(explainer_lm, loss_function = loss_root_mean_square)
vi_lm
```

It is much easier to compare both models when these values are plotted close to each other.
The generic `plot()` function may handle both models. 

```{r modelImportanceRaw, message=FALSE, warning=FALSE, fig.height=3.5, fig.cap="Model agnostic variable importance plot. Right edges correspond to loss function after permutation of a single variable. Left edges correspond to loss of a full model"}
plot(vi_lm, vi_rf)
```

What we can read out of this plot?

* left edges of intervals start in `_full_model_` for a given model. As we can see. the performances are similar for both models,
* length of the interval corresponds to variable importance. In both models the most important variables are `district` and `surface`,
* in the random forest model the `construction_year` variable has some importance, while its importance for linear model is almost equal to zero,
* the variable `no.rooms` (which is correlated with `surface`) has some importance in the random forest model but not in the linear model.

We may be interested in variables that behave differently between models (like `construction_year`) or variables that are important in both models (like `district` or `surface`). In the next section we introduce explainers for further investigation of these variables.


*NOTE:* If you want variable importance hooked at 0, just add `type = "difference"` parameter to `variable_importance()`.

```{r modelImportanceDifference, message=FALSE, warning=FALSE, fig.height=3.5, fig.cap="Model agnostic variable importance plot. Right edges correspond to difference between loss after permutation of a single variable and loss of a full model"}
vi_lm <- variable_importance(explainer_lm, loss_function = loss_root_mean_square, type = "difference")
vi_rf <- variable_importance(explainer_rf, loss_function = loss_root_mean_square, type = "difference")
plot(vi_lm, vi_rf)
```


### Model specific {#modelSpecific}

Some models have build-in tools for calculation of variable importance.
Random forest uses two different measures - one based on out-of-bag data and second one based on gains in nodes. Read more about this approach in [@randomForest]. 

Below we show an example of a dot plot that summarizes default importance measure for a random forest. The `varImpPlot()` function is available in the `randomForest` package.

```{r modelImportanceRF, message=FALSE, warning=FALSE, fig.height=3.5, fig.cap="Built-in variable importance plot for random forest"}
varImpPlot(apartments_rf_model)
```

It is easy to assess variable importance for linear models and generalized models, since model coefficients have direct interpretation.

[Forest plots](https://en.wikipedia.org/wiki/Forest_plot) were initially used in the meta analysis to visualize effects in different studies. . At present, however, they are frequently used to present summary characteristics for models with linear structure / created with `lm` or `glm` functions.

There are various implementations of forest plots in R. In the package `forestmodel` (see [@forestmodel]) one can use `forest_model()` function to draw a forest plot. This package is based on the `broom` package (see [@broom]) and this is why it handles a large variety of different regression models. 

```{r forestmodel, warning=FALSE, message=FALSE, fig.width=10, fig.height=5, fig.cap='Forest plot created with `forestmodel` package'}
library("forestmodel")
forest_model(apartments_lm_model)
```

In the package `sjPlot` (see [@sjPlot]) one can find `sjp.xyz()` function to visualize coefficients of a `xyz` model (like `sjp.glm()` for `glm` models) or a generic wrapper `plot_model()`. 


```{r sjpglm, message=FALSE, warning=FALSE, fig.width=10, fig.width=8, fig.cap='Model coefficients plotted  with sjPlot package'}
library("sjPlot")
plot_model(apartments_lm_model, type = "est", sort.est = TRUE)
```

**Note!** 

The `forestmodel` package handles factor variables in a better way while the plots from `sjPlot` are easier to read.


## Variable response {#variableResponse}

Explainers presented in this section are designed to better understand the relation between a variable and a model output.

Subsection \@ref(pdpchapter) presents Partial Dependence Plots (PDP), one of the most popular methods for exploration of a relation between a continuous variable and a model outcome. 
Subsection \@ref(accumulatedLocalEffects) presents Accumulated Local Effects Plots (ALEP), an extension of PDP more suited for highly correlated variables.

Subsection \@ref(mergingPathPlot) presents Merging Path Plots, a method for exploration of a relation between a categorical variable and a model outcome.


### Partial Dependence Plot {#pdpchapter}

Partial Dependence Plots (see `pdp` package [@pdp]) for a black box $f(x; \theta)$ show the expected output condition on a selected variable.

$$
p_i(x_i) = E_{x_{-i}}[ f(x^i, x^{-i}; \theta) ].
$$

Of course, this expectation cannot be calculated directly as we do not know fully neither the distribution of $x_{-i}$ nor the $f()$. Yet this value may be estimated by 

$$
\hat p_i(x_i) = \frac{1}{n} \sum_{j=1}^{n} f(x^i_j, x_j^{-i}, \hat \theta).
$$

Let's see an example for the model `apartments_rf_model`. Below we use `variable_response()` from `DALEX`, which calls `pdp::partial` function to calculate PDP response.

Section \@ref(featureImportance) shows variable importance plots for different models. The variable `construction.year` is interesting as it is important for the random forest model `apartments_rf_model` but not for the linear model `apartments_lm_model`. Let's see the relation between the variable and the model output. 

```{r pdpRandomForest, message=FALSE, warning=FALSE, fig.cap="Relation between output from `apartments_rf_model` and variable `construction.year`"}
sv_rf  <- single_variable(explainer_rf, variable =  "construction.year", type = "pdp")
plot(sv_rf)
```

We can use PDP plots to compare two or more models. Below we plot PDP for the linear model against the random forest model.

```{r pdpRandomForestLM, message=FALSE, warning=FALSE, fig.cap="Relation between output from models `apartments_rf_model` and `apartments_lm_model` against the variable `construction.year`"}
sv_lm  <- single_variable(explainer_lm, variable =  "construction.year", type = "pdp")

plot(sv_rf, sv_lm)
```

It looks like the random forest captures the non-linear relation that cannot be captured by linear models.

### Accumulated Local Effects Plot {#accumulatedLocalEffects}

As demonstrated in section \@ref(pdpchapter), the Partial Dependence Plot presents the expected model response with respect to marginal distribution of $x_{-i}$. 
In some cases, e.g. when repressors are highly correlated, expectation towards the marginal distribution may lead to biases/poorly extrapolated model responses. 

Accumulated local effects (ALE) plots (see `ALEPlot` package [@ALEPlot]) solve this problem by using conditional distribution $x_{-i}|x_i = x_i^*$. This solution leads to more stable and reliable estimates (at least when the predictors are highly correlated).

Estimation of the main effects for `construction.year` is similar to the PDP curves. We use here `DALEX::single_variable` function that calls  `ALEPlot::ALEPlot` function to calculate the ALE curve for the variable `construction.year`. 


```{r alePlotsRF, message=FALSE, warning=FALSE, fig.cap="Relation between output from models `apartments_rf_model` and `apartments_lm_model` against the variable `construction.year` calculated with Accumulated local effects."}
sva_rf  <- single_variable(explainer_rf, variable = "construction.year", type = "ale")
sva_lm  <- single_variable(explainer_lm, variable = "construction.year", type = "ale")

plot(sva_rf, sva_lm)
```

Results for PDP and ALEP are very similar except that effects for ALEP are centered around 0.


### Mering Path Plot {#mergingPathPlot}

The package `ICEbox` does not work for factor variables, while the `pdp` package returns plots that are hard to interpret.

An interesting tool that helps to understand what happens with factor variables is the **factorMerger** package. See [@factorMerger].

Below you may see a Merging Path Plot for a factor variable `district`.


```{r mergingPathPlots, message=FALSE, warning=FALSE, fig.width=12, fig.height=8, fig.cap="Merging Path Plot for `district` variable. Left panel shows the dendrogram for districts, here we have clearly three clusters. Right panel shows distribution of predictions for each district."}
svd_rf  <- single_variable(explainer_rf, variable = "district", type = "factor")
svd_lm  <- single_variable(explainer_lm, variable = "district", type = "factor")

plot(svd_rf, svd_lm)
```

The three clusters are: the city center (Srodmiescie), districts well communicated with city center (Ochota, Mokotow, Zoliborz) and other districts closer to city boundaries.

Factor variables are handled very differently by random forest and linear model, yet despite these differences both models result in very similar plots.


