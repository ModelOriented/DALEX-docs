---
title: "DALEX for Multi Layer Perceptron with H2O and Keras"
author: "Micha≈Ç Maj"
date: "2019-04-17"
output: 
  html_document:
    toc: true  
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warnings = FALSE, message = FALSE, results='hide', fig.keep='all')
```

# Introduction

DALEX is designed to work with various black-box models like tree ensembles, linear models, neural networks etc. Unfortunately R packages that create such models are very inconsistent. Different tools use different interfaces to train, validate and use models. Fortunately DALEX can handle it all easily.

In this vignette we will show explanations for Multi Layer Perceptron (MLP) trained with [h2o ](https://cran.r-project.org/web/packages/h2o/index.html) and [keras](https://cran.r-project.org/web/packages/keras/index.html) packages.

# Classification use case - Titanic data

```{r}
library(tidyverse)
library(DALEX)
library(keras)
library(titanic)
library(fastDummies)
library(h2o)
set.seed(123)
```


To illustrate applications of DALEX to classification problems we will use the `titanic_train` dataset available in the `titanic` package. Our goal is to predict the probability that the person will survive the catastrophe based on selected features such as cabin class, sex, age, number of family members on the ship, fare and embarkation. In both packages we will try to build a model with same architecture and inputs.

In the begining we will prepare and clean the data. The first difference between `keras` and `h2o` is that in `keras` predictors and exaplined variable have to be specified as separate numeric tensors. This means that if we want to insert a factor into `keras` model we have to encode it first. We will do one-hot encoding using `dummy_cols` function from `fastDummies` package (for both, `h2o` and `keras`, so the number and type of inputs will be identical).

```{r, results='markup'}
# Data preparation and cleaning
titanic_small <- titanic_train %>%
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
  mutate_at(c("Survived", "Sex", "Embarked"), as.factor) %>%
  mutate(Family_members = SibSp + Parch) %>% # Calculate family members
  na.omit() %>%
  dummy_cols() %>%
  select(-Sex, -Embarked, -Survived_0, -Survived_1, -Parch, -SibSp)
print(head(titanic_small))
# Data preprocessing for Keras
titanic_small_y <- titanic_small %>% select(Survived) %>% mutate(Survived = as.numeric(as.character(Survived))) %>% as.matrix()
titanic_small_x <- titanic_small %>% select(-Survived) %>% as.matrix()
```

# Models

We can build MLP model in `h2o` using `h2o.deeplearning` function.
To do this w need to first initialize `h2o` and we need to convert `titanic_small` to `H2OFrame`.

```{r}
h2o.init()

titanic_h2o <- as.h2o(titanic_small, destination_frame = "titanic_small")

model_h2o <- h2o.deeplearning(x = 2:11,
                              y = 1,
                              training_frame = "titanic_small",
                              activation = "Rectifier", # ReLU as activation functions
                              hidden = c(16, 8), # Two hidden layers with 16 and 8 neurons
                              epochs = 100,
                              rate = 0.01, # Learning rate
                              adaptive_rate = FALSE, # Simple SGD
                              loss = "CrossEntropy")
```

To build neural network in `keras` we have to stack layers, in case of MLP we will use `layer_dense`. 

```{r}
model_keras <- keras_model_sequential() %>% # Initialization
  layer_dense(units = 16, # 16 neurons in first hidden layer
              activation = "relu", # ReLU as activation function
              input_shape = c(10)) %>% # Ten inputs
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model_keras %>% compile(
  optimizer = optimizer_sgd(lr = 0.01), # Simple SGD with learning rate 0.01
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- model_keras %>% fit(
  titanic_small_x,
  titanic_small_y,
  epochs = 100,
  validation_split = 0.2
)
```

We can now check predictions from both models. Remember that `h2o` and `keras` are using different implementations of same algorithms and there is a lot of other, often randomized, parameters like initial weights values, so to get exactly the same results you would have to consider all of them.

```{r}
henry <- data.frame(
  Pclass = 1,
  Age = 8,
  Family_members = 0,
  Fare = 72,
  Sex_male = 1,
  Sex_female = 0,
  Embarked_S = 0,
  Embarked_C = 1,
  Embarked_Q = 0,
  Embarked_ = 0
)

henry_h2o <- as.h2o(henry, destination_frame = "henry")
henry_keras <- as.matrix(henry)
predict(model_h2o, henry_h2o) %>% print()
predict(model_keras, henry_keras) %>% print()
```


# The explain() function

The first step of using the DALEX package is to wrap-up the black-box model with meta-data that unifies model interfacing.

To create an explainer we use `explain()` function. For the models created by h2o package we have to provide custom predict function which takes two arguments: `model` and `newdata` and returns a numeric vector with predictions.

```{r}
custom_predict_h2o <- function(model, newdata)  {
  newdata_h2o <- as.h2o(newdata)
  res <- as.data.frame(h2o.predict(model, newdata_h2o))
  return(as.numeric(res$p1))
}

explainer_titanic_h2o <- explain(model = model_h2o, data = titanic_small[ , -1], y = as.numeric(as.character(titanic_small$Survived)),
                                 predict_function = custom_predict_h2o, label = "MLP_h2o")
explainer_titanic_keras <- explain(model = model_keras, data = titanic_small_x, y = as.numeric(titanic_small_y),
                                   predict_function = predict, label = "MLP_keras")
```

# Model performance

Function `model_performance()` calculates predictions and residuals for validation dataset.

```{r, cache=TRUE}
mp_titinic_h2o <- model_performance(explainer_titanic_h2o)
mp_titanic_keras <- model_performance(explainer_titanic_keras)
```

Generic function `print()` returns quantiles for residuals.

```{r, results='markup'}
mp_titinic_h2o
```

```{r, results='markup'}
mp_titanic_keras
```

Generic function `plot()` shows reversed empirical cumulative distribution function for absolute values from residuals. Plots can be generated for one or more models.

```{r, cache=TRUE}
plot(mp_titinic_h2o, mp_titanic_keras)
```

We are also able to use the `plot()` function to get an alternative comparison of residuals. Setting the `geom = "boxplot"` parameter we can compare the distribution of residuals for selected models.

```{r}
plot(mp_titinic_h2o, mp_titanic_keras, geom = "boxplot")
```

# Variable importance

Using he DALEX package we are able to better understand which variables are important.

Model agnostic variable importance is calculated by means of permutations. We simply substract the loss function calculated for validation dataset with permuted values for a single variable from the loss function calculated for validation dataset.

This method is implemented in the `variable_importance()` function.

```{r, cache=TRUE}
vi_titinic_h2o <- variable_importance(explainer_titanic_h2o)
vi_titinic_keras <- variable_importance(explainer_titanic_keras)
```

We can compare all models using the generic `plot()` function.

```{r}
plot(vi_titinic_h2o, vi_titinic_keras)
```

Length of the interval coresponds to a variable importance. Longer interval means larger loss, so the variable is more important.

For better comparison of the models we can hook the variabe importance at 0 using the `type="difference"`.

```{r, cache=TRUE}
vi_titinic_h2o <- variable_importance(explainer_titanic_h2o, type="difference")
vi_titinic_keras <- variable_importance(explainer_titanic_keras, type="difference")
plot(vi_titinic_h2o, vi_titinic_keras)
```

# Variable response

Explainers presented in this section are designed to better understand the relation between a variable and model output.

For more details of methods desribed in this section see [Variable response section in DALEX docs](https://pbiecek.github.io/DALEX_docs/3-3-variableResponse.html).

## Partial Dependence Plot

Partial Dependence Plots (PDP) are one of the most popular methods for exploration of the relation between a continuous variable and the model outcome.

Function `variable_response()` with the parameter `type = "pdp"` calls `pdp::partial()` function to calculate PDP response.

```{r}
vr_age_h2o  <- variable_response(explainer_titanic_h2o, variable =  "Age")
vr_age_keras  <- variable_response(explainer_titanic_keras, variable =  "Age")
plot(vr_age_h2o, vr_age_keras)
```

## Acumulated Local Effects plot

Acumulated Local Effects (ALE) plot is the extension of PDP, that is more suited for highly correlated variables.

Function `variable_response()` with the parameter `type = "ale"` calls `ALEPlot::ALEPlot()` function to calculate the ALE curve for the variable `Age`.

```{r, cache=TRUE}
vr_age_h2o  <- variable_response(explainer_titanic_h2o, variable =  "Age", type = "ale")
vr_age_keras  <- variable_response(explainer_titanic_keras, variable =  "Age", type = "ale")
plot(vr_age_h2o, vr_age_keras)
```

# Prediction understanding

The function `prediction_breakdown()` is a wrapper around a [breakDown](https://CRAN.R-project.org/package=breakDown) package.
Model prediction is visualized with Break Down Plots, which show the contribution of every variable present in the model.
Function `prediction_breakdown()` generates variable attributions for selected prediction. The generic `plot()` function shows these attributions.


```{r, cache=TRUE}
sp_h2o <- prediction_breakdown(explainer_titanic_h2o, henry)
sp_keras <- prediction_breakdown(explainer_titanic_keras, henry_keras)
plot(sp_h2o, sp_keras)
```
