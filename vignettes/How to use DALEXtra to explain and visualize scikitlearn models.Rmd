---
title: "How to use DALEXtra to explain and visualize scikitlearn models"
author: "Szymon Maksymiuk"
date: "1 August 2019"
output: 
  html_document:
    toc: true  
    toc_float: true
    number_sections: true
---
```{r include=FALSE}
reticulate::use_condaenv("myenv")
```

```{r setup}
# system.file() function extracts files from package files that are going to be used with examples.
titanic_test <- read.csv(system.file("extdata", "titanic_test.csv", package = "DALEXtra")) 
titanic_train <- read.csv(system.file("extdata", "titanic_train.csv", package = "DALEXtra"))
yml <- system.file("extdata", "scikitlearn.yml", package = "DALEXtra")
pkl_gbm <- system.file("extdata", "scikitlearn.pkl", package = "DALEXtra")
pkl_SGDC <- "SGDC.pkl"
```

# Introduction

Everyone knows *DALEX*. It is tool designed to work with various black-box models like tree ensembles, linear models, neural networks etc. In fact not everyone is familiar with R and prefer to build models using Python *scikit-learn*. Thats why *DALEXtra* was created. Our goal is to help integrate *DALEX* with other environments, libraries (eg. mlr), or even progarmming languages (Pyhton and java). 

To illustrate applications of *DALEXtra* we use preprocessed *titanic* dataset. Original version is available in the base *DALEX* package, while modified is avaiable along with *DALEXtra* (see first chunk). Our goal is to make predictions using scikit-learn Python models and explain them using *DALEX*. Most of the common erroros will be coverd along the following vignette.

# Creating Python model

In order to save created scikit-learn model you have to use pickle library

```{python eval = FALSE}
import pandas as pd 
import pickle
import sklearn.ensemble
titanic_train = pd.read_csv('https://raw.githubusercontent.com/ModelOriented/DALEXtra/master/inst/extdata/titanic_train.csv', delimiter=",")
titanic_test = pd.read_csv('https://raw.githubusercontent.com/ModelOriented/DALEXtra/master/inst/extdata/titanic_test.csv', delimiter=",")
titanic_train = titanic_train[['gender.female', 'gender.male', 'age', 'class.1st',
       'class.2nd', 'class.3rd', 'class.deck.crew', 'class.engineering.crew',
       'class.restaurant.staff', 'class.victualling.crew', 'embarked.Belfast',
       'embarked.Cherbourg', 'embarked.Queenstown', 'embarked.Southampton',
       'fare', 'sibsp', 'parch', 'survived']]
titanic_test = titanic_test[['gender.female', 'gender.male', 'age', 'class.1st',
       'class.2nd', 'class.3rd', 'class.deck.crew', 'class.engineering.crew',
       'class.restaurant.staff', 'class.victualling.crew', 'embarked.Belfast',
       'embarked.Cherbourg', 'embarked.Queenstown', 'embarked.Southampton',
       'fare', 'sibsp', 'parch', 'survived']]
titanic_train_Y = titanic_train[['survived']]
titanic_train_X = titanic_train[['gender.female', 'gender.male', 'age', 'class.1st',
       'class.2nd', 'class.3rd', 'class.deck.crew', 'class.engineering.crew',
       'class.restaurant.staff', 'class.victualling.crew', 'embarked.Belfast',
       'embarked.Cherbourg', 'embarked.Queenstown', 'embarked.Southampton', 'sibsp', 'parch', 'fare']]
titanic_test_Y = titanic_test[['survived']]
titanic_test_X = titanic_test[['gender.female', 'gender.male', 'age', 'class.1st',
       'class.2nd', 'class.3rd', 'class.deck.crew', 'class.engineering.crew',
       'class.restaurant.staff', 'class.victualling.crew', 'embarked.Belfast',
       'embarked.Cherbourg', 'embarked.Queenstown', 'embarked.Southampton',
       'fare', 'sibsp', 'parch']]
model = sklearn.ensemble.GradientBoostingClassifier(
  n_estimators= 5000,
  learning_rate=0.001, 
  max_depth=4, 
  min_samples_split = 12
)
model = model.fit(titanic_train_X, titanic_train_Y)
pickle.dump(model, open("scikitlearn.pkl", "wb"))
```

Note that .pkl file is created based on specific libraries and Python version. Thats why it is highly recommended to save your environment, otherwise a friend of yours, who would like to run your model may find difficulties doing so. In order to extract crucial information about your env, lunch your anaconda prompt, activate environment executing `activate name_of_the_env` and export .yml file `conda env export > environment.yml`. In the further chapters it will be explained how to use that file.

# Model created and explained using different machines

## Environment creation

Python along with it libraries is version sensitive. Thats why we need tool that let's us arrange launch environment for model we have got and want to explain. Unfortuantely due to system limitations, usecase works sligtly different for windows and unix-like OS.

### Windows

First thing is that anconda has to be in system PATH. Othrwise Pyhton libraries would not download due to SSL verification problems. When you mange to solve that issue, free way to creating explainer lays in front of you. Just specify path to .yml file. Do not bother about virtual env name, it is defined in the header of .yml file. `condaenv` argument will be omitted when `yml` specified and OS is windows.

```{r eval=FALSE}
explainer_3 <- explain_scikitlearn(pkl_gbm, yml = yml, data = titanic_test[,1:17], y = titanic_test[,18])
```

### Unix

It is recommended to pass path where Anaconda may be found (eg. ./anaconda3) along with yml file path. `condaenv` argument should be used for this purpose. Therefore, when `yml` argument is specified and OS is Unix, `condaenv` means path to conda. If user left `condaenv` as NULL, DALEXtra will try to look for Anaconda on it's own.

```{r eval=FALSE}
explainer_4 <- explain_scikitlearn(pkl_gbm, yml = yml, condaenv = "/home/user/anaconda", data = titanic_test[,1:17], y = titanic_test[,18])
```

# Common problems


## ResolvePackageNotFound

This is probably the most common problem, any user can meet during usage of `DALXtra`. It is being thrown when packages specified in .yml are not avialable from channels you have specified. There are some ways of fixing it.

### Version

When .yml is exported, it saves not only main version (eg. 1.16.4) but specific OS related version aswell (eg. py36h19fb1c0_0). This way, when you are trying to rebuild env using different machine, a lot of problems may occur. The easiest way to fix it, is removing that statement.

```{r eval=FALSE}
R/win-library/3.6/DALEXtra/extdata/scikitlearn.yml

name: myenv
channels:
  - defaults
dependencies:
  - blas=1.0=mkl
  - certifi=2019.6.16=py36_0
  - icc_rt=2019.0.0=h0cc432a_1
  - intel-openmp=2019.4=245
  - joblib=0.13.2=py36_0
  - mkl=2019.4=245
  - mkl-service=2.0.2=py36he774522_0
  - mkl_fft=1.0.12=py36h14836fe_0
  - mkl_random=1.0.2=py36h343c172_0
  - numpy=1.16.4=py36h19fb1c0_0
  - numpy-base=1.16.4=py36hc3f5095_0
  - pip=19.1.1=py36_0
  - python=3.6.8=h9f7ef89_7
  - scikit-learn=0.21.2=py36h6288b17_0
  - scipy=1.2.1=py36h29ff71c_0
  - setuptools=41.0.1=py36_0
  - six=1.12.0=py36_0
  - sqlite=3.28.0=he774522_0
  - vc=14.1=h0510ff6_4
  - vs2015_runtime=14.15.26706=h3a45250_4
  - wheel=0.33.4=py36_0
  - wincertstore=0.2=py36h7fe50ca_0
  - pandas
```

```{r, eval=FALSE}
R/win-library/3.6/DALEXtra/extdata/scikitlearn_unix.yml


name: myenv
channels:
  - defaults
dependencies:
  - blas=1.0=mkl
  - certifi=2019.6.16
  - intel-openmp
  - joblib=0.13.2
  - mkl=2019.4
  - mkl-service=2.0.2
  - mkl_fft=1.0.12
  - mkl_random=1.0.2
  - numpy=1.16.4
  - numpy-base=1.16.4
  - pip=19.1.1
  - python=3.6.8
  - scikit-learn=0.21.2
  - scipy=1.2.1
  - setuptools=41.0.1
  - six=1.12.0
  - sqlite=3.28.0
  - wheel=0.33.4
  - pandas
```

as it may be seen, not only specific version but some packages disappeared aswell. It is beacuse some libraries are necessary only for Windows, and some only for unix. In order to determinate which packegs you should remove, look at error message, with what packages there is a problem.

### Package not avialble from standard conda repo
conda, has a lot of repositories where packages are stored. Some outdated one are moved to older repos. In order to get them, just add links in .yml header.

```{r eval=FALSE}
name: deeplearning
channels:
- defaults
- https://repo.continuum.io/pkgs/free/win-64/
- conda-forge
- ericmjl
dependencies:
- python=3.6
- matplotlib=2.0.2
- jupyter=1.0.0
- numpy=1.13.1
- seaborn=0.8
- pymc3=3.1
- pandas=0.20.3
- scipy=0.19.1
- biopython=1.69
```

### pip statement 

If nothing above works, there is also a way to use pip statement. Keep in mind that it is not recommended solution but.

```{r eval = FALSE}
name: PATS
channels:
  - conda-forge
dependencies:
  - python=3
  - vtk>=8.2.0
  - numpy
  - matplotlib
  - scipy
  - pip:
    - pynrrd
    - pyqt5
    - pydicom
    - colorama
    - nibabel
    - matplotlib
    - imageio
    - pywt
    - polarTransform
    - pyqt5ac
```

## Environment already exists
If you provide .yml file that in its header contatins name exact to name of environment that already exists, existing will be set active without changing it.

You have two ways of solving that issue. Both connected with anaconda prompt. First is removing conda env with command: 
```{r eval = FALSE}
conda env remove --name myenv
```
And execute function once again. Second is updating env via: 
```{r eval = FALSE}
conda env create -f environment.yml
```

keep in mind that conda env may be removed using R aswell
```{r eval=FALSE}
reticulate::conda_remove("name_of_the_env")
```

## Create conda env manually

Creating environments is a very delicate use case, so when nothing above works try to create it by yourself via conda prompt or using `reticulate` library.

```{r, eval=FALSE}
conda prompt

conda create -n name_of_env python=3.4 
conda install -n name_of_env name_of_package=0.20 
```


```{r, eval=FALSE}
reticulate

reticulate::conda_install(envname = "myenv", packages = )
```


# Model created and explained using the same machine

## Default env

One of the most common use case. R tools for XAI are much more developed, therefore its quite obvious that you want to explain model using for exmaple DALEX, even if it was created using scikit-learn. When creation and explanation process are held using the same machine, you will probably not witness environmente problems, so usage is simple.

```{r}
suppressMessages(library(DALEXtra))
explainer_1 <- explain_scikitlearn(pkl_gbm, data = titanic_test[,1:17], y = titanic_test[,18])
```

Keep in mind, that scikit-learn models does not store training data, so it is required to pass test data in order to create explainer.

```{r message=FALSE}
library(DALEX)
library(iBreakDown)
plot(model_performance(explainer_1))
plot(break_down(explainer_1, new_observation = explainer_1$data[1,]))
```

As we can see, our explainer works perfectly fine and can be used by any package from DrWhy.ai universe.

## Specifying environment

Sometimes we compute things using virtual environment, thats why `explain_scikitlearn()` function allow users to specify Python version that will be in use. `condaenv` and `env` are mutally exclusive arguments. First of them is a string that determintes conda virtual environment. We pass it using name of a virtual env. Second one is a path to virtual environemnt (non conda one). This my cause problems when OS is windows due to `reticulate` not supporting that way of sepcifying env on windows.

```{r message=FALSE}
library(ingredients)
explainer_2 <- explain_scikitlearn(pkl_SGDC, condaenv = "myenv", data = titanic_test[,1:17], y = titanic_test[,18])
plotD3(feature_importance(explainer_2))
plotD3(feature_importance(explainer_1))
```

