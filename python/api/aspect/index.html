<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dalex.aspect API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<style>.homelink{display:block;font-size:2em;font-weight:bold;color:#555;padding-bottom:.5em;border-bottom:1px solid silver}.homelink:hover{color:inherit}.homelink img{max-width:50%;max-height:6em;margin:auto;margin-bottom:.3em}</style>
<link rel="canonical" href="https://dalex.drwhy.ai/python/api/dalex/aspect/">
<link rel="icon" type="image/png" href="https://dalex.drwhy.ai/favicon.svg"/>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dalex.aspect</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/__init__.py#L1-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">from .object import Aspect
from ._predict_aspect_importance import PredictAspectImportance
from ._model_aspect_importance import ModelAspectImportance
from ._predict_triplot import PredictTriplot
from ._model_triplot import ModelTriplot

__all__ = [
    &#34;Aspect&#34;,
    &#34;PredictAspectImportance&#34;,
    &#34;ModelAspectImportance&#34;,
    &#34;PredictTriplot&#34;,
    &#34;ModelTriplot&#34;
    ]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="dalex.aspect.checks" href="checks.html">dalex.aspect.checks</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="dalex.aspect.plot" href="plot.html">dalex.aspect.plot</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="dalex.aspect.utils" href="utils.html">dalex.aspect.utils</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dalex.aspect.Aspect"><code class="flex name class">
<span>class <span class="ident">Aspect</span></span>
<span>(</span><span>explainer, depend_method='assoc', clust_method='complete', corr_method='spearman', agg_method='max')</span>
</code></dt>
<dd>
<div class="desc"><p>Create Aspect</p>
<p>Explanation methods that do not take into account dependencies between variables
can produce misleading results. This class creates a representation of a model based
on an Explainer object. In addition, it calculates the relationships between
the variables that can be used to create explanations. Methods of this class produce
explanation objects, that contain the main result attribute, and can be visualised
using the plot method.</p>
<p>The <code>explainer</code> is the only required parameter.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>explainer</code></strong> :&ensp;<code>Explainer <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Model wrapper created using the Explainer class.</dd>
<dt><strong><code>depend_method</code></strong> :&ensp;<code>{'assoc', 'pps'}</code> or <code>function</code>, optional</dt>
<dd>The method of calculating the dependencies between variables (i.e. the dependency
matrix). Default is <code>'assoc'</code>, which means the use of statistical association
(correlation coefficient, Cram√©r's V based on Pearson's chi-squared statistic
and eta-quared based on Kruskal-Wallis H-statistic);
<code>'pps'</code> stands for Power Predictive Score.
NOTE: When a function is passed, it is called with the <code>explainer.data</code> and it
must return a symmetric dependency matrix (<code>pd.DataFrame</code> with variable names as
columns and rows).</dd>
<dt><strong><code>clust_method</code></strong> :&ensp;<code>{'complete', 'single', 'average', 'weighted', 'centroid', 'median', 'ward'}</code>, optional</dt>
<dd>The linkage algorithm to use for variables hierarchical clustering
(default is <code>'complete'</code>).</dd>
<dt><strong><code>corr_method</code></strong> :&ensp;<code>{'spearman', 'pearson', 'kendall'}</code>, optional</dt>
<dd>The method of calculating correlation between numerical variables
(default is <code>'spearman'</code>).
NOTE: Ignored if <code>depend_method</code> is not <code>'assoc'</code>.</dd>
<dt><strong><code>agg_method</code></strong> :&ensp;<code>{'max', 'min', 'avg'}</code>, optional</dt>
<dd>The method of aggregating the PPS values for pairs of variables
(default is <code>'max'</code>).
NOTE: Ignored if <code>depend_method</code> is not <code>'pps'</code>.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>explainer</code></strong> :&ensp;<code>Explainer <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Model wrapper created using the Explainer class.</dd>
<dt><strong><code>depend_method</code></strong> :&ensp;<code>{'assoc', 'pps'}</code> or <code>function</code></dt>
<dd>The method of calculating the dependencies between variables.</dd>
<dt><strong><code>clust_method</code></strong> :&ensp;<code>{'complete', 'single', 'average', 'weighted', 'centroid', 'median', 'ward'}</code></dt>
<dd>The linkage algorithm to use for variables hierarchical clustering.</dd>
<dt><strong><code>corr_method</code></strong> :&ensp;<code>{'spearman', 'pearson', 'kendall'}</code></dt>
<dd>The method of calculating correlation between numerical variables.</dd>
<dt><strong><code>agg_method</code></strong> :&ensp;<code>{'max', 'min', 'avg'}</code></dt>
<dd>The method of aggregating the PPS values for pairs of variables.</dd>
<dt><strong><code>depend_matrix</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The dependency matrix (with variable names as columns and rows).</dd>
</dl>
<p>linkage_matrix :
The hierarchical clustering of variables encoded as a <code>scipy</code> linkage matrix.</p>
<h2 id="notes">Notes</h2>
<ul>
<li>assoc, eta-squared: <a href="http://tss.awf.poznan.pl/files/3_Trends_Vol21_2014__no1_20.pdf">http://tss.awf.poznan.pl/files/3_Trends_Vol21_2014__no1_20.pdf</a></li>
<li>assoc, Cram√©r's V: <a href="http://stats.lse.ac.uk/bergsma/pdf/cramerV3.pdf">http://stats.lse.ac.uk/bergsma/pdf/cramerV3.pdf</a></li>
<li>PPS: <a href="https://github.com/8080labs/ppscore">https://github.com/8080labs/ppscore</a></li>
<li>triplot: <a href="https://arxiv.org/abs/2104.03403">https://arxiv.org/abs/2104.03403</a></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/object.py#L13-L481" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Aspect:
    &#34;&#34;&#34;Create Aspect

    Explanation methods that do not take into account dependencies between variables
    can produce misleading results. This class creates a representation of a model based
    on an Explainer object. In addition, it calculates the relationships between
    the variables that can be used to create explanations. Methods of this class produce
    explanation objects, that contain the main result attribute, and can be visualised
    using the plot method.

    The `explainer` is the only required parameter.

    Parameters
    ----------
    explainer : Explainer object
        Model wrapper created using the Explainer class.
    depend_method: {&#39;assoc&#39;, &#39;pps&#39;} or function, optional
        The method of calculating the dependencies between variables (i.e. the dependency
        matrix). Default is `&#39;assoc&#39;`, which means the use of statistical association
        (correlation coefficient, Cram√©r&#39;s V based on Pearson&#39;s chi-squared statistic 
        and eta-quared based on Kruskal-Wallis H-statistic);
        `&#39;pps&#39;` stands for Power Predictive Score.
        NOTE: When a function is passed, it is called with the `explainer.data` and it
        must return a symmetric dependency matrix (`pd.DataFrame` with variable names as
        columns and rows).
    clust_method : {&#39;complete&#39;, &#39;single&#39;, &#39;average&#39;, &#39;weighted&#39;, &#39;centroid&#39;, &#39;median&#39;, &#39;ward&#39;}, optional
        The linkage algorithm to use for variables hierarchical clustering
        (default is `&#39;complete&#39;`).
    corr_method : {&#39;spearman&#39;, &#39;pearson&#39;, &#39;kendall&#39;}, optional
        The method of calculating correlation between numerical variables
        (default is `&#39;spearman&#39;`).
        NOTE: Ignored if `depend_method` is not `&#39;assoc&#39;`.
    agg_method : {&#39;max&#39;, &#39;min&#39;, &#39;avg&#39;}, optional
        The method of aggregating the PPS values for pairs of variables
        (default is `&#39;max&#39;`).
        NOTE: Ignored if `depend_method` is not `&#39;pps&#39;`.

    Attributes
    --------
    explainer : Explainer object
        Model wrapper created using the Explainer class.
    depend_method : {&#39;assoc&#39;, &#39;pps&#39;} or function
        The method of calculating the dependencies between variables.
    clust_method : {&#39;complete&#39;, &#39;single&#39;, &#39;average&#39;, &#39;weighted&#39;, &#39;centroid&#39;, &#39;median&#39;, &#39;ward&#39;}
        The linkage algorithm to use for variables hierarchical clustering.
    corr_method : {&#39;spearman&#39;, &#39;pearson&#39;, &#39;kendall&#39;}
        The method of calculating correlation between numerical variables.
    agg_method : {&#39;max&#39;, &#39;min&#39;, &#39;avg&#39;}
        The method of aggregating the PPS values for pairs of variables.
    depend_matrix : pd.DataFrame
        The dependency matrix (with variable names as columns and rows).
    linkage_matrix :
        The hierarchical clustering of variables encoded as a `scipy` linkage matrix.

    Notes
    -----
    - assoc, eta-squared: http://tss.awf.poznan.pl/files/3_Trends_Vol21_2014__no1_20.pdf
    - assoc, Cram√©r&#39;s V: http://stats.lse.ac.uk/bergsma/pdf/cramerV3.pdf
    - PPS: https://github.com/8080labs/ppscore
    - triplot: https://arxiv.org/abs/2104.03403
    &#34;&#34;&#34;

    def __init__(
        self,
        explainer,
        depend_method=&#34;assoc&#34;,
        clust_method=&#34;complete&#34;,
        corr_method=&#34;spearman&#34;,
        agg_method=&#34;max&#34;,
    ):  
        _depend_method, _corr_method, _agg_method = checks.check_method_depend(depend_method, corr_method, agg_method)
        self.explainer = explainer
        self.depend_method = _depend_method
        self.clust_method = clust_method
        self.corr_method = _corr_method
        self.agg_method = _agg_method
        self.depend_matrix = utils.calculate_depend_matrix(
            self.explainer.data, self.depend_method, self.corr_method, self.agg_method
        )
        self.linkage_matrix = utils.calculate_linkage_matrix(
            self.depend_matrix, clust_method
        )
        self._hierarchical_clustering_dendrogram = plot.plot_dendrogram(
            self.linkage_matrix, self.depend_matrix.columns
        )
        self._dendrogram_aspects_ordered = utils.get_dendrogram_aspects_ordered(
            self._hierarchical_clustering_dendrogram, self.depend_matrix
        )
        self._full_hierarchical_aspect_importance = None
        self._mt_params = None

    def get_aspects(self, h=0.5, n=None):
        from scipy.cluster.hierarchy import fcluster
        &#34;&#34;&#34;Form aspects of variables from the hierarchical clustering

        Parameters
        ----------
        h : float, optional
            Threshold to apply when forming aspects, i.e., the minimum value of the dependency
            between the variables grouped in one aspect (default is `0.5`).
            NOTE: Ignored if `n` is not `None`.
        n : int, optional
            Maximum number of aspects to form 
            (default is `None`, which means the use of `h` parameter).

        Returns
        -------
        dict of lists
            Variables grouped in aspects, e.g. `{&#39;aspect_1&#39;: [&#39;x1&#39;, &#39;x2&#39;], &#39;aspect_2&#39;: [&#39;y1&#39;, &#39;y2&#39;]}`.
        &#34;&#34;&#34;
        if n is None:
            aspect_label = fcluster(self.linkage_matrix, 1 - h, criterion=&#34;distance&#34;)
        else:
            aspect_label = fcluster(self.linkage_matrix, n, criterion=&#34;maxclust&#34;)
        aspects = pd.DataFrame(
            {&#34;feature&#34;: self.depend_matrix.columns, &#34;aspect&#34;: aspect_label}
        )
        aspects = aspects.groupby(&#34;aspect&#34;)[&#34;feature&#34;].apply(list).reset_index()
        aspects_dict = {}

        # rename an aspect when there is a single variable in it
        i = 1
        for index, row in aspects.iterrows():
            if len(row[&#34;feature&#34;]) &gt; 1:
                aspects_dict[f&#34;aspect_{i}&#34;] = row[&#34;feature&#34;]
                i += 1
            else:
                aspects_dict[row[&#34;feature&#34;][0]] = row[&#34;feature&#34;]

        return aspects_dict

    def plot_dendrogram(
        self,
        title=&#34;Hierarchical clustering dendrogram&#34;,
        lines_interspace=20,
        rounding_function=np.round,
        digits=3,
        show=True,
    ):
        &#34;&#34;&#34;Plot the hierarchical clustering dendrogram of variables

        Parameters
        ----------
        title : str, optional
            Title of the plot (default is &#34;Hierarchical clustering dendrogram&#34;).
        lines_interspace : float, optional
            Interspace between lines of dendrogram in px (default is `20`).
        rounding_function : function, optional
            A function that will be used for rounding numbers (default is `np.around`).
        digits : int, optional
            Number of decimal places (`np.around`) to round contributions.
            See `rounding_function` parameter (default is `3`).
        show : bool, optional
            `True` shows the plot; `False` returns the plotly Figure object that can
            be edited or saved using the `write_image()` method (default is `True`).

        Returns
        -------
        None or plotly.graph_objects.Figure
            Return figure that can be edited or saved. See `show` parameter.
        &#34;&#34;&#34;
        m = len(self.depend_matrix.columns)
        plot_height = 78 + 71 + m * lines_interspace + (m + 1) * lines_interspace / 4
        fig = self._hierarchical_clustering_dendrogram
        fig = plot.add_text_and_tooltips_to_dendrogram(
            fig, self._dendrogram_aspects_ordered, rounding_function, digits
        )
        fig = plot._add_points_on_dendrogram_traces(fig)
        fig.update_layout(
            title={&#34;text&#34;: title, &#34;x&#34;: 0.15},
            yaxis={&#34;automargin&#34;: True, &#34;autorange&#34;: &#34;reversed&#34;},
            height=plot_height,
        )
        if show:
            fig.show(config=_theme.get_default_config())
        else:
            return fig

    def predict_parts(
        self,
        new_observation,
        variable_groups=None,
        type=&#34;default&#34;,
        h=0.5,
        N=2000,
        B=25,
        n_aspects=None,
        sample_method=&#34;default&#34;,
        f=2,
        label=None,
        processes=1,
        random_state=None,
    ):
        &#34;&#34;&#34;Calculate predict-level aspect importance

        Parameters
        ----------
        new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
            An observation for which a prediction needs to be explained.
        variable_groups : dict of lists or None
            Variables grouped in aspects to calculate their importance (default is `None`).
        type : {&#39;default&#39;, &#39;shap&#39;}, optional
            Type of aspect importance/attributions (default is `&#39;default&#39;`, which means
            the use of simplified LIME method).
        h : float, optional
            Threshold to apply when forming aspects, i.e., the minimum value of the dependency
            between the variables grouped in one aspect (default is `0.5`).
        N : int, optional
            Number of observations that will be sampled from the `explainer.data` attribute
            before the calculation of aspect importance (default is `2000`).
        B : int, optional
            Parameter specific for `type == &#39;shap&#39;`. Number of random paths to calculate aspect
            attributions (default is `25`).
            NOTE: Ignored if `type` is not `&#39;shap&#39;`.
        n_aspects : int, optional
            Parameter specific for `type == &#39;default&#39;`. Maximum number of non-zero importances, i.e.
            coefficients after lasso fitting (default is `None`, which means the linear regression is used).
            NOTE: Ignored if `type` is not `&#39;default&#39;`.
        sample_method : {&#39;default&#39;, &#39;binom&#39;}, optional
            Parameter specific for `type == &#39;default&#39;`. Sampling method for creating binary matrix
            used as mask for replacing aspects in sampled data (default is `&#39;default&#39;`, which means
            it randomly replaces one or two zeros per row; `&#39;binom&#39;` replaces random number of zeros
            per row).
            NOTE: Ignored if `type` is not `&#39;default&#39;`.
        f : int, optional
            Parameter specific for `type == &#39;default&#39;` and `sample_method == &#39;binom&#39;`. Parameter
            controlling average number of replaced zeros for binomial sampling (default is `2`).
            NOTE: Ignored if `type` is not `&#39;default&#39;` or `sample_method` is not `&#39;binom&#39;`.
        label : str, optional
            Name to appear in result and plots. Overrides default.
        processes : int, optional
            Parameter specific for `type == &#39;shap&#39;`. Number of parallel processes to use in calculations.
            Iterated over `B` (default is `1`, which means no parallel computation).
        random_state : int, optional
            Set seed for random number generator (default is random seed).

        Returns
        -------
        PredictAspectImportance class object
            Explanation object containing the main result attribute and the plot method.
        &#34;&#34;&#34;

        if variable_groups is None:
            variable_groups = self.get_aspects(h)

        pai = PredictAspectImportance(
            variable_groups,
            type,
            N,
            B,
            n_aspects,
            sample_method,
            f,
            self.depend_method,
            self.corr_method,
            self.agg_method,
            processes,
            random_state,
            _depend_matrix=self.depend_matrix
        )

        pai.fit(self.explainer, new_observation)

        if label is not None:
            pai.result[&#34;label&#34;] = label

        return pai

    def model_parts(
        self,
        variable_groups=None,
        h=0.5,
        loss_function=None,
        type=&#34;variable_importance&#34;,
        N=1000,
        B=10,
        processes=1,
        label=None,
        random_state=None,
    ):
        &#34;&#34;&#34;Calculate model-level aspect importance

        Parameters
        ----------
        variable_groups : dict of lists or None
            Variables grouped in aspects to calculate their importance (default is `None`).
        h : float, optional
            Threshold to apply when forming aspects, i.e., the minimum value of the dependency
            between the variables grouped in one aspect (default is `0.5`).
        loss_function :  {&#39;rmse&#39;, &#39;1-auc&#39;, &#39;mse&#39;, &#39;mae&#39;, &#39;mad&#39;} or function, optional
            If string, then such loss function will be used to assess aspect importance
            (default is `&#39;rmse&#39;` or `&#39;1-auc&#39;`, depends on `explainer.model_type` attribute).
        type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;}, optional
            Type of transformation that will be applied to dropout loss
            (default is `&#39;variable_importance&#39;`, which is Permutational Variable Importance).
        N : int, optional
            Number of observations that will be sampled from the `explainer.data` attribute before
            the calculation of aspect importance. `None` means all `data` (default is `1000`).
        B : int, optional
            Number of permutation rounds to perform on each variable (default is `10`).
        processes : int, optional
            Number of parallel processes to use in calculations. Iterated over `B`
            (default is `1`, which means no parallel computation).
        label : str, optional
            Name to appear in result and plots. Overrides default.
        random_state : int, optional
            Set seed for random number generator (default is random seed).

        Returns
        -------
        ModelAspectImportance class object
            Explanation object containing the main result attribute and the plot method.
        &#34;&#34;&#34;

        loss_function = checks.check_method_loss_function(self.explainer, loss_function)
        mai_result = None

        if variable_groups is None:
            variable_groups = self.get_aspects(h)

            # get results from triplot if it was precalculated with the same params
            if self._full_hierarchical_aspect_importance is not None:
                if (
                    self._mt_params[&#34;loss_function&#34;] == loss_function
                    and self._mt_params[&#34;N&#34;] == N
                    and self._mt_params[&#34;B&#34;] == B
                    and self._mt_params[&#34;type&#34;] == type
                ):
                    h = min(1, h)
                    h_selected = np.unique(
                        self._full_hierarchical_aspect_importance.loc[
                            self._full_hierarchical_aspect_importance.h &gt;= h
                        ].h
                    )[0]
                    mai_result = self._full_hierarchical_aspect_importance.loc[
                        self._full_hierarchical_aspect_importance.h == h_selected
                    ]

        ai = ModelAspectImportance(
            loss_function=loss_function,
            type=type,
            N=N,
            B=B,
            variable_groups=variable_groups,
            processes=processes,
            random_state=random_state,
            _depend_matrix=self.depend_matrix
        )

        # calculate if there was no results
        if mai_result is None:
            ai.fit(self.explainer)
        else: 
            mai_result = mai_result[
                [
                    &#34;aspect_name&#34;,
                    &#34;variable_names&#34;,
                    &#34;dropout_loss&#34;,
                    &#34;dropout_loss_change&#34;,
                    &#34;min_depend&#34;,
                    &#34;vars_min_depend&#34;,
                    &#34;label&#34;,
                ]
            ]
            ai.result = mai_result

        if label is not None:
            ai.result[&#34;label&#34;] = label

        return ai

    def predict_triplot(
        self,
        new_observation,
        type=&#34;default&#34;,
        N=2000,
        B=25,
        sample_method=&#34;default&#34;,
        f=2,
        processes=1,
        random_state=None,
    ):
        &#34;&#34;&#34;Calculate predict-level hierarchical aspect importance

        Parameters
        ----------
        new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
            An observation for which a prediction needs to be explained.
        type : {&#39;default&#39;, &#39;shap&#39;}, optional
            Type of aspect importance/attributions (default is `&#39;default&#39;`, which means
            the use of simplified LIME method).
        N : int, optional
            Number of observations that will be sampled from the `explainer.data` attribute
            before the calculation of aspect importance (default is `2000`).
        B : int, optional
            Parameter specific for `type == &#39;shap&#39;`. Number of random paths to calculate aspect
            attributions (default is `25`).
            NOTE: Ignored if `type` is not `&#39;shap&#39;`.
        sample_method : {&#39;default&#39;, &#39;binom&#39;}, optional
            Parameter specific for `type == &#39;default&#39;`. Sampling method for creating binary matrix
            used as mask for replacing aspects in data (default is `&#39;default&#39;`, which means
            it randomly replaces one or two zeros per row; `&#39;binom&#39;` replaces random number of zeros
            per row).
            NOTE: Ignored if `type` is not `&#39;default&#39;`.
        f : int, optional
            Parameter specific for `type == &#39;default&#39;` and `sample_method == &#39;binom&#39;`. Parameter
            controlling average number of replaced zeros for binomial sampling (default is `2`).
            NOTE: Ignored if `type` is not `&#39;default&#39;` or `sample_method` is not `&#39;binom&#39;`.
        processes : int, optional
            Number of parallel processes to use in calculations. Iterated over `B`
            (default is `1`, which means no parallel computation).
        random_state : int, optional
            Set seed for random number generator (default is random seed).

        Returns
        -------
        PredictTriplot class object
            Explanation object containing the main result attribute and the plot method.
        &#34;&#34;&#34;

        pt = PredictTriplot(type, N, B, sample_method, f, processes, random_state)

        pt.fit(self, new_observation)

        return pt

    def model_triplot(
        self,
        loss_function=None,
        type=&#34;variable_importance&#34;,
        N=1000,
        B=10,
        processes=1,
        random_state=None,
    ):
        &#34;&#34;&#34;Calculate model-level hierarchical aspect importance

        Parameters
        ----------
        loss_function :  {&#39;rmse&#39;, &#39;1-auc&#39;, &#39;mse&#39;, &#39;mae&#39;, &#39;mad&#39;} or function, optional
            If string, then such loss function will be used to assess aspect importance
            (default is `&#39;rmse&#39;` or `&#39;1-auc&#39;`, depends on `explainer.model_type` attribute).
        type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;}, optional
            Type of transformation that will be applied to dropout loss
            (default is `&#39;variable_importance&#39;`, which is Permutational Variable Importance).
        N : int, optional
            Number of observations that will be sampled from the `explainer.data` attribute before
            the calculation of aspect importance. `None` means all `data` (default is `1000`).
        B : int, optional
            Number of permutation rounds to perform on each variable (default is `10`).
        processes : int, optional
            Number of parallel processes to use in calculations. Iterated over `B`
            (default is `1`, which means no parallel computation).
        random_state : int, optional
            Set seed for random number generator (default is random seed).

        Returns
        -------
        ModelTriplot class object
            Explanation object containing the main result attribute and the plot method.
        &#34;&#34;&#34;

        
        loss_function = checks.check_method_loss_function(self.explainer, loss_function) # get proper loss_function for model_type
        mt = ModelTriplot(loss_function, type, N, B, processes, random_state)
        self._mt_params = {&#34;loss_function&#34;: loss_function, &#34;type&#34;: type, &#34;N&#34;: N, &#34;B&#34;: B} # save params for future calls of model_parts
        mt.fit(self)

        return mt</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="dalex.aspect.Aspect.get_aspects"><code class="name flex">
<span>def <span class="ident">get_aspects</span></span>(<span>self, h=0.5, n=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/object.py#L104-L142" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def get_aspects(self, h=0.5, n=None):
    from scipy.cluster.hierarchy import fcluster
    &#34;&#34;&#34;Form aspects of variables from the hierarchical clustering

    Parameters
    ----------
    h : float, optional
        Threshold to apply when forming aspects, i.e., the minimum value of the dependency
        between the variables grouped in one aspect (default is `0.5`).
        NOTE: Ignored if `n` is not `None`.
    n : int, optional
        Maximum number of aspects to form 
        (default is `None`, which means the use of `h` parameter).

    Returns
    -------
    dict of lists
        Variables grouped in aspects, e.g. `{&#39;aspect_1&#39;: [&#39;x1&#39;, &#39;x2&#39;], &#39;aspect_2&#39;: [&#39;y1&#39;, &#39;y2&#39;]}`.
    &#34;&#34;&#34;
    if n is None:
        aspect_label = fcluster(self.linkage_matrix, 1 - h, criterion=&#34;distance&#34;)
    else:
        aspect_label = fcluster(self.linkage_matrix, n, criterion=&#34;maxclust&#34;)
    aspects = pd.DataFrame(
        {&#34;feature&#34;: self.depend_matrix.columns, &#34;aspect&#34;: aspect_label}
    )
    aspects = aspects.groupby(&#34;aspect&#34;)[&#34;feature&#34;].apply(list).reset_index()
    aspects_dict = {}

    # rename an aspect when there is a single variable in it
    i = 1
    for index, row in aspects.iterrows():
        if len(row[&#34;feature&#34;]) &gt; 1:
            aspects_dict[f&#34;aspect_{i}&#34;] = row[&#34;feature&#34;]
            i += 1
        else:
            aspects_dict[row[&#34;feature&#34;][0]] = row[&#34;feature&#34;]

    return aspects_dict</code></pre>
</details>
</dd>
<dt id="dalex.aspect.Aspect.model_parts"><code class="name flex">
<span>def <span class="ident">model_parts</span></span>(<span>self, variable_groups=None, h=0.5, loss_function=None, type='variable_importance', N=1000, B=10, processes=1, label=None, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate model-level aspect importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>variable_groups</code></strong> :&ensp;<code>dict</code> of <code>lists</code> or <code>None</code></dt>
<dd>Variables grouped in aspects to calculate their importance (default is <code>None</code>).</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Threshold to apply when forming aspects, i.e., the minimum value of the dependency
between the variables grouped in one aspect (default is <code>0.5</code>).</dd>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>{'rmse', '1-auc', 'mse', 'mae', 'mad'}</code> or <code>function</code>, optional</dt>
<dd>If string, then such loss function will be used to assess aspect importance
(default is <code>'rmse'</code> or <code>'1-auc'</code>, depends on <code>explainer.model_type</code> attribute).</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'variable_importance', 'ratio', 'difference'}</code>, optional</dt>
<dd>Type of transformation that will be applied to dropout loss
(default is <code>'variable_importance'</code>, which is Permutational Variable Importance).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>explainer.data</code> attribute before
the calculation of aspect importance. <code>None</code> means all <code>data</code> (default is <code>1000</code>).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of permutation rounds to perform on each variable (default is <code>10</code>).</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>
(default is <code>1</code>, which means no parallel computation).</dd>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name to appear in result and plots. Overrides default.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="dalex.aspect.ModelAspectImportance" href="#dalex.aspect.ModelAspectImportance">ModelAspectImportance</a> class <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Explanation object containing the main result attribute and the plot method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/object.py#L281-L382" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def model_parts(
    self,
    variable_groups=None,
    h=0.5,
    loss_function=None,
    type=&#34;variable_importance&#34;,
    N=1000,
    B=10,
    processes=1,
    label=None,
    random_state=None,
):
    &#34;&#34;&#34;Calculate model-level aspect importance

    Parameters
    ----------
    variable_groups : dict of lists or None
        Variables grouped in aspects to calculate their importance (default is `None`).
    h : float, optional
        Threshold to apply when forming aspects, i.e., the minimum value of the dependency
        between the variables grouped in one aspect (default is `0.5`).
    loss_function :  {&#39;rmse&#39;, &#39;1-auc&#39;, &#39;mse&#39;, &#39;mae&#39;, &#39;mad&#39;} or function, optional
        If string, then such loss function will be used to assess aspect importance
        (default is `&#39;rmse&#39;` or `&#39;1-auc&#39;`, depends on `explainer.model_type` attribute).
    type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;}, optional
        Type of transformation that will be applied to dropout loss
        (default is `&#39;variable_importance&#39;`, which is Permutational Variable Importance).
    N : int, optional
        Number of observations that will be sampled from the `explainer.data` attribute before
        the calculation of aspect importance. `None` means all `data` (default is `1000`).
    B : int, optional
        Number of permutation rounds to perform on each variable (default is `10`).
    processes : int, optional
        Number of parallel processes to use in calculations. Iterated over `B`
        (default is `1`, which means no parallel computation).
    label : str, optional
        Name to appear in result and plots. Overrides default.
    random_state : int, optional
        Set seed for random number generator (default is random seed).

    Returns
    -------
    ModelAspectImportance class object
        Explanation object containing the main result attribute and the plot method.
    &#34;&#34;&#34;

    loss_function = checks.check_method_loss_function(self.explainer, loss_function)
    mai_result = None

    if variable_groups is None:
        variable_groups = self.get_aspects(h)

        # get results from triplot if it was precalculated with the same params
        if self._full_hierarchical_aspect_importance is not None:
            if (
                self._mt_params[&#34;loss_function&#34;] == loss_function
                and self._mt_params[&#34;N&#34;] == N
                and self._mt_params[&#34;B&#34;] == B
                and self._mt_params[&#34;type&#34;] == type
            ):
                h = min(1, h)
                h_selected = np.unique(
                    self._full_hierarchical_aspect_importance.loc[
                        self._full_hierarchical_aspect_importance.h &gt;= h
                    ].h
                )[0]
                mai_result = self._full_hierarchical_aspect_importance.loc[
                    self._full_hierarchical_aspect_importance.h == h_selected
                ]

    ai = ModelAspectImportance(
        loss_function=loss_function,
        type=type,
        N=N,
        B=B,
        variable_groups=variable_groups,
        processes=processes,
        random_state=random_state,
        _depend_matrix=self.depend_matrix
    )

    # calculate if there was no results
    if mai_result is None:
        ai.fit(self.explainer)
    else: 
        mai_result = mai_result[
            [
                &#34;aspect_name&#34;,
                &#34;variable_names&#34;,
                &#34;dropout_loss&#34;,
                &#34;dropout_loss_change&#34;,
                &#34;min_depend&#34;,
                &#34;vars_min_depend&#34;,
                &#34;label&#34;,
            ]
        ]
        ai.result = mai_result

    if label is not None:
        ai.result[&#34;label&#34;] = label

    return ai</code></pre>
</details>
</dd>
<dt id="dalex.aspect.Aspect.model_triplot"><code class="name flex">
<span>def <span class="ident">model_triplot</span></span>(<span>self, loss_function=None, type='variable_importance', N=1000, B=10, processes=1, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate model-level hierarchical aspect importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>{'rmse', '1-auc', 'mse', 'mae', 'mad'}</code> or <code>function</code>, optional</dt>
<dd>If string, then such loss function will be used to assess aspect importance
(default is <code>'rmse'</code> or <code>'1-auc'</code>, depends on <code>explainer.model_type</code> attribute).</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'variable_importance', 'ratio', 'difference'}</code>, optional</dt>
<dd>Type of transformation that will be applied to dropout loss
(default is <code>'variable_importance'</code>, which is Permutational Variable Importance).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>explainer.data</code> attribute before
the calculation of aspect importance. <code>None</code> means all <code>data</code> (default is <code>1000</code>).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of permutation rounds to perform on each variable (default is <code>10</code>).</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>
(default is <code>1</code>, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="dalex.aspect.ModelTriplot" href="#dalex.aspect.ModelTriplot">ModelTriplot</a> class <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Explanation object containing the main result attribute and the plot method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/object.py#L439-L481" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def model_triplot(
    self,
    loss_function=None,
    type=&#34;variable_importance&#34;,
    N=1000,
    B=10,
    processes=1,
    random_state=None,
):
    &#34;&#34;&#34;Calculate model-level hierarchical aspect importance

    Parameters
    ----------
    loss_function :  {&#39;rmse&#39;, &#39;1-auc&#39;, &#39;mse&#39;, &#39;mae&#39;, &#39;mad&#39;} or function, optional
        If string, then such loss function will be used to assess aspect importance
        (default is `&#39;rmse&#39;` or `&#39;1-auc&#39;`, depends on `explainer.model_type` attribute).
    type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;}, optional
        Type of transformation that will be applied to dropout loss
        (default is `&#39;variable_importance&#39;`, which is Permutational Variable Importance).
    N : int, optional
        Number of observations that will be sampled from the `explainer.data` attribute before
        the calculation of aspect importance. `None` means all `data` (default is `1000`).
    B : int, optional
        Number of permutation rounds to perform on each variable (default is `10`).
    processes : int, optional
        Number of parallel processes to use in calculations. Iterated over `B`
        (default is `1`, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).

    Returns
    -------
    ModelTriplot class object
        Explanation object containing the main result attribute and the plot method.
    &#34;&#34;&#34;

    
    loss_function = checks.check_method_loss_function(self.explainer, loss_function) # get proper loss_function for model_type
    mt = ModelTriplot(loss_function, type, N, B, processes, random_state)
    self._mt_params = {&#34;loss_function&#34;: loss_function, &#34;type&#34;: type, &#34;N&#34;: N, &#34;B&#34;: B} # save params for future calls of model_parts
    mt.fit(self)

    return mt</code></pre>
</details>
</dd>
<dt id="dalex.aspect.Aspect.plot_dendrogram"><code class="name flex">
<span>def <span class="ident">plot_dendrogram</span></span>(<span>self, title='Hierarchical clustering dendrogram', lines_interspace=20, rounding_function=&lt;function round_&gt;, digits=3, show=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the hierarchical clustering dendrogram of variables</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Title of the plot (default is "Hierarchical clustering dendrogram").</dd>
<dt><strong><code>lines_interspace</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Interspace between lines of dendrogram in px (default is <code>20</code>).</dd>
<dt><strong><code>rounding_function</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>A function that will be used for rounding numbers (default is <code>np.around</code>).</dd>
<dt><strong><code>digits</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of decimal places (<code>np.around</code>) to round contributions.
See <code>rounding_function</code> parameter (default is <code>3</code>).</dd>
<dt><strong><code>show</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd><code>True</code> shows the plot; <code>False</code> returns the plotly Figure object that can
be edited or saved using the <code>write_image()</code> method (default is <code>True</code>).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code> or <code>plotly.graph_objects.Figure</code></dt>
<dd>Return figure that can be edited or saved. See <code>show</code> parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/object.py#L144-L189" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot_dendrogram(
    self,
    title=&#34;Hierarchical clustering dendrogram&#34;,
    lines_interspace=20,
    rounding_function=np.round,
    digits=3,
    show=True,
):
    &#34;&#34;&#34;Plot the hierarchical clustering dendrogram of variables

    Parameters
    ----------
    title : str, optional
        Title of the plot (default is &#34;Hierarchical clustering dendrogram&#34;).
    lines_interspace : float, optional
        Interspace between lines of dendrogram in px (default is `20`).
    rounding_function : function, optional
        A function that will be used for rounding numbers (default is `np.around`).
    digits : int, optional
        Number of decimal places (`np.around`) to round contributions.
        See `rounding_function` parameter (default is `3`).
    show : bool, optional
        `True` shows the plot; `False` returns the plotly Figure object that can
        be edited or saved using the `write_image()` method (default is `True`).

    Returns
    -------
    None or plotly.graph_objects.Figure
        Return figure that can be edited or saved. See `show` parameter.
    &#34;&#34;&#34;
    m = len(self.depend_matrix.columns)
    plot_height = 78 + 71 + m * lines_interspace + (m + 1) * lines_interspace / 4
    fig = self._hierarchical_clustering_dendrogram
    fig = plot.add_text_and_tooltips_to_dendrogram(
        fig, self._dendrogram_aspects_ordered, rounding_function, digits
    )
    fig = plot._add_points_on_dendrogram_traces(fig)
    fig.update_layout(
        title={&#34;text&#34;: title, &#34;x&#34;: 0.15},
        yaxis={&#34;automargin&#34;: True, &#34;autorange&#34;: &#34;reversed&#34;},
        height=plot_height,
    )
    if show:
        fig.show(config=_theme.get_default_config())
    else:
        return fig</code></pre>
</details>
</dd>
<dt id="dalex.aspect.Aspect.predict_parts"><code class="name flex">
<span>def <span class="ident">predict_parts</span></span>(<span>self, new_observation, variable_groups=None, type='default', h=0.5, N=2000, B=25, n_aspects=None, sample_method='default', f=2, label=None, processes=1, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate predict-level aspect importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>new_observation</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code> or <code>pd.DataFrame (1,p)</code></dt>
<dd>An observation for which a prediction needs to be explained.</dd>
<dt><strong><code>variable_groups</code></strong> :&ensp;<code>dict</code> of <code>lists</code> or <code>None</code></dt>
<dd>Variables grouped in aspects to calculate their importance (default is <code>None</code>).</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'default', 'shap'}</code>, optional</dt>
<dd>Type of aspect importance/attributions (default is <code>'default'</code>, which means
the use of simplified LIME method).</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Threshold to apply when forming aspects, i.e., the minimum value of the dependency
between the variables grouped in one aspect (default is <code>0.5</code>).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>explainer.data</code> attribute
before the calculation of aspect importance (default is <code>2000</code>).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'shap'</code>. Number of random paths to calculate aspect
attributions (default is <code>25</code>).
NOTE: Ignored if <code>type</code> is not <code>'shap'</code>.</dd>
<dt><strong><code>n_aspects</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code>. Maximum number of non-zero importances, i.e.
coefficients after lasso fitting (default is <code>None</code>, which means the linear regression is used).
NOTE: Ignored if <code>type</code> is not <code>'default'</code>.</dd>
<dt><strong><code>sample_method</code></strong> :&ensp;<code>{'default', 'binom'}</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code>. Sampling method for creating binary matrix
used as mask for replacing aspects in sampled data (default is <code>'default'</code>, which means
it randomly replaces one or two zeros per row; <code>'binom'</code> replaces random number of zeros
per row).
NOTE: Ignored if <code>type</code> is not <code>'default'</code>.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code> and <code>sample_method == 'binom'</code>. Parameter
controlling average number of replaced zeros for binomial sampling (default is <code>2</code>).
NOTE: Ignored if <code>type</code> is not <code>'default'</code> or <code>sample_method</code> is not <code>'binom'</code>.</dd>
<dt><strong><code>label</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name to appear in result and plots. Overrides default.</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'shap'</code>. Number of parallel processes to use in calculations.
Iterated over <code>B</code> (default is <code>1</code>, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="dalex.aspect.PredictAspectImportance" href="#dalex.aspect.PredictAspectImportance">PredictAspectImportance</a> class <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Explanation object containing the main result attribute and the plot method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/object.py#L191-L279" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def predict_parts(
    self,
    new_observation,
    variable_groups=None,
    type=&#34;default&#34;,
    h=0.5,
    N=2000,
    B=25,
    n_aspects=None,
    sample_method=&#34;default&#34;,
    f=2,
    label=None,
    processes=1,
    random_state=None,
):
    &#34;&#34;&#34;Calculate predict-level aspect importance

    Parameters
    ----------
    new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
        An observation for which a prediction needs to be explained.
    variable_groups : dict of lists or None
        Variables grouped in aspects to calculate their importance (default is `None`).
    type : {&#39;default&#39;, &#39;shap&#39;}, optional
        Type of aspect importance/attributions (default is `&#39;default&#39;`, which means
        the use of simplified LIME method).
    h : float, optional
        Threshold to apply when forming aspects, i.e., the minimum value of the dependency
        between the variables grouped in one aspect (default is `0.5`).
    N : int, optional
        Number of observations that will be sampled from the `explainer.data` attribute
        before the calculation of aspect importance (default is `2000`).
    B : int, optional
        Parameter specific for `type == &#39;shap&#39;`. Number of random paths to calculate aspect
        attributions (default is `25`).
        NOTE: Ignored if `type` is not `&#39;shap&#39;`.
    n_aspects : int, optional
        Parameter specific for `type == &#39;default&#39;`. Maximum number of non-zero importances, i.e.
        coefficients after lasso fitting (default is `None`, which means the linear regression is used).
        NOTE: Ignored if `type` is not `&#39;default&#39;`.
    sample_method : {&#39;default&#39;, &#39;binom&#39;}, optional
        Parameter specific for `type == &#39;default&#39;`. Sampling method for creating binary matrix
        used as mask for replacing aspects in sampled data (default is `&#39;default&#39;`, which means
        it randomly replaces one or two zeros per row; `&#39;binom&#39;` replaces random number of zeros
        per row).
        NOTE: Ignored if `type` is not `&#39;default&#39;`.
    f : int, optional
        Parameter specific for `type == &#39;default&#39;` and `sample_method == &#39;binom&#39;`. Parameter
        controlling average number of replaced zeros for binomial sampling (default is `2`).
        NOTE: Ignored if `type` is not `&#39;default&#39;` or `sample_method` is not `&#39;binom&#39;`.
    label : str, optional
        Name to appear in result and plots. Overrides default.
    processes : int, optional
        Parameter specific for `type == &#39;shap&#39;`. Number of parallel processes to use in calculations.
        Iterated over `B` (default is `1`, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).

    Returns
    -------
    PredictAspectImportance class object
        Explanation object containing the main result attribute and the plot method.
    &#34;&#34;&#34;

    if variable_groups is None:
        variable_groups = self.get_aspects(h)

    pai = PredictAspectImportance(
        variable_groups,
        type,
        N,
        B,
        n_aspects,
        sample_method,
        f,
        self.depend_method,
        self.corr_method,
        self.agg_method,
        processes,
        random_state,
        _depend_matrix=self.depend_matrix
    )

    pai.fit(self.explainer, new_observation)

    if label is not None:
        pai.result[&#34;label&#34;] = label

    return pai</code></pre>
</details>
</dd>
<dt id="dalex.aspect.Aspect.predict_triplot"><code class="name flex">
<span>def <span class="ident">predict_triplot</span></span>(<span>self, new_observation, type='default', N=2000, B=25, sample_method='default', f=2, processes=1, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate predict-level hierarchical aspect importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>new_observation</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code> or <code>pd.DataFrame (1,p)</code></dt>
<dd>An observation for which a prediction needs to be explained.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'default', 'shap'}</code>, optional</dt>
<dd>Type of aspect importance/attributions (default is <code>'default'</code>, which means
the use of simplified LIME method).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>explainer.data</code> attribute
before the calculation of aspect importance (default is <code>2000</code>).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'shap'</code>. Number of random paths to calculate aspect
attributions (default is <code>25</code>).
NOTE: Ignored if <code>type</code> is not <code>'shap'</code>.</dd>
<dt><strong><code>sample_method</code></strong> :&ensp;<code>{'default', 'binom'}</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code>. Sampling method for creating binary matrix
used as mask for replacing aspects in data (default is <code>'default'</code>, which means
it randomly replaces one or two zeros per row; <code>'binom'</code> replaces random number of zeros
per row).
NOTE: Ignored if <code>type</code> is not <code>'default'</code>.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code> and <code>sample_method == 'binom'</code>. Parameter
controlling average number of replaced zeros for binomial sampling (default is <code>2</code>).
NOTE: Ignored if <code>type</code> is not <code>'default'</code> or <code>sample_method</code> is not <code>'binom'</code>.</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>
(default is <code>1</code>, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="dalex.aspect.PredictTriplot" href="#dalex.aspect.PredictTriplot">PredictTriplot</a> class <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Explanation object containing the main result attribute and the plot method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/object.py#L384-L437" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def predict_triplot(
    self,
    new_observation,
    type=&#34;default&#34;,
    N=2000,
    B=25,
    sample_method=&#34;default&#34;,
    f=2,
    processes=1,
    random_state=None,
):
    &#34;&#34;&#34;Calculate predict-level hierarchical aspect importance

    Parameters
    ----------
    new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
        An observation for which a prediction needs to be explained.
    type : {&#39;default&#39;, &#39;shap&#39;}, optional
        Type of aspect importance/attributions (default is `&#39;default&#39;`, which means
        the use of simplified LIME method).
    N : int, optional
        Number of observations that will be sampled from the `explainer.data` attribute
        before the calculation of aspect importance (default is `2000`).
    B : int, optional
        Parameter specific for `type == &#39;shap&#39;`. Number of random paths to calculate aspect
        attributions (default is `25`).
        NOTE: Ignored if `type` is not `&#39;shap&#39;`.
    sample_method : {&#39;default&#39;, &#39;binom&#39;}, optional
        Parameter specific for `type == &#39;default&#39;`. Sampling method for creating binary matrix
        used as mask for replacing aspects in data (default is `&#39;default&#39;`, which means
        it randomly replaces one or two zeros per row; `&#39;binom&#39;` replaces random number of zeros
        per row).
        NOTE: Ignored if `type` is not `&#39;default&#39;`.
    f : int, optional
        Parameter specific for `type == &#39;default&#39;` and `sample_method == &#39;binom&#39;`. Parameter
        controlling average number of replaced zeros for binomial sampling (default is `2`).
        NOTE: Ignored if `type` is not `&#39;default&#39;` or `sample_method` is not `&#39;binom&#39;`.
    processes : int, optional
        Number of parallel processes to use in calculations. Iterated over `B`
        (default is `1`, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).

    Returns
    -------
    PredictTriplot class object
        Explanation object containing the main result attribute and the plot method.
    &#34;&#34;&#34;

    pt = PredictTriplot(type, N, B, sample_method, f, processes, random_state)

    pt.fit(self, new_observation)

    return pt</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dalex.aspect.ModelAspectImportance"><code class="flex name class">
<span>class <span class="ident">ModelAspectImportance</span></span>
<span>(</span><span>variable_groups, loss_function=None, type='variable_importance', N=1000, B=10, depend_method='assoc', corr_method='spearman', agg_method='max', processes=1, random_state=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate model-level aspect importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>variable_groups</code></strong> :&ensp;<code>dict</code> of <code>lists </code></dt>
<dd>Variables grouped in aspects to calculate their importance.</dd>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>{'rmse', '1-auc', 'mse', 'mae', 'mad'}</code> or <code>function</code>, optional</dt>
<dd>If string, then such loss function will be used to assess aspect importance
(default is <code>'rmse'</code> or <code>'1-auc'</code>, depends on <code>explainer.model_type</code> attribute).</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'variable_importance', 'ratio', 'difference'}</code>, optional</dt>
<dd>Type of transformation that will be applied to dropout loss
(default is <code>'variable_importance'</code>, which is Permutational Variable Importance).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>explainer.data</code> attribute before
the calculation of aspect importance. <code>None</code> means all <code>data</code> (default is <code>1000</code>).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of permutation rounds to perform on each variable (default is <code>10</code>).</dd>
<dt><strong><code>depend_method</code></strong> :&ensp;<code>{'assoc', 'pps'}</code> or <code>function</code>, optional</dt>
<dd>The method of calculating the dependencies between variables (i.e. the dependency
matrix). Default is <code>'assoc'</code>, which means the use of statistical association
(correlation coefficient, Cram√©r's V and eta-quared);
<code>'pps'</code> stands for Power Predictive Score.
NOTE: When a function is passed, it is called with the <code>data</code> and it
must return a symmetric dependency matrix (<code>pd.DataFrame</code> with variable names as
columns and rows).</dd>
<dt><strong><code>corr_method</code></strong> :&ensp;<code>{'spearman', 'pearson', 'kendall'}</code>, optional</dt>
<dd>The method of calculating correlation between numerical variables
(default is <code>'spearman'</code>).
NOTE: Ignored if <code>depend_method</code> is not <code>'assoc'</code>.</dd>
<dt><strong><code>agg_method</code></strong> :&ensp;<code>{'max', 'min', 'avg'}</code>, optional</dt>
<dd>The method of aggregating the PPS values for pairs of variables
(default is <code>'max'</code>).
NOTE: Ignored if <code>depend_method</code> is not <code>'pps'</code>.</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>
(default is <code>1</code>, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>result</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Main result attribute of an explanation.</dd>
<dt><strong><code>variable_groups</code></strong> :&ensp;<code>dict</code> of <code>lists </code></dt>
<dd>Variables grouped in aspects to calculate their importance.</dd>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>function</code></dt>
<dd>Loss function used to assess the variable importance.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'variable_importance', 'ratio', 'difference'}</code></dt>
<dd>Type of transformation that will be applied to dropout loss.</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of observations that will be sampled from the <code>explainer.data</code> attribute before
the calculation of aspect importance.</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of permutation rounds to perform on each variable.</dd>
<dt><strong><code>depend_method</code></strong> :&ensp;<code>{'assoc', 'pps'}</code></dt>
<dd>The method of calculating the dependencies between variables.</dd>
<dt><strong><code>corr_method</code></strong> :&ensp;<code>{'spearman', 'pearson', 'kendall'}</code></dt>
<dd>The method of calculating correlation between numerical variables.</dd>
<dt><strong><code>agg_method</code></strong> :&ensp;<code>{'max', 'min', 'avg'}</code></dt>
<dd>The method of aggregating the PPS values for pairs of variables.</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code></dt>
<dd>Set seed for random number generator.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_model_aspect_importance/object.py#L12-L537" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ModelAspectImportance(VariableImportance):
    &#34;&#34;&#34;Calculate model-level aspect importance

    Parameters
    ----------
    variable_groups : dict of lists 
        Variables grouped in aspects to calculate their importance.
    loss_function :  {&#39;rmse&#39;, &#39;1-auc&#39;, &#39;mse&#39;, &#39;mae&#39;, &#39;mad&#39;} or function, optional
        If string, then such loss function will be used to assess aspect importance
        (default is `&#39;rmse&#39;` or `&#39;1-auc&#39;`, depends on `explainer.model_type` attribute).
    type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;}, optional
        Type of transformation that will be applied to dropout loss
        (default is `&#39;variable_importance&#39;`, which is Permutational Variable Importance).
    N : int, optional
        Number of observations that will be sampled from the `explainer.data` attribute before
        the calculation of aspect importance. `None` means all `data` (default is `1000`).
    B : int, optional
        Number of permutation rounds to perform on each variable (default is `10`).
    depend_method: {&#39;assoc&#39;, &#39;pps&#39;} or function, optional
        The method of calculating the dependencies between variables (i.e. the dependency 
        matrix). Default is `&#39;assoc&#39;`, which means the use of statistical association 
        (correlation coefficient, Cram√©r&#39;s V and eta-quared); 
        `&#39;pps&#39;` stands for Power Predictive Score.
        NOTE: When a function is passed, it is called with the `data` and it 
        must return a symmetric dependency matrix (`pd.DataFrame` with variable names as 
        columns and rows).
    corr_method : {&#39;spearman&#39;, &#39;pearson&#39;, &#39;kendall&#39;}, optional
        The method of calculating correlation between numerical variables 
        (default is `&#39;spearman&#39;`).
        NOTE: Ignored if `depend_method` is not `&#39;assoc&#39;`.
    agg_method : {&#39;max&#39;, &#39;min&#39;, &#39;avg&#39;}, optional
        The method of aggregating the PPS values for pairs of variables 
        (default is `&#39;max&#39;`).
        NOTE: Ignored if `depend_method` is not `&#39;pps&#39;`. 
    processes : int, optional
        Number of parallel processes to use in calculations. Iterated over `B`
        (default is `1`, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).

    Attributes
    -----------
    result : pd.DataFrame
        Main result attribute of an explanation.
    variable_groups : dict of lists 
        Variables grouped in aspects to calculate their importance. 
    loss_function : function
        Loss function used to assess the variable importance.
    type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;}
        Type of transformation that will be applied to dropout loss.
    N : int
        Number of observations that will be sampled from the `explainer.data` attribute before
        the calculation of aspect importance. 
    B : int
        Number of permutation rounds to perform on each variable.
    depend_method : {&#39;assoc&#39;, &#39;pps&#39;}
        The method of calculating the dependencies between variables.
    corr_method : {&#39;spearman&#39;, &#39;pearson&#39;, &#39;kendall&#39;}
        The method of calculating correlation between numerical variables.
    agg_method : {&#39;max&#39;, &#39;min&#39;, &#39;avg&#39;}
        The method of aggregating the PPS values for pairs of variables.
    processes : int
        Number of parallel processes to use in calculations. Iterated over `B`.
    random_state : int
        Set seed for random number generator.
    &#34;&#34;&#34;
    def __init__(
        self,
        variable_groups,
        loss_function=None,
        type=&#34;variable_importance&#34;,
        N=1000,
        B=10,
        depend_method=&#34;assoc&#34;,
        corr_method=&#34;spearman&#34;,
        agg_method=&#34;max&#34;,
        processes=1,
        random_state=None,
        **kwargs
    ):
        super().__init__(
            loss_function,
            type,
            N,
            B,
            None,
            variable_groups,
            True,
            processes,
            random_state,
        )
        _depend_method, _corr_method, _agg_method = checks.check_method_depend(depend_method, corr_method, agg_method)
        self.result = pd.DataFrame()
        self._depend_matrix = None
        if &#34;_depend_matrix&#34; in kwargs:
            self._depend_matrix = kwargs.get(&#34;_depend_matrix&#34;)
        self.depend_method = _depend_method
        self.corr_method = _corr_method
        self.agg_method = _agg_method
        self.loss_function = loss_function

    def _repr_html_(self):
        return self.result._repr_html_()

    def fit(self, explainer):
        &#34;&#34;&#34;Calculate the result of explanation
        Fit method makes calculations in place and changes the attributes.

        Parameters
        ----------
        explainer : Explainer object
            Model wrapper created using the Explainer class.
        
        Returns
        -----------
        None
        &#34;&#34;&#34;
        _loss_function = checks.check_method_loss_function(explainer, self.loss_function)
        self.loss_function = checks.check_loss_function(_loss_function)

        super().fit(explainer)

        self.result[&#34;variable_names&#34;] = self.result[&#34;variable&#34;].map(self.variable_groups)
        baseline = self.result[self.result[&#34;variable&#34;] == &#34;_full_model_&#34;][&#34;dropout_loss&#34;].values[0]
        self.result = self.result.assign(
            dropout_loss_change=lambda x: x[&#34;dropout_loss&#34;] - baseline
        )
        self.result = self.result.rename(columns={&#34;variable&#34;: &#34;aspect_name&#34;})
        self.result.insert(4, &#34;min_depend&#34;, None)
        self.result.insert(5, &#34;vars_min_depend&#34;, None)
        # if there is _depend_matrix in kwargs (called from Aspect object) 
        if self._depend_matrix is not None:
            vars_min_depend, min_depend = get_min_depend_from_matrix(self._depend_matrix, 
                    self.result.variable_names
                )
        else:
            vars_min_depend, min_depend = calculate_min_depend(
                self.result.variable_names, 
                explainer.data,
                self.depend_method,
                self.corr_method,
                self.agg_method,
            )

        self.result[&#34;min_depend&#34;] = min_depend
        self.result[&#34;vars_min_depend&#34;] = vars_min_depend

        self.result = self.result[
            [
                &#34;aspect_name&#34;,
                &#34;variable_names&#34;,
                &#34;dropout_loss&#34;,
                &#34;dropout_loss_change&#34;,
                &#34;min_depend&#34;,
                &#34;vars_min_depend&#34;,
                &#34;label&#34;,
            ]
        ]
      

    def plot(
        self,
        objects=None,
        max_aspects=10,
        show_variable_names=True,
        digits=3,
        rounding_function=np.around,
        bar_width=25,
        split=(&#34;model&#34;, &#34;aspect&#34;),
        title=&#34;Model Aspect Importance&#34;,
        vertical_spacing=None,
        show=True,
    ):
        &#34;&#34;&#34;Plot the Model Aspect Importance explanation.

        Parameters
        -----------
        objects : ModelAspectImportance object or array_like of ModelAspectImportance objects
            Additional objects to plot in subplots (default is `None`).
        max_aspects : int, optional
            Maximum number of aspects that will be presented for for each subplot
            (default is `10`).
        show_variable_names : bool, optional
            `True` shows names of variables grouped in aspects; `False` shows names of aspects
            (default is `True`).
        digits : int, optional
            Number of decimal places (`np.around`) to round contributions.
            See `rounding_function` parameter (default is `3`).
        rounding_function : function, optional
            A function that will be used for rounding numbers (default is `np.around`).
        bar_width : float, optional
            Width of bars in px (default is `25`).
        split : {&#39;model&#39;, &#39;aspect&#39;}, optional
            Split the subplots by model or aspect (default is `&#39;model&#39;`).
        title : str, optional
            Title of the plot (default is `&#34;Model Aspect Importance&#34;`).
        vertical_spacing : float &lt;0, 1&gt;, optional
            Ratio of vertical space between the plots (default is `0.2/number of rows`).
        show : bool, optional
            `True` shows the plot; `False` returns the plotly Figure object that can
            be edited or saved using the `write_image()` method (default is `True`).

        Returns
        -----------
        None or plotly.graph_objects.Figure
            Return figure that can be edited or saved. See `show` parameter.
        &#34;&#34;&#34;

        if isinstance(split, tuple):
            split = split[0]

        if split not in (&#34;model&#34;, &#34;aspect&#34;):
            raise TypeError(&#34;split should be &#39;model&#39; or &#39;aspect&#39;&#34;)

        # are there any other objects to plot?
        if objects is None:
            n = 1
            _result_df = self.result.copy()
            if split == &#34;aspect&#34;:  # force split by model if only one explainer
                split = &#34;model&#34;
        elif isinstance(
            objects, self.__class__
        ):  # allow for objects to be a single element
            n = 2
            _result_df = pd.concat([self.result.copy(), objects.result.copy()])
        elif isinstance(objects, (list, tuple)):  # objects as tuple or array
            n = len(objects) + 1
            _result_df = self.result.copy()
            for ob in objects:
                _global_checks.global_check_object_class(ob, self.__class__)
                _result_df = pd.concat([_result_df, ob.result.copy()])
        else:
            _global_checks.global_raise_objects_class(objects, self.__class__)

        dl = _result_df.loc[
            _result_df.aspect_name != &#34;_baseline_&#34;, &#34;dropout_loss&#34;
        ].to_numpy()
        min_max_margin = dl.ptp() * 0.15
        min_max = [dl.min() - min_max_margin, dl.max() + min_max_margin]

        # take out full model
        best_fits = _result_df[_result_df.aspect_name == &#34;_full_model_&#34;]

        # this produces dropout_loss_x and dropout_loss_y columns
        _result_df = _result_df.merge(
            best_fits[[&#34;label&#34;, &#34;dropout_loss&#34;]], how=&#34;left&#34;, on=&#34;label&#34;
        )
        # remove full_model and baseline
        _result_df = _result_df[
            (_result_df.aspect_name != &#34;_full_model_&#34;)
            &amp; (_result_df.aspect_name != &#34;_baseline_&#34;)
        ]
        _result_df = _result_df[
            [
                &#34;label&#34;,
                &#34;aspect_name&#34;,
                &#34;dropout_loss_x&#34;,
                &#34;dropout_loss_y&#34;,
                &#34;variable_names&#34;,
                &#34;min_depend&#34;,
                &#34;vars_min_depend&#34;,
            ]
        ].rename(
            columns={
                &#34;dropout_loss_x&#34;: &#34;dropout_loss&#34;,
                &#34;dropout_loss_y&#34;: &#34;full_model&#34;,
            }
        )
        # calculate order of bars or variable plots (split = &#39;aspect&#39;)
        # get variable permutation
        perm = (
            _result_df[[&#34;aspect_name&#34;, &#34;dropout_loss&#34;]]
            .groupby(&#34;aspect_name&#34;)
            .mean()
            .reset_index()
            .sort_values(&#34;dropout_loss&#34;, ascending=False)
            .aspect_name.values
        )
        model_names = _result_df[&#34;label&#34;].unique().tolist()

        if len(model_names) != n:
            raise ValueError(&#34;label must be unique for each model&#34;)

        plot_height = 78 + 71

        colors = _theme.get_default_colors(n, &#34;bar&#34;)

        if vertical_spacing is None:
            vertical_spacing = 0.2 / n

        # init plot
        fig = make_subplots(
            rows=n,
            cols=1,
            shared_xaxes=True,
            vertical_spacing=vertical_spacing,
            x_title=&#34;drop-out loss&#34;,
            subplot_titles=model_names,
        )
        if split == &#34;model&#34;:
            # split df by model
            df_list = [v for k, v in _result_df.groupby(&#34;label&#34;, sort=False)]

            for i, df in enumerate(df_list):
                m = df.shape[0]
                if max_aspects is not None and max_aspects &lt; m:
                    m = max_aspects

                # take only m variables (for max_aspects)
                # sort rows of df by variable permutation and drop unused variables
                df = (
                    df.sort_values(&#34;dropout_loss&#34;)
                    .tail(m)
                    .set_index(&#34;aspect_name&#34;)
                    .reindex(perm)
                    .dropna()
                    .reset_index()
                )

                baseline = df.iloc[0, df.columns.get_loc(&#34;full_model&#34;)]

                df = df.assign(difference=lambda x: x[&#34;dropout_loss&#34;] - baseline)

                lt = df.difference.apply(
                    lambda val: &#34;+&#34; + str(rounding_function(np.abs(val), digits))
                    if val &gt; 0
                    else str(rounding_function(np.abs(val), digits))
                )
                tt = df.apply(
                    lambda row: plot.tooltip_text(
                        row, rounding_function, digits
                    ),
                    axis=1,
                )
               
                df = df.assign(label_text=lt, tooltip_text=tt)

                fig.add_shape(
                    type=&#34;line&#34;,
                    x0=baseline,
                    x1=baseline,
                    y0=-1,
                    y1=m,
                    yref=&#34;paper&#34;,
                    xref=&#34;x&#34;,
                    line={&#34;color&#34;: &#34;#371ea3&#34;, &#34;width&#34;: 1.5, &#34;dash&#34;: &#34;dot&#34;},
                    row=i + 1,
                    col=1,
                )

                if show_variable_names:
                    y_axis_ticks = [
                        &#34;, &#34;.join(variables_list)
                        for variables_list in df[&#34;variable_names&#34;]
                    ]
                else:
                    y_axis_ticks = df[&#34;aspect_name&#34;]

                fig.add_bar(
                    orientation=&#34;h&#34;,
                    y=y_axis_ticks,
                    x=df[&#34;difference&#34;].tolist(),
                    textposition=&#34;outside&#34;,
                    text=df[&#34;label_text&#34;].tolist(),
                    marker_color=colors[i],
                    base=baseline,
                    hovertext=df[&#34;tooltip_text&#34;].tolist(),
                    hoverinfo=&#34;text&#34;,
                    hoverlabel={&#34;bgcolor&#34;: &#34;rgba(0,0,0,0.8)&#34;},
                    showlegend=False,
                    row=i + 1,
                    col=1,
                )

                fig.update_yaxes(
                    {
                        &#34;type&#34;: &#34;category&#34;,
                        &#34;autorange&#34;: &#34;reversed&#34;,
                        &#34;gridwidth&#34;: 2,
                        &#34;automargin&#34;: True,
                        &#34;ticks&#34;: &#34;outside&#34;,
                        &#34;tickcolor&#34;: &#34;white&#34;,
                        &#34;ticklen&#34;: 10,
                        &#34;fixedrange&#34;: True,
                    },
                    row=i + 1,
                    col=1,
                )

                fig.update_xaxes(
                    {
                        &#34;type&#34;: &#34;linear&#34;,
                        &#34;gridwidth&#34;: 2,
                        &#34;zeroline&#34;: False,
                        &#34;automargin&#34;: True,
                        &#34;ticks&#34;: &#34;outside&#34;,
                        &#34;tickcolor&#34;: &#34;white&#34;,
                        &#34;ticklen&#34;: 3,
                        &#34;fixedrange&#34;: True,
                    },
                    row=i + 1,
                    col=1,
                )

                plot_height += m * bar_width + (m + 1) * bar_width / 4 + 30
        elif split == &#34;aspect&#34;:
            # split df by aspect
            df_list = [v for k, v in _result_df.groupby(&#34;aspect_name&#34;, sort=False)]

            n = len(df_list)
            if max_aspects is not None and max_aspects &lt; n:
                n = max_aspects

            if vertical_spacing is None:
                vertical_spacing = 0.2 / n
            # init plot
            variable_names = perm[0:n]
            fig = make_subplots(
                rows=n,
                cols=1,
                shared_xaxes=True,
                vertical_spacing=vertical_spacing,
                x_title=&#34;drop-out loss&#34;,
                subplot_titles=variable_names,
            )

            df_dict = {e.aspect_name.array[0]: e for e in df_list}

            # take only n=max_aspects elements from df_dict
            for i in range(n):
                df = df_dict[perm[i]]
                m = df.shape[0]

                baseline = 0

                df = df.assign(difference=lambda x: x[&#34;dropout_loss&#34;] - x[&#34;full_model&#34;])

                lt = df.difference.apply(
                    lambda val: &#34;+&#34; + str(rounding_function(np.abs(val), digits))
                    if val &gt; 0
                    else str(rounding_function(np.abs(val), digits))
                )
                tt = df.apply(
                    lambda row: plot.tooltip_text(row, rounding_function, digits),
                    axis=1,
                )
                df = df.assign(label_text=lt, tooltip_text=tt)

                fig.add_shape(
                    type=&#34;line&#34;,
                    x0=baseline,
                    x1=baseline,
                    y0=-1,
                    y1=m,
                    yref=&#34;paper&#34;,
                    xref=&#34;x&#34;,
                    line={&#34;color&#34;: &#34;#371ea3&#34;, &#34;width&#34;: 1.5, &#34;dash&#34;: &#34;dot&#34;},
                    row=i + 1,
                    col=1,
                )

                fig.add_bar(
                    orientation=&#34;h&#34;,
                    y=df[&#34;label&#34;].tolist(),
                    x=df[&#34;dropout_loss&#34;].tolist(),
                    textposition=&#34;outside&#34;,
                    text=df[&#34;label_text&#34;].tolist(),
                    marker_color=colors,
                    base=baseline,
                    hovertext=df[&#34;tooltip_text&#34;].tolist(),
                    hoverinfo=&#34;text&#34;,
                    hoverlabel={&#34;bgcolor&#34;: &#34;rgba(0,0,0,0.8)&#34;},
                    showlegend=False,
                    row=i + 1,
                    col=1,
                )

                fig.update_yaxes(
                    {
                        &#34;type&#34;: &#34;category&#34;,
                        &#34;autorange&#34;: &#34;reversed&#34;,
                        &#34;gridwidth&#34;: 2,
                        &#34;automargin&#34;: True,
                        &#34;ticks&#34;: &#34;outside&#34;,
                        &#34;tickcolor&#34;: &#34;white&#34;,
                        &#34;ticklen&#34;: 10,
                        &#34;dtick&#34;: 1,
                        &#34;fixedrange&#34;: True,
                    },
                    row=i + 1,
                    col=1,
                )

                fig.update_xaxes(
                    {
                        &#34;type&#34;: &#34;linear&#34;,
                        &#34;gridwidth&#34;: 2,
                        &#34;zeroline&#34;: False,
                        &#34;automargin&#34;: True,
                        &#34;ticks&#34;: &#34;outside&#34;,
                        &#34;tickcolor&#34;: &#34;white&#34;,
                        &#34;ticklen&#34;: 3,
                        &#34;fixedrange&#34;: True,
                    },
                    row=i + 1,
                    col=1,
                )

                plot_height += m * bar_width + (m + 1) * bar_width / 4

        plot_height += (n - 1) * 70

        fig.update_xaxes({&#34;range&#34;: min_max})
        fig.update_layout(
            title_text=title,
            title_x=0.15,
            font={&#34;color&#34;: &#34;#371ea3&#34;},
            template=&#34;none&#34;,
            height=plot_height,
            margin={&#34;t&#34;: 78, &#34;b&#34;: 71, &#34;r&#34;: 30},
        )

        if show:
            fig.show(config=_theme.get_default_config())
        else:
            return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>dalex.model_explanations._variable_importance.object.VariableImportance</li>
<li>dalex._explanation.Explanation</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dalex.aspect.ModelAspectImportance.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, explainer)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the result of explanation
Fit method makes calculations in place and changes the attributes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>explainer</code></strong> :&ensp;<code>Explainer <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Model wrapper created using the Explainer class.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_model_aspect_importance/object.py#L116-L169" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def fit(self, explainer):
    &#34;&#34;&#34;Calculate the result of explanation
    Fit method makes calculations in place and changes the attributes.

    Parameters
    ----------
    explainer : Explainer object
        Model wrapper created using the Explainer class.
    
    Returns
    -----------
    None
    &#34;&#34;&#34;
    _loss_function = checks.check_method_loss_function(explainer, self.loss_function)
    self.loss_function = checks.check_loss_function(_loss_function)

    super().fit(explainer)

    self.result[&#34;variable_names&#34;] = self.result[&#34;variable&#34;].map(self.variable_groups)
    baseline = self.result[self.result[&#34;variable&#34;] == &#34;_full_model_&#34;][&#34;dropout_loss&#34;].values[0]
    self.result = self.result.assign(
        dropout_loss_change=lambda x: x[&#34;dropout_loss&#34;] - baseline
    )
    self.result = self.result.rename(columns={&#34;variable&#34;: &#34;aspect_name&#34;})
    self.result.insert(4, &#34;min_depend&#34;, None)
    self.result.insert(5, &#34;vars_min_depend&#34;, None)
    # if there is _depend_matrix in kwargs (called from Aspect object) 
    if self._depend_matrix is not None:
        vars_min_depend, min_depend = get_min_depend_from_matrix(self._depend_matrix, 
                self.result.variable_names
            )
    else:
        vars_min_depend, min_depend = calculate_min_depend(
            self.result.variable_names, 
            explainer.data,
            self.depend_method,
            self.corr_method,
            self.agg_method,
        )

    self.result[&#34;min_depend&#34;] = min_depend
    self.result[&#34;vars_min_depend&#34;] = vars_min_depend

    self.result = self.result[
        [
            &#34;aspect_name&#34;,
            &#34;variable_names&#34;,
            &#34;dropout_loss&#34;,
            &#34;dropout_loss_change&#34;,
            &#34;min_depend&#34;,
            &#34;vars_min_depend&#34;,
            &#34;label&#34;,
        ]
    ]</code></pre>
</details>
</dd>
<dt id="dalex.aspect.ModelAspectImportance.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, objects=None, max_aspects=10, show_variable_names=True, digits=3, rounding_function=&lt;function around&gt;, bar_width=25, split=('model', 'aspect'), title='Model Aspect Importance', vertical_spacing=None, show=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the Model Aspect Importance explanation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>objects</code></strong> :&ensp;<code><a title="dalex.aspect.ModelAspectImportance" href="#dalex.aspect.ModelAspectImportance">ModelAspectImportance</a> <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code> or <code>array_like</code> of <code><a title="dalex.aspect.ModelAspectImportance" href="#dalex.aspect.ModelAspectImportance">ModelAspectImportance</a> objects</code></dt>
<dd>Additional objects to plot in subplots (default is <code>None</code>).</dd>
<dt><strong><code>max_aspects</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of aspects that will be presented for for each subplot
(default is <code>10</code>).</dd>
<dt><strong><code>show_variable_names</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd><code>True</code> shows names of variables grouped in aspects; <code>False</code> shows names of aspects
(default is <code>True</code>).</dd>
<dt><strong><code>digits</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of decimal places (<code>np.around</code>) to round contributions.
See <code>rounding_function</code> parameter (default is <code>3</code>).</dd>
<dt><strong><code>rounding_function</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>A function that will be used for rounding numbers (default is <code>np.around</code>).</dd>
<dt><strong><code>bar_width</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Width of bars in px (default is <code>25</code>).</dd>
<dt><strong><code>split</code></strong> :&ensp;<code>{'model', 'aspect'}</code>, optional</dt>
<dd>Split the subplots by model or aspect (default is <code>'model'</code>).</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Title of the plot (default is <code>"Model Aspect Importance"</code>).</dd>
<dt><strong><code>vertical_spacing</code></strong> :&ensp;<code>float &lt;0, 1&gt;</code>, optional</dt>
<dd>Ratio of vertical space between the plots (default is <code>0.2/number of rows</code>).</dd>
<dt><strong><code>show</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd><code>True</code> shows the plot; <code>False</code> returns the plotly Figure object that can
be edited or saved using the <code>write_image()</code> method (default is <code>True</code>).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code> or <code>plotly.graph_objects.Figure</code></dt>
<dd>Return figure that can be edited or saved. See <code>show</code> parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_model_aspect_importance/object.py#L172-L537" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot(
    self,
    objects=None,
    max_aspects=10,
    show_variable_names=True,
    digits=3,
    rounding_function=np.around,
    bar_width=25,
    split=(&#34;model&#34;, &#34;aspect&#34;),
    title=&#34;Model Aspect Importance&#34;,
    vertical_spacing=None,
    show=True,
):
    &#34;&#34;&#34;Plot the Model Aspect Importance explanation.

    Parameters
    -----------
    objects : ModelAspectImportance object or array_like of ModelAspectImportance objects
        Additional objects to plot in subplots (default is `None`).
    max_aspects : int, optional
        Maximum number of aspects that will be presented for for each subplot
        (default is `10`).
    show_variable_names : bool, optional
        `True` shows names of variables grouped in aspects; `False` shows names of aspects
        (default is `True`).
    digits : int, optional
        Number of decimal places (`np.around`) to round contributions.
        See `rounding_function` parameter (default is `3`).
    rounding_function : function, optional
        A function that will be used for rounding numbers (default is `np.around`).
    bar_width : float, optional
        Width of bars in px (default is `25`).
    split : {&#39;model&#39;, &#39;aspect&#39;}, optional
        Split the subplots by model or aspect (default is `&#39;model&#39;`).
    title : str, optional
        Title of the plot (default is `&#34;Model Aspect Importance&#34;`).
    vertical_spacing : float &lt;0, 1&gt;, optional
        Ratio of vertical space between the plots (default is `0.2/number of rows`).
    show : bool, optional
        `True` shows the plot; `False` returns the plotly Figure object that can
        be edited or saved using the `write_image()` method (default is `True`).

    Returns
    -----------
    None or plotly.graph_objects.Figure
        Return figure that can be edited or saved. See `show` parameter.
    &#34;&#34;&#34;

    if isinstance(split, tuple):
        split = split[0]

    if split not in (&#34;model&#34;, &#34;aspect&#34;):
        raise TypeError(&#34;split should be &#39;model&#39; or &#39;aspect&#39;&#34;)

    # are there any other objects to plot?
    if objects is None:
        n = 1
        _result_df = self.result.copy()
        if split == &#34;aspect&#34;:  # force split by model if only one explainer
            split = &#34;model&#34;
    elif isinstance(
        objects, self.__class__
    ):  # allow for objects to be a single element
        n = 2
        _result_df = pd.concat([self.result.copy(), objects.result.copy()])
    elif isinstance(objects, (list, tuple)):  # objects as tuple or array
        n = len(objects) + 1
        _result_df = self.result.copy()
        for ob in objects:
            _global_checks.global_check_object_class(ob, self.__class__)
            _result_df = pd.concat([_result_df, ob.result.copy()])
    else:
        _global_checks.global_raise_objects_class(objects, self.__class__)

    dl = _result_df.loc[
        _result_df.aspect_name != &#34;_baseline_&#34;, &#34;dropout_loss&#34;
    ].to_numpy()
    min_max_margin = dl.ptp() * 0.15
    min_max = [dl.min() - min_max_margin, dl.max() + min_max_margin]

    # take out full model
    best_fits = _result_df[_result_df.aspect_name == &#34;_full_model_&#34;]

    # this produces dropout_loss_x and dropout_loss_y columns
    _result_df = _result_df.merge(
        best_fits[[&#34;label&#34;, &#34;dropout_loss&#34;]], how=&#34;left&#34;, on=&#34;label&#34;
    )
    # remove full_model and baseline
    _result_df = _result_df[
        (_result_df.aspect_name != &#34;_full_model_&#34;)
        &amp; (_result_df.aspect_name != &#34;_baseline_&#34;)
    ]
    _result_df = _result_df[
        [
            &#34;label&#34;,
            &#34;aspect_name&#34;,
            &#34;dropout_loss_x&#34;,
            &#34;dropout_loss_y&#34;,
            &#34;variable_names&#34;,
            &#34;min_depend&#34;,
            &#34;vars_min_depend&#34;,
        ]
    ].rename(
        columns={
            &#34;dropout_loss_x&#34;: &#34;dropout_loss&#34;,
            &#34;dropout_loss_y&#34;: &#34;full_model&#34;,
        }
    )
    # calculate order of bars or variable plots (split = &#39;aspect&#39;)
    # get variable permutation
    perm = (
        _result_df[[&#34;aspect_name&#34;, &#34;dropout_loss&#34;]]
        .groupby(&#34;aspect_name&#34;)
        .mean()
        .reset_index()
        .sort_values(&#34;dropout_loss&#34;, ascending=False)
        .aspect_name.values
    )
    model_names = _result_df[&#34;label&#34;].unique().tolist()

    if len(model_names) != n:
        raise ValueError(&#34;label must be unique for each model&#34;)

    plot_height = 78 + 71

    colors = _theme.get_default_colors(n, &#34;bar&#34;)

    if vertical_spacing is None:
        vertical_spacing = 0.2 / n

    # init plot
    fig = make_subplots(
        rows=n,
        cols=1,
        shared_xaxes=True,
        vertical_spacing=vertical_spacing,
        x_title=&#34;drop-out loss&#34;,
        subplot_titles=model_names,
    )
    if split == &#34;model&#34;:
        # split df by model
        df_list = [v for k, v in _result_df.groupby(&#34;label&#34;, sort=False)]

        for i, df in enumerate(df_list):
            m = df.shape[0]
            if max_aspects is not None and max_aspects &lt; m:
                m = max_aspects

            # take only m variables (for max_aspects)
            # sort rows of df by variable permutation and drop unused variables
            df = (
                df.sort_values(&#34;dropout_loss&#34;)
                .tail(m)
                .set_index(&#34;aspect_name&#34;)
                .reindex(perm)
                .dropna()
                .reset_index()
            )

            baseline = df.iloc[0, df.columns.get_loc(&#34;full_model&#34;)]

            df = df.assign(difference=lambda x: x[&#34;dropout_loss&#34;] - baseline)

            lt = df.difference.apply(
                lambda val: &#34;+&#34; + str(rounding_function(np.abs(val), digits))
                if val &gt; 0
                else str(rounding_function(np.abs(val), digits))
            )
            tt = df.apply(
                lambda row: plot.tooltip_text(
                    row, rounding_function, digits
                ),
                axis=1,
            )
           
            df = df.assign(label_text=lt, tooltip_text=tt)

            fig.add_shape(
                type=&#34;line&#34;,
                x0=baseline,
                x1=baseline,
                y0=-1,
                y1=m,
                yref=&#34;paper&#34;,
                xref=&#34;x&#34;,
                line={&#34;color&#34;: &#34;#371ea3&#34;, &#34;width&#34;: 1.5, &#34;dash&#34;: &#34;dot&#34;},
                row=i + 1,
                col=1,
            )

            if show_variable_names:
                y_axis_ticks = [
                    &#34;, &#34;.join(variables_list)
                    for variables_list in df[&#34;variable_names&#34;]
                ]
            else:
                y_axis_ticks = df[&#34;aspect_name&#34;]

            fig.add_bar(
                orientation=&#34;h&#34;,
                y=y_axis_ticks,
                x=df[&#34;difference&#34;].tolist(),
                textposition=&#34;outside&#34;,
                text=df[&#34;label_text&#34;].tolist(),
                marker_color=colors[i],
                base=baseline,
                hovertext=df[&#34;tooltip_text&#34;].tolist(),
                hoverinfo=&#34;text&#34;,
                hoverlabel={&#34;bgcolor&#34;: &#34;rgba(0,0,0,0.8)&#34;},
                showlegend=False,
                row=i + 1,
                col=1,
            )

            fig.update_yaxes(
                {
                    &#34;type&#34;: &#34;category&#34;,
                    &#34;autorange&#34;: &#34;reversed&#34;,
                    &#34;gridwidth&#34;: 2,
                    &#34;automargin&#34;: True,
                    &#34;ticks&#34;: &#34;outside&#34;,
                    &#34;tickcolor&#34;: &#34;white&#34;,
                    &#34;ticklen&#34;: 10,
                    &#34;fixedrange&#34;: True,
                },
                row=i + 1,
                col=1,
            )

            fig.update_xaxes(
                {
                    &#34;type&#34;: &#34;linear&#34;,
                    &#34;gridwidth&#34;: 2,
                    &#34;zeroline&#34;: False,
                    &#34;automargin&#34;: True,
                    &#34;ticks&#34;: &#34;outside&#34;,
                    &#34;tickcolor&#34;: &#34;white&#34;,
                    &#34;ticklen&#34;: 3,
                    &#34;fixedrange&#34;: True,
                },
                row=i + 1,
                col=1,
            )

            plot_height += m * bar_width + (m + 1) * bar_width / 4 + 30
    elif split == &#34;aspect&#34;:
        # split df by aspect
        df_list = [v for k, v in _result_df.groupby(&#34;aspect_name&#34;, sort=False)]

        n = len(df_list)
        if max_aspects is not None and max_aspects &lt; n:
            n = max_aspects

        if vertical_spacing is None:
            vertical_spacing = 0.2 / n
        # init plot
        variable_names = perm[0:n]
        fig = make_subplots(
            rows=n,
            cols=1,
            shared_xaxes=True,
            vertical_spacing=vertical_spacing,
            x_title=&#34;drop-out loss&#34;,
            subplot_titles=variable_names,
        )

        df_dict = {e.aspect_name.array[0]: e for e in df_list}

        # take only n=max_aspects elements from df_dict
        for i in range(n):
            df = df_dict[perm[i]]
            m = df.shape[0]

            baseline = 0

            df = df.assign(difference=lambda x: x[&#34;dropout_loss&#34;] - x[&#34;full_model&#34;])

            lt = df.difference.apply(
                lambda val: &#34;+&#34; + str(rounding_function(np.abs(val), digits))
                if val &gt; 0
                else str(rounding_function(np.abs(val), digits))
            )
            tt = df.apply(
                lambda row: plot.tooltip_text(row, rounding_function, digits),
                axis=1,
            )
            df = df.assign(label_text=lt, tooltip_text=tt)

            fig.add_shape(
                type=&#34;line&#34;,
                x0=baseline,
                x1=baseline,
                y0=-1,
                y1=m,
                yref=&#34;paper&#34;,
                xref=&#34;x&#34;,
                line={&#34;color&#34;: &#34;#371ea3&#34;, &#34;width&#34;: 1.5, &#34;dash&#34;: &#34;dot&#34;},
                row=i + 1,
                col=1,
            )

            fig.add_bar(
                orientation=&#34;h&#34;,
                y=df[&#34;label&#34;].tolist(),
                x=df[&#34;dropout_loss&#34;].tolist(),
                textposition=&#34;outside&#34;,
                text=df[&#34;label_text&#34;].tolist(),
                marker_color=colors,
                base=baseline,
                hovertext=df[&#34;tooltip_text&#34;].tolist(),
                hoverinfo=&#34;text&#34;,
                hoverlabel={&#34;bgcolor&#34;: &#34;rgba(0,0,0,0.8)&#34;},
                showlegend=False,
                row=i + 1,
                col=1,
            )

            fig.update_yaxes(
                {
                    &#34;type&#34;: &#34;category&#34;,
                    &#34;autorange&#34;: &#34;reversed&#34;,
                    &#34;gridwidth&#34;: 2,
                    &#34;automargin&#34;: True,
                    &#34;ticks&#34;: &#34;outside&#34;,
                    &#34;tickcolor&#34;: &#34;white&#34;,
                    &#34;ticklen&#34;: 10,
                    &#34;dtick&#34;: 1,
                    &#34;fixedrange&#34;: True,
                },
                row=i + 1,
                col=1,
            )

            fig.update_xaxes(
                {
                    &#34;type&#34;: &#34;linear&#34;,
                    &#34;gridwidth&#34;: 2,
                    &#34;zeroline&#34;: False,
                    &#34;automargin&#34;: True,
                    &#34;ticks&#34;: &#34;outside&#34;,
                    &#34;tickcolor&#34;: &#34;white&#34;,
                    &#34;ticklen&#34;: 3,
                    &#34;fixedrange&#34;: True,
                },
                row=i + 1,
                col=1,
            )

            plot_height += m * bar_width + (m + 1) * bar_width / 4

    plot_height += (n - 1) * 70

    fig.update_xaxes({&#34;range&#34;: min_max})
    fig.update_layout(
        title_text=title,
        title_x=0.15,
        font={&#34;color&#34;: &#34;#371ea3&#34;},
        template=&#34;none&#34;,
        height=plot_height,
        margin={&#34;t&#34;: 78, &#34;b&#34;: 71, &#34;r&#34;: 30},
    )

    if show:
        fig.show(config=_theme.get_default_config())
    else:
        return fig</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dalex.aspect.ModelTriplot"><code class="flex name class">
<span>class <span class="ident">ModelTriplot</span></span>
<span>(</span><span>loss_function=None, type='variable_importance', N=1000, B=10, processes=1, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate model-level hierarchical aspect importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>{'rmse', '1-auc', 'mse', 'mae', 'mad'}</code> or <code>function</code>, optional</dt>
<dd>If string, then such loss function will be used to assess aspect importance
(default is <code>'rmse'</code> or <code>'1-auc'</code>, depends on <code>explainer.model_type</code> attribute).</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'variable_importance', 'ratio', 'difference'}</code>, optional</dt>
<dd>Type of transformation that will be applied to dropout loss
(default is <code>'variable_importance'</code>, which is Permutational Variable Importance).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>explainer.data</code> attribute before
the calculation of aspect importance. <code>None</code> means all <code>data</code> (default is <code>1000</code>).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of permutation rounds to perform on each variable (default is <code>10</code>).</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>
(default is <code>1</code>, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>result</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Main result attribute of an explanation.</dd>
<dt><strong><code>single_variable_importance</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Additional result attribute of an explanation (it contains information
about the importance of individual variables).</dd>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>function</code></dt>
<dd>Loss function used to assess the variable importance.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'variable_importance', 'ratio', 'difference'}</code></dt>
<dd>Type of transformation that will be applied to dropout loss.</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of observations that will be sampled from the <code>explainer.data</code> attribute before
the calculation of aspect importance. <code>None</code> means all <code>data</code> (default is <code>1000</code>).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of permutation rounds to perform on each variable (default is <code>10</code>).</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>
(default is <code>1</code>, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>Set seed for random number generator.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li><a href="https://arxiv.org/abs/2104.03403">https://arxiv.org/abs/2104.03403</a></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_model_triplot/object.py#L12-L441" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ModelTriplot(Explanation):
    &#34;&#34;&#34;Calculate model-level hierarchical aspect importance

    Parameters
    ----------
    loss_function :  {&#39;rmse&#39;, &#39;1-auc&#39;, &#39;mse&#39;, &#39;mae&#39;, &#39;mad&#39;} or function, optional
        If string, then such loss function will be used to assess aspect importance
        (default is `&#39;rmse&#39;` or `&#39;1-auc&#39;`, depends on `explainer.model_type` attribute).
    type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;}, optional
        Type of transformation that will be applied to dropout loss
        (default is `&#39;variable_importance&#39;`, which is Permutational Variable Importance).
    N : int, optional
        Number of observations that will be sampled from the `explainer.data` attribute before
        the calculation of aspect importance. `None` means all `data` (default is `1000`).
    B : int, optional
        Number of permutation rounds to perform on each variable (default is `10`).
    processes : int, optional
        Number of parallel processes to use in calculations. Iterated over `B`
        (default is `1`, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).

    Attributes
    -----------
    result : pd.DataFrame
        Main result attribute of an explanation.
    single_variable_importance : pd.DataFrame
        Additional result attribute of an explanation (it contains information 
        about the importance of individual variables).
    loss_function : function
        Loss function used to assess the variable importance.
    type : {&#39;variable_importance&#39;, &#39;ratio&#39;, &#39;difference&#39;}
        Type of transformation that will be applied to dropout loss.
    N : int
        Number of observations that will be sampled from the `explainer.data` attribute before
        the calculation of aspect importance. `None` means all `data` (default is `1000`).
    B : int
        Number of permutation rounds to perform on each variable (default is `10`).
    processes : int
        Number of parallel processes to use in calculations. Iterated over `B`
        (default is `1`, which means no parallel computation).
    random_state : int or None
        Set seed for random number generator.

    Notes
    -----
    - https://arxiv.org/abs/2104.03403
    &#34;&#34;&#34;
    def __init__(
        self,
        loss_function=None,
        type=&#34;variable_importance&#34;,
        N=1000,
        B=10,
        processes=1,
        random_state=None,
    ):
        _B = checks.check_B(B)
        _type = checks.check_type(type)
        _random_state = checks.check_random_state(random_state)
        _processes = checks.check_processes(processes)

        self.loss_function = loss_function
        self.type = _type
        self.N = N
        self.B = _B
        self.random_state = _random_state
        self.processes = _processes
        self.result = pd.DataFrame()
        self.single_variable_importance = None
        self._hierarchical_clustering_dendrogram = None

    def _repr_html_(self):
        return self.result._repr_html_()

    def fit(self, aspect):
        &#34;&#34;&#34;Calculate the result of explanation
        Fit method makes calculations in place and changes the attributes.

        Parameters
        ----------
        aspect : Aspect object
            Explainer wrapper created using the Aspect class.
        
        Returns
        -----------
        None
        &#34;&#34;&#34;
        _loss_function = checks.check_method_loss_function(aspect.explainer, self.loss_function)
        self.loss_function = checks.check_loss_function(_loss_function)

        self._hierarchical_clustering_dendrogram = aspect._hierarchical_clustering_dendrogram
        
        ## middle plot
        (
        self.result,
        aspect._full_hierarchical_aspect_importance
        ) = utils.calculate_model_hierarchical_importance(
            aspect,
            self.loss_function,
            self.type,
            self.N,
            self.B,
            self.processes,
            self.random_state
        )

        ## left plot data
        self.single_variable_importance, svi_full = utils.calculate_single_variable_importance(
            aspect,
            self.loss_function,
            self.type,
            self.N,
            self.B,
            self.processes,
            self.random_state,
        )
    
        aspect._full_hierarchical_aspect_importance = pd.concat(
            [aspect._full_hierarchical_aspect_importance, svi_full]
        )

    def plot(
        self,
        digits=3,
        rounding_function=np.around,
        show_change = True,
        bar_width=25,
        width=1500,
        vcolors=None,
        title=&#34;Model Triplot&#34;,
        widget=False,
        show=True
    ):
        &#34;&#34;&#34;Plot the Model Triplot explanation (triplot visualization).

        Parameters
        ----------
        digits : int, optional
            Number of decimal places (`np.around`) to round contributions.
            See `rounding_function` parameter (default is `3`).
        rounding_function : function, optional
            A function that will be used for rounding numbers (default is `np.around`).
        show_change : bool, optional
            If `True` middle panel shows dropout loss change, otherwise dropout loss
            (default is `True`). 
        bar_width : float, optional
            Width of bars in px (default is `16`).
        width : float, optional
            Width of triplot in px (default is `1500`).
        vcolors : str, optional
            Color of bars (default is `&#34;#46bac2&#34;`).
        title : str, optional
            Title of the plot (default is `&#34;Model Triplot&#34;`).
        widget : bool, optional
            If `True` triplot interactive widget version is generated
            (default is `False`).
        show : bool, optional
            `True` shows the plot; `False` returns the plotly Figure object 
            (default is `True`).
            NOTE: Ignored if `widget` is `True`.

        Returns
        -------
        None or plotly.graph_objects.Figure or ipywidgets.HBox with plotly.graph_objs._figurewidget.FigureWidget
            Return figure that can be edited or saved. See `show` parameter.
        &#34;&#34;&#34;
        _global_checks.global_check_import(&#39;kaleido&#39;, &#39;Model Triplot&#39;)
        ## right plot
        hierarchical_clustering_dendrogram_plot_without_annotations = (
            self._hierarchical_clustering_dendrogram
        )
        variables_order = list(
            hierarchical_clustering_dendrogram_plot_without_annotations.layout.yaxis.ticktext
        )
        m = len(variables_order)

        ## middle plot
        (
            hierarchical_importance_dendrogram_plot_without_annotations,
            updated_dendro_traces,
        ) = plot.plot_model_hierarchical_importance(
            hierarchical_clustering_dendrogram_plot_without_annotations,
            self.result,
            rounding_function,
            digits,
            show_change
        )

        hierarchical_clustering_dendrogram_plot = plot.add_text_to_dendrogram(
            hierarchical_clustering_dendrogram_plot_without_annotations,
            updated_dendro_traces,
            rounding_function, 
            digits,
            type=&#34;clustering&#34;,
        )

        hierarchical_importance_dendrogram_plot = plot.add_text_to_dendrogram(
            hierarchical_importance_dendrogram_plot_without_annotations,
            updated_dendro_traces,
            rounding_function, 
            digits,
            type=&#34;importance&#34;,
        )

        ## left plot
        fig = plot.plot_single_aspects_importance(
            self.single_variable_importance,
            variables_order,
            rounding_function,
            digits,
            vcolors
        )
        
        fig.layout[&#34;xaxis&#34;][&#34;range&#34;] = (
            fig.layout[&#34;xaxis&#34;][&#34;range&#34;][0],
            fig.layout[&#34;xaxis&#34;][&#34;range&#34;][1] * 1.05,
        )
        y_vals = [-5 - i * 10 for i in range(m)]
        fig.data[0][&#34;y&#34;] = y_vals

        ## triplot
        min_x_imp, max_x_imp = np.Inf, -np.Inf
        for data in hierarchical_importance_dendrogram_plot[&#34;data&#34;][::-1]:
            data[&#34;xaxis&#34;] = &#34;x2&#34;
            data[&#34;hoverinfo&#34;] = &#34;text&#34;
            data[&#34;line&#34;] = {&#34;color&#34;: &#34;#46bac2&#34;, &#34;width&#34;: 2}
            fig.add_trace(data)
            min_x_imp = np.min([min_x_imp, np.min(data[&#34;x&#34;])])
            max_x_imp = np.max([max_x_imp, np.max(data[&#34;x&#34;])])
        min_max_margin_imp = (max_x_imp - min_x_imp) * 0.15

        min_x_clust, max_x_clust = np.Inf, -np.Inf
        for data in hierarchical_clustering_dendrogram_plot[&#34;data&#34;]:
            data[&#34;xaxis&#34;] = &#34;x3&#34;
            data[&#34;hoverinfo&#34;] = &#34;text&#34;
            data[&#34;line&#34;] = {&#34;color&#34;: &#34;#46bac2&#34;, &#34;width&#34;: 2}
            fig.add_trace(data)
            min_x_clust = np.min([min_x_clust, np.min(data[&#34;x&#34;])])
            max_x_clust = np.max([max_x_clust, np.max(data[&#34;x&#34;])])
        min_max_margin_clust = (max_x_clust - min_x_clust) * 0.15

        plot_height = 78 + 71 + m * bar_width + (m + 1) * bar_width / 4

        fig.update_layout(
            xaxis={
                &#34;autorange&#34;: False,
                &#34;domain&#34;: [0, 0.33],
                &#34;mirror&#34;: False,
                &#34;showgrid&#34;: False,
                &#34;showline&#34;: False,
                &#34;zeroline&#34;: False,
                &#34;showticklabels&#34;: True,
                &#34;ticks&#34;: &#34;&#34;,
                &#34;title_text&#34;: &#34;Variable importance&#34;,
            },
            xaxis2={
                &#34;domain&#34;: [0.33, 0.66],
                &#34;mirror&#34;: False,
                &#34;showgrid&#34;: False,
                &#34;showline&#34;: False,
                &#34;zeroline&#34;: False,
                &#34;showticklabels&#34;: True,
                &#34;tickvals&#34;: [0],
                &#34;ticktext&#34;: [&#34;&#34;],
                &#34;ticks&#34;: &#34;&#34;,
                &#34;title_text&#34;: &#34;Hierarchical aspect importance&#34;,
                &#34;fixedrange&#34;: True,
                &#34;autorange&#34;: False,
                &#34;range&#34;: [
                    min_x_imp - min_max_margin_imp,
                    max_x_imp + min_max_margin_imp,
                ],
            },
            xaxis3={
                &#34;domain&#34;: [0.66, 0.99],
                &#34;mirror&#34;: False,
                &#34;showgrid&#34;: False,
                &#34;showline&#34;: False,
                &#34;zeroline&#34;: False,
                &#34;showticklabels&#34;: True,
                &#34;tickvals&#34;: [0],
                &#34;ticktext&#34;: [&#34;&#34;],
                &#34;ticks&#34;: &#34;&#34;,
                &#34;title_text&#34;: &#34;Hierarchical clustering&#34;,
                &#34;fixedrange&#34;: True,
                &#34;autorange&#34;: False,
                &#34;range&#34;: [
                    min_x_clust - min_max_margin_clust,
                    max_x_clust + min_max_margin_clust,
                ],
            },
            yaxis={
                &#34;mirror&#34;: False,
                &#34;ticks&#34;: &#34;&#34;,
                &#34;fixedrange&#34;: True,
                &#34;gridwidth&#34;: 1,
                &#34;type&#34;: &#34;linear&#34;,
                &#34;tickmode&#34;: &#34;array&#34;,
                &#34;tickvals&#34;: y_vals,
                &#34;ticktext&#34;: variables_order,
            },
            title_text=title,
            title_x=0.5,
            font={&#34;color&#34;: &#34;#371ea3&#34;},
            template=&#34;none&#34;,
            margin={&#34;t&#34;: 78, &#34;b&#34;: 71, &#34;r&#34;: 30},
            width=width,
            height=plot_height,
            showlegend=False,
            hovermode=&#34;closest&#34;,

        )

        fig, middle_point = plot._add_points_on_dendrogram_traces(fig)

        ##################################################################

        if widget:
            _global_checks.global_check_import(&#39;ipywidgets&#39;, &#39;Model Triplot&#39;)
            from ipywidgets import HBox, Layout
            fig = go.FigureWidget(fig, layout={&#34;autosize&#34;: True, &#34;hoverdistance&#34;: 100})
            original_bar_colors = deepcopy([fig.data[0][&#34;marker&#34;][&#34;color&#34;]] * m)
            original_text_colors = deepcopy(list(fig.data[0][&#34;textfont&#34;][&#34;color&#34;]))
            k = len(fig.data)
            updated_dendro_traces_in_full_figure = list(
                np.array(updated_dendro_traces) + (k - 1) / 2 + 1
            ) + list((k - 1) / 2 - np.array(updated_dendro_traces))

            def _update_childs(x, y, fig, k, selected, selected_y_cord):
                for i in range(1, k):
                    if middle_point[i] == (x, y):
                        fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                        fig.data[i][&#34;line&#34;][&#34;width&#34;] = 3
                        fig.data[k - i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                        fig.data[k - i][&#34;line&#34;][&#34;width&#34;] = 3
                        selected.append(i)
                        selected.append(k - i)
                        if (fig.data[i][&#34;y&#34;][0] + 5) % 10 == 0:
                            selected_y_cord.append((fig.data[i][&#34;y&#34;][0] + 5) // -10)
                        if (fig.data[i][&#34;y&#34;][-1] - 5) % 10 == 0:
                            selected_y_cord.append((fig.data[i][&#34;y&#34;][-1] + 5) // -10)
                        _update_childs(
                            fig.data[i][&#34;x&#34;][0],
                            fig.data[i][&#34;y&#34;][0],
                            fig,
                            k,
                            selected,
                            selected_y_cord,
                        )
                        _update_childs(
                            fig.data[i][&#34;x&#34;][-1],
                            fig.data[i][&#34;y&#34;][-1],
                            fig,
                            k,
                            selected,
                            selected_y_cord,
                        )

            def _update_trace(trace, points, selector):
                if len(points.point_inds) == 1:
                    selected_ind = points.trace_index
                    with fig.batch_update():
                        if max(fig.data[selected_ind][&#34;x&#34;]) in (max_x_clust, max_x_imp):
                            for i in range(1, k):
                                fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                                fig.data[i][&#34;line&#34;][&#34;width&#34;] = 2
                                fig.data[i][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                                fig.data[i][&#34;textfont&#34;][&#34;size&#34;] = 12
                            fig.data[0][&#34;marker&#34;][&#34;color&#34;] = original_bar_colors
                            fig.data[0][&#34;textfont&#34;][&#34;color&#34;] = original_text_colors
                        else:
                            selected = [selected_ind, k - selected_ind]
                            selected_y_cord = []
                            if (fig.data[selected_ind][&#34;y&#34;][0] - 5) % 10 == 0:
                                selected_y_cord.append(
                                    (fig.data[selected_ind][&#34;y&#34;][0] + 5) // -10
                                )
                            if (fig.data[selected_ind][&#34;y&#34;][-1] - 5) % 10 == 0:
                                selected_y_cord.append(
                                    (fig.data[selected_ind][&#34;y&#34;][-1] + 5) // -10
                                )
                            fig.data[selected_ind][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                            fig.data[selected_ind][&#34;line&#34;][&#34;width&#34;] = 3
                            fig.data[selected_ind][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                            fig.data[selected_ind][&#34;textfont&#34;][&#34;size&#34;] = 14
                            fig.data[k - selected_ind][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                            fig.data[k - selected_ind][&#34;line&#34;][&#34;width&#34;] = 3
                            fig.data[k - selected_ind][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                            fig.data[k - selected_ind][&#34;textfont&#34;][&#34;size&#34;] = 14
                            _update_childs(
                                fig.data[selected_ind][&#34;x&#34;][0],
                                fig.data[selected_ind][&#34;y&#34;][0],
                                fig,
                                k,
                                selected,
                                selected_y_cord,
                            )
                            _update_childs(
                                fig.data[selected_ind][&#34;x&#34;][-1],
                                fig.data[selected_ind][&#34;y&#34;][-1],
                                fig,
                                k,
                                selected,
                                selected_y_cord,
                            )
                            for i in range(1, k):
                                if i not in [selected_ind, k - selected_ind]:
                                    fig.data[i][&#34;textfont&#34;][&#34;color&#34;] = &#34;#ceced9&#34;
                                    fig.data[i][&#34;textfont&#34;][&#34;size&#34;] = 12
                                    if i not in selected:
                                        fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#ceced9&#34;
                                        fig.data[i][&#34;line&#34;][&#34;width&#34;] = 1

                            bars_colors_list = deepcopy(original_bar_colors)
                            text_colors_list = deepcopy(original_text_colors)
                            for i in range(m):
                                if i not in selected_y_cord:
                                    bars_colors_list[i] = &#34;#ceced9&#34;
                                    text_colors_list[i] = &#34;#ceced9&#34;
                            fig.data[0][&#34;marker&#34;][&#34;color&#34;] = bars_colors_list
                            fig.data[0][&#34;textfont&#34;][&#34;color&#34;] = text_colors_list

            for i in range(1, k):
                fig.data[i].on_click(_update_trace)
            return HBox([fig], layout=Layout(overflow=&#39;scroll&#39;, width=f&#39;{fig.layout.width}px&#39;))
        if show:
            fig.show(config=_theme.get_default_config())
        else:
            return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>dalex._explanation.Explanation</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dalex.aspect.ModelTriplot.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, aspect)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the result of explanation
Fit method makes calculations in place and changes the attributes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>aspect</code></strong> :&ensp;<code><a title="dalex.aspect.Aspect" href="#dalex.aspect.Aspect">Aspect</a> <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Explainer wrapper created using the Aspect class.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_model_triplot/object.py#L87-L132" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def fit(self, aspect):
    &#34;&#34;&#34;Calculate the result of explanation
    Fit method makes calculations in place and changes the attributes.

    Parameters
    ----------
    aspect : Aspect object
        Explainer wrapper created using the Aspect class.
    
    Returns
    -----------
    None
    &#34;&#34;&#34;
    _loss_function = checks.check_method_loss_function(aspect.explainer, self.loss_function)
    self.loss_function = checks.check_loss_function(_loss_function)

    self._hierarchical_clustering_dendrogram = aspect._hierarchical_clustering_dendrogram
    
    ## middle plot
    (
    self.result,
    aspect._full_hierarchical_aspect_importance
    ) = utils.calculate_model_hierarchical_importance(
        aspect,
        self.loss_function,
        self.type,
        self.N,
        self.B,
        self.processes,
        self.random_state
    )

    ## left plot data
    self.single_variable_importance, svi_full = utils.calculate_single_variable_importance(
        aspect,
        self.loss_function,
        self.type,
        self.N,
        self.B,
        self.processes,
        self.random_state,
    )

    aspect._full_hierarchical_aspect_importance = pd.concat(
        [aspect._full_hierarchical_aspect_importance, svi_full]
    )</code></pre>
</details>
</dd>
<dt id="dalex.aspect.ModelTriplot.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, digits=3, rounding_function=&lt;function around&gt;, show_change=True, bar_width=25, width=1500, vcolors=None, title='Model Triplot', widget=False, show=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the Model Triplot explanation (triplot visualization).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>digits</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of decimal places (<code>np.around</code>) to round contributions.
See <code>rounding_function</code> parameter (default is <code>3</code>).</dd>
<dt><strong><code>rounding_function</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>A function that will be used for rounding numbers (default is <code>np.around</code>).</dd>
<dt><strong><code>show_change</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code> middle panel shows dropout loss change, otherwise dropout loss
(default is <code>True</code>).</dd>
<dt><strong><code>bar_width</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Width of bars in px (default is <code>16</code>).</dd>
<dt><strong><code>width</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Width of triplot in px (default is <code>1500</code>).</dd>
<dt><strong><code>vcolors</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Color of bars (default is <code>"#46bac2"</code>).</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Title of the plot (default is <code>"Model Triplot"</code>).</dd>
<dt><strong><code>widget</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code> triplot interactive widget version is generated
(default is <code>False</code>).</dd>
<dt><strong><code>show</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd><code>True</code> shows the plot; <code>False</code> returns the plotly Figure object
(default is <code>True</code>).
NOTE: Ignored if <code>widget</code> is <code>True</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code> or <code>plotly.graph_objects.Figure</code> or <code>ipywidgets.HBox with plotly.graph_objs._figurewidget.FigureWidget</code></dt>
<dd>Return figure that can be edited or saved. See <code>show</code> parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_model_triplot/object.py#L134-L441" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot(
    self,
    digits=3,
    rounding_function=np.around,
    show_change = True,
    bar_width=25,
    width=1500,
    vcolors=None,
    title=&#34;Model Triplot&#34;,
    widget=False,
    show=True
):
    &#34;&#34;&#34;Plot the Model Triplot explanation (triplot visualization).

    Parameters
    ----------
    digits : int, optional
        Number of decimal places (`np.around`) to round contributions.
        See `rounding_function` parameter (default is `3`).
    rounding_function : function, optional
        A function that will be used for rounding numbers (default is `np.around`).
    show_change : bool, optional
        If `True` middle panel shows dropout loss change, otherwise dropout loss
        (default is `True`). 
    bar_width : float, optional
        Width of bars in px (default is `16`).
    width : float, optional
        Width of triplot in px (default is `1500`).
    vcolors : str, optional
        Color of bars (default is `&#34;#46bac2&#34;`).
    title : str, optional
        Title of the plot (default is `&#34;Model Triplot&#34;`).
    widget : bool, optional
        If `True` triplot interactive widget version is generated
        (default is `False`).
    show : bool, optional
        `True` shows the plot; `False` returns the plotly Figure object 
        (default is `True`).
        NOTE: Ignored if `widget` is `True`.

    Returns
    -------
    None or plotly.graph_objects.Figure or ipywidgets.HBox with plotly.graph_objs._figurewidget.FigureWidget
        Return figure that can be edited or saved. See `show` parameter.
    &#34;&#34;&#34;
    _global_checks.global_check_import(&#39;kaleido&#39;, &#39;Model Triplot&#39;)
    ## right plot
    hierarchical_clustering_dendrogram_plot_without_annotations = (
        self._hierarchical_clustering_dendrogram
    )
    variables_order = list(
        hierarchical_clustering_dendrogram_plot_without_annotations.layout.yaxis.ticktext
    )
    m = len(variables_order)

    ## middle plot
    (
        hierarchical_importance_dendrogram_plot_without_annotations,
        updated_dendro_traces,
    ) = plot.plot_model_hierarchical_importance(
        hierarchical_clustering_dendrogram_plot_without_annotations,
        self.result,
        rounding_function,
        digits,
        show_change
    )

    hierarchical_clustering_dendrogram_plot = plot.add_text_to_dendrogram(
        hierarchical_clustering_dendrogram_plot_without_annotations,
        updated_dendro_traces,
        rounding_function, 
        digits,
        type=&#34;clustering&#34;,
    )

    hierarchical_importance_dendrogram_plot = plot.add_text_to_dendrogram(
        hierarchical_importance_dendrogram_plot_without_annotations,
        updated_dendro_traces,
        rounding_function, 
        digits,
        type=&#34;importance&#34;,
    )

    ## left plot
    fig = plot.plot_single_aspects_importance(
        self.single_variable_importance,
        variables_order,
        rounding_function,
        digits,
        vcolors
    )
    
    fig.layout[&#34;xaxis&#34;][&#34;range&#34;] = (
        fig.layout[&#34;xaxis&#34;][&#34;range&#34;][0],
        fig.layout[&#34;xaxis&#34;][&#34;range&#34;][1] * 1.05,
    )
    y_vals = [-5 - i * 10 for i in range(m)]
    fig.data[0][&#34;y&#34;] = y_vals

    ## triplot
    min_x_imp, max_x_imp = np.Inf, -np.Inf
    for data in hierarchical_importance_dendrogram_plot[&#34;data&#34;][::-1]:
        data[&#34;xaxis&#34;] = &#34;x2&#34;
        data[&#34;hoverinfo&#34;] = &#34;text&#34;
        data[&#34;line&#34;] = {&#34;color&#34;: &#34;#46bac2&#34;, &#34;width&#34;: 2}
        fig.add_trace(data)
        min_x_imp = np.min([min_x_imp, np.min(data[&#34;x&#34;])])
        max_x_imp = np.max([max_x_imp, np.max(data[&#34;x&#34;])])
    min_max_margin_imp = (max_x_imp - min_x_imp) * 0.15

    min_x_clust, max_x_clust = np.Inf, -np.Inf
    for data in hierarchical_clustering_dendrogram_plot[&#34;data&#34;]:
        data[&#34;xaxis&#34;] = &#34;x3&#34;
        data[&#34;hoverinfo&#34;] = &#34;text&#34;
        data[&#34;line&#34;] = {&#34;color&#34;: &#34;#46bac2&#34;, &#34;width&#34;: 2}
        fig.add_trace(data)
        min_x_clust = np.min([min_x_clust, np.min(data[&#34;x&#34;])])
        max_x_clust = np.max([max_x_clust, np.max(data[&#34;x&#34;])])
    min_max_margin_clust = (max_x_clust - min_x_clust) * 0.15

    plot_height = 78 + 71 + m * bar_width + (m + 1) * bar_width / 4

    fig.update_layout(
        xaxis={
            &#34;autorange&#34;: False,
            &#34;domain&#34;: [0, 0.33],
            &#34;mirror&#34;: False,
            &#34;showgrid&#34;: False,
            &#34;showline&#34;: False,
            &#34;zeroline&#34;: False,
            &#34;showticklabels&#34;: True,
            &#34;ticks&#34;: &#34;&#34;,
            &#34;title_text&#34;: &#34;Variable importance&#34;,
        },
        xaxis2={
            &#34;domain&#34;: [0.33, 0.66],
            &#34;mirror&#34;: False,
            &#34;showgrid&#34;: False,
            &#34;showline&#34;: False,
            &#34;zeroline&#34;: False,
            &#34;showticklabels&#34;: True,
            &#34;tickvals&#34;: [0],
            &#34;ticktext&#34;: [&#34;&#34;],
            &#34;ticks&#34;: &#34;&#34;,
            &#34;title_text&#34;: &#34;Hierarchical aspect importance&#34;,
            &#34;fixedrange&#34;: True,
            &#34;autorange&#34;: False,
            &#34;range&#34;: [
                min_x_imp - min_max_margin_imp,
                max_x_imp + min_max_margin_imp,
            ],
        },
        xaxis3={
            &#34;domain&#34;: [0.66, 0.99],
            &#34;mirror&#34;: False,
            &#34;showgrid&#34;: False,
            &#34;showline&#34;: False,
            &#34;zeroline&#34;: False,
            &#34;showticklabels&#34;: True,
            &#34;tickvals&#34;: [0],
            &#34;ticktext&#34;: [&#34;&#34;],
            &#34;ticks&#34;: &#34;&#34;,
            &#34;title_text&#34;: &#34;Hierarchical clustering&#34;,
            &#34;fixedrange&#34;: True,
            &#34;autorange&#34;: False,
            &#34;range&#34;: [
                min_x_clust - min_max_margin_clust,
                max_x_clust + min_max_margin_clust,
            ],
        },
        yaxis={
            &#34;mirror&#34;: False,
            &#34;ticks&#34;: &#34;&#34;,
            &#34;fixedrange&#34;: True,
            &#34;gridwidth&#34;: 1,
            &#34;type&#34;: &#34;linear&#34;,
            &#34;tickmode&#34;: &#34;array&#34;,
            &#34;tickvals&#34;: y_vals,
            &#34;ticktext&#34;: variables_order,
        },
        title_text=title,
        title_x=0.5,
        font={&#34;color&#34;: &#34;#371ea3&#34;},
        template=&#34;none&#34;,
        margin={&#34;t&#34;: 78, &#34;b&#34;: 71, &#34;r&#34;: 30},
        width=width,
        height=plot_height,
        showlegend=False,
        hovermode=&#34;closest&#34;,

    )

    fig, middle_point = plot._add_points_on_dendrogram_traces(fig)

    ##################################################################

    if widget:
        _global_checks.global_check_import(&#39;ipywidgets&#39;, &#39;Model Triplot&#39;)
        from ipywidgets import HBox, Layout
        fig = go.FigureWidget(fig, layout={&#34;autosize&#34;: True, &#34;hoverdistance&#34;: 100})
        original_bar_colors = deepcopy([fig.data[0][&#34;marker&#34;][&#34;color&#34;]] * m)
        original_text_colors = deepcopy(list(fig.data[0][&#34;textfont&#34;][&#34;color&#34;]))
        k = len(fig.data)
        updated_dendro_traces_in_full_figure = list(
            np.array(updated_dendro_traces) + (k - 1) / 2 + 1
        ) + list((k - 1) / 2 - np.array(updated_dendro_traces))

        def _update_childs(x, y, fig, k, selected, selected_y_cord):
            for i in range(1, k):
                if middle_point[i] == (x, y):
                    fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                    fig.data[i][&#34;line&#34;][&#34;width&#34;] = 3
                    fig.data[k - i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                    fig.data[k - i][&#34;line&#34;][&#34;width&#34;] = 3
                    selected.append(i)
                    selected.append(k - i)
                    if (fig.data[i][&#34;y&#34;][0] + 5) % 10 == 0:
                        selected_y_cord.append((fig.data[i][&#34;y&#34;][0] + 5) // -10)
                    if (fig.data[i][&#34;y&#34;][-1] - 5) % 10 == 0:
                        selected_y_cord.append((fig.data[i][&#34;y&#34;][-1] + 5) // -10)
                    _update_childs(
                        fig.data[i][&#34;x&#34;][0],
                        fig.data[i][&#34;y&#34;][0],
                        fig,
                        k,
                        selected,
                        selected_y_cord,
                    )
                    _update_childs(
                        fig.data[i][&#34;x&#34;][-1],
                        fig.data[i][&#34;y&#34;][-1],
                        fig,
                        k,
                        selected,
                        selected_y_cord,
                    )

        def _update_trace(trace, points, selector):
            if len(points.point_inds) == 1:
                selected_ind = points.trace_index
                with fig.batch_update():
                    if max(fig.data[selected_ind][&#34;x&#34;]) in (max_x_clust, max_x_imp):
                        for i in range(1, k):
                            fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                            fig.data[i][&#34;line&#34;][&#34;width&#34;] = 2
                            fig.data[i][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                            fig.data[i][&#34;textfont&#34;][&#34;size&#34;] = 12
                        fig.data[0][&#34;marker&#34;][&#34;color&#34;] = original_bar_colors
                        fig.data[0][&#34;textfont&#34;][&#34;color&#34;] = original_text_colors
                    else:
                        selected = [selected_ind, k - selected_ind]
                        selected_y_cord = []
                        if (fig.data[selected_ind][&#34;y&#34;][0] - 5) % 10 == 0:
                            selected_y_cord.append(
                                (fig.data[selected_ind][&#34;y&#34;][0] + 5) // -10
                            )
                        if (fig.data[selected_ind][&#34;y&#34;][-1] - 5) % 10 == 0:
                            selected_y_cord.append(
                                (fig.data[selected_ind][&#34;y&#34;][-1] + 5) // -10
                            )
                        fig.data[selected_ind][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                        fig.data[selected_ind][&#34;line&#34;][&#34;width&#34;] = 3
                        fig.data[selected_ind][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                        fig.data[selected_ind][&#34;textfont&#34;][&#34;size&#34;] = 14
                        fig.data[k - selected_ind][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                        fig.data[k - selected_ind][&#34;line&#34;][&#34;width&#34;] = 3
                        fig.data[k - selected_ind][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                        fig.data[k - selected_ind][&#34;textfont&#34;][&#34;size&#34;] = 14
                        _update_childs(
                            fig.data[selected_ind][&#34;x&#34;][0],
                            fig.data[selected_ind][&#34;y&#34;][0],
                            fig,
                            k,
                            selected,
                            selected_y_cord,
                        )
                        _update_childs(
                            fig.data[selected_ind][&#34;x&#34;][-1],
                            fig.data[selected_ind][&#34;y&#34;][-1],
                            fig,
                            k,
                            selected,
                            selected_y_cord,
                        )
                        for i in range(1, k):
                            if i not in [selected_ind, k - selected_ind]:
                                fig.data[i][&#34;textfont&#34;][&#34;color&#34;] = &#34;#ceced9&#34;
                                fig.data[i][&#34;textfont&#34;][&#34;size&#34;] = 12
                                if i not in selected:
                                    fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#ceced9&#34;
                                    fig.data[i][&#34;line&#34;][&#34;width&#34;] = 1

                        bars_colors_list = deepcopy(original_bar_colors)
                        text_colors_list = deepcopy(original_text_colors)
                        for i in range(m):
                            if i not in selected_y_cord:
                                bars_colors_list[i] = &#34;#ceced9&#34;
                                text_colors_list[i] = &#34;#ceced9&#34;
                        fig.data[0][&#34;marker&#34;][&#34;color&#34;] = bars_colors_list
                        fig.data[0][&#34;textfont&#34;][&#34;color&#34;] = text_colors_list

        for i in range(1, k):
            fig.data[i].on_click(_update_trace)
        return HBox([fig], layout=Layout(overflow=&#39;scroll&#39;, width=f&#39;{fig.layout.width}px&#39;))
    if show:
        fig.show(config=_theme.get_default_config())
    else:
        return fig</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dalex.aspect.PredictAspectImportance"><code class="flex name class">
<span>class <span class="ident">PredictAspectImportance</span></span>
<span>(</span><span>variable_groups, type='default', N=2000, B=25, n_aspects=None, sample_method='default', f=2, depend_method='assoc', corr_method='spearman', agg_method='max', processes=1, random_state=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate predict-level aspect importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>variable_groups</code></strong> :&ensp;<code>dict</code> of <code>lists </code></dt>
<dd>Variables grouped in aspects to calculate their importance.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'default', 'shap'}</code>, optional</dt>
<dd>Type of aspect importance/attributions (default is <code>'default'</code>, which means
the use of simplified LIME method).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>data</code> attribute
before the calculation of aspect importance (default is <code>2000</code>).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'shap'</code>. Number of random paths to calculate aspect
attributions (default is <code>25</code>).
NOTE: Ignored if <code>type</code> is not <code>'shap'</code>.</dd>
<dt><strong><code>n_aspects</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code>. Maximum number of non-zero importances, i.e.
coefficients after lasso fitting (default is <code>None</code>, which means the linear regression is used).
NOTE: Ignored if <code>type</code> is not <code>'default'</code>.</dd>
<dt><strong><code>sample_method</code></strong> :&ensp;<code>{'default', 'binom'}</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code>. Sampling method for creating binary matrix
used as mask for replacing aspects in sampled data (default is <code>'default'</code>, which means
it randomly replaces one or two zeros per row; <code>'binom'</code> replaces random number of zeros
per row).
NOTE: Ignored if <code>type</code> is not <code>'default'</code>.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code> and <code>sample_method == 'binom'</code>. Parameter
controlling average number of replaced zeros for binomial sampling (default is <code>2</code>).
NOTE: Ignored if <code>type</code> is not <code>'default'</code> or <code>sample_method</code> is not <code>'binom'</code>.</dd>
<dt><strong><code>depend_method</code></strong> :&ensp;<code>{'assoc', 'pps'}</code> or <code>function</code>, optional</dt>
<dd>The method of calculating the dependencies between variables (i.e. the dependency
matrix). Default is <code>'assoc'</code>, which means the use of statistical association
(correlation coefficient, Cram√©r's V and eta-quared);
<code>'pps'</code> stands for Power Predictive Score.
NOTE: When a function is passed, it is called with the <code>data</code> and it
must return a symmetric dependency matrix (<code>pd.DataFrame</code> with variable names as
columns and rows).</dd>
<dt><strong><code>corr_method</code></strong> :&ensp;<code>{'spearman', 'pearson', 'kendall'}</code>, optional</dt>
<dd>The method of calculating correlation between numerical variables
(default is <code>'spearman'</code>).
NOTE: Ignored if <code>depend_method</code> is not <code>'assoc'</code>.</dd>
<dt><strong><code>agg_method</code></strong> :&ensp;<code>{'max', 'min', 'avg'}</code>, optional</dt>
<dd>The method of aggregating the PPS values for pairs of variables
(default is <code>'max'</code>).
NOTE: Ignored if <code>depend_method</code> is not <code>'pps'</code>.</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'shap'</code>. Number of parallel processes to use in calculations.
Iterated over <code>B</code> (default is <code>1</code>, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>result</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Main result attribute of an explanation.</dd>
<dt><strong><code>prediction</code></strong> :&ensp;<code>float</code></dt>
<dd>Prediction for <code>new_observation</code>.</dd>
<dt><strong><code>intercept</code></strong> :&ensp;<code>float</code></dt>
<dd>Average prediction for <code>data</code>.</dd>
<dt><strong><code>variable_groups</code></strong> :&ensp;<code>dict</code> of <code>lists </code></dt>
<dd>Variables grouped in aspects to calculate their importance.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'default', 'shap'}</code></dt>
<dd>Type of aspect importance/attributions to calculate.</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of observations that will be sampled from the <code>data</code> attribute
before the calculation of aspect importance.</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of random paths to calculate aspect attributions.</dd>
<dt><strong><code>n_aspects</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum number of non-zero importances.</dd>
<dt><strong><code>sample_method</code></strong> :&ensp;<code>{'default', 'binom'}</code></dt>
<dd>Sampling method for creating binary matrix used as mask for replacing aspects in sampled data.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>int</code></dt>
<dd>Average number of replaced zeros for binomial sampling.</dd>
<dt><strong><code>depend_method</code></strong> :&ensp;<code>{'assoc', 'pps'}</code></dt>
<dd>The method of calculating the dependencies between variables.</dd>
<dt><strong><code>corr_method</code></strong> :&ensp;<code>{'spearman', 'pearson', 'kendall'}</code></dt>
<dd>The method of calculating correlation between numerical variables.</dd>
<dt><strong><code>agg_method</code></strong> :&ensp;<code>{'max', 'min', 'avg'}</code></dt>
<dd>The method of aggregating the PPS values for pairs of variables.</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code></dt>
<dd>Set seed for random number generator.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li><a href="https://arxiv.org/abs/2104.03403">https://arxiv.org/abs/2104.03403</a></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_predict_aspect_importance/object.py#L12-L436" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class PredictAspectImportance(Explanation):
    &#34;&#34;&#34;Calculate predict-level aspect importance

    Parameters
    -----------
    variable_groups : dict of lists 
        Variables grouped in aspects to calculate their importance. 
    type : {&#39;default&#39;, &#39;shap&#39;}, optional
        Type of aspect importance/attributions (default is `&#39;default&#39;`, which means 
        the use of simplified LIME method).
    N : int, optional
        Number of observations that will be sampled from the `data` attribute
        before the calculation of aspect importance (default is `2000`).
    B : int, optional
        Parameter specific for `type == &#39;shap&#39;`. Number of random paths to calculate aspect
        attributions (default is `25`).
        NOTE: Ignored if `type` is not `&#39;shap&#39;`.
    n_aspects : int, optional
        Parameter specific for `type == &#39;default&#39;`. Maximum number of non-zero importances, i.e.
        coefficients after lasso fitting (default is `None`, which means the linear regression is used).
        NOTE: Ignored if `type` is not `&#39;default&#39;`.
    sample_method : {&#39;default&#39;, &#39;binom&#39;}, optional
        Parameter specific for `type == &#39;default&#39;`. Sampling method for creating binary matrix 
        used as mask for replacing aspects in sampled data (default is `&#39;default&#39;`, which means 
        it randomly replaces one or two zeros per row; `&#39;binom&#39;` replaces random number of zeros 
        per row).
        NOTE: Ignored if `type` is not `&#39;default&#39;`.
    f : int, optional
        Parameter specific for `type == &#39;default&#39;` and `sample_method == &#39;binom&#39;`. Parameter 
        controlling average number of replaced zeros for binomial sampling (default is `2`). 
        NOTE: Ignored if `type` is not `&#39;default&#39;` or `sample_method` is not `&#39;binom&#39;`.
    depend_method: {&#39;assoc&#39;, &#39;pps&#39;} or function, optional
        The method of calculating the dependencies between variables (i.e. the dependency 
        matrix). Default is `&#39;assoc&#39;`, which means the use of statistical association 
        (correlation coefficient, Cram√©r&#39;s V and eta-quared); 
        `&#39;pps&#39;` stands for Power Predictive Score.
        NOTE: When a function is passed, it is called with the `data` and it 
        must return a symmetric dependency matrix (`pd.DataFrame` with variable names as 
        columns and rows).
    corr_method : {&#39;spearman&#39;, &#39;pearson&#39;, &#39;kendall&#39;}, optional
        The method of calculating correlation between numerical variables 
        (default is `&#39;spearman&#39;`).
        NOTE: Ignored if `depend_method` is not `&#39;assoc&#39;`.
    agg_method : {&#39;max&#39;, &#39;min&#39;, &#39;avg&#39;}, optional
        The method of aggregating the PPS values for pairs of variables 
        (default is `&#39;max&#39;`).
        NOTE: Ignored if `depend_method` is not `&#39;pps&#39;`. 
    processes : int, optional
        Parameter specific for `type == &#39;shap&#39;`. Number of parallel processes to use in calculations.
        Iterated over `B` (default is `1`, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).

    Attributes
    -----------
    result : pd.DataFrame
        Main result attribute of an explanation.
    prediction : float
        Prediction for `new_observation`.
    intercept : float
        Average prediction for `data`.
    variable_groups : dict of lists 
        Variables grouped in aspects to calculate their importance. 
    type : {&#39;default&#39;, &#39;shap&#39;}
        Type of aspect importance/attributions to calculate.
    N : int
        Number of observations that will be sampled from the `data` attribute
        before the calculation of aspect importance.
    B : int
        Number of random paths to calculate aspect attributions.
    n_aspects : int
        Maximum number of non-zero importances.
    sample_method : {&#39;default&#39;, &#39;binom&#39;}
        Sampling method for creating binary matrix used as mask for replacing aspects in sampled data.
    f : int
        Average number of replaced zeros for binomial sampling.
    depend_method : {&#39;assoc&#39;, &#39;pps&#39;}
        The method of calculating the dependencies between variables.
    corr_method : {&#39;spearman&#39;, &#39;pearson&#39;, &#39;kendall&#39;}
        The method of calculating correlation between numerical variables.
    agg_method : {&#39;max&#39;, &#39;min&#39;, &#39;avg&#39;}
        The method of aggregating the PPS values for pairs of variables.
    processes : int
        Number of parallel processes to use in calculations. Iterated over `B`.
    random_state : int
        Set seed for random number generator.
    
    Notes
    -----
    - https://arxiv.org/abs/2104.03403
    &#34;&#34;&#34;
    def __init__(
        self,
        variable_groups,
        type=&#34;default&#34;,
        N=2000,
        B=25,
        n_aspects=None,
        sample_method=&#34;default&#34;,
        f=2,
        depend_method=&#34;assoc&#34;,
        corr_method=&#34;spearman&#34;,
        agg_method=&#34;max&#34;,
        processes=1,
        random_state=None,
        **kwargs
    ):
        
        types = (&#39;default&#39;, &#39;shap&#39;)
        aliases = {&#39;simplified_lime&#39;: &#39;default&#39;, &#39;lime&#39;: &#39;default&#39;, &#39;shapley_values&#39;: &#39;shap&#39;}
        _type = checks.check_method_type(type, types, aliases)
        _processes = checks.check_processes(processes)
        _random_state = checks.check_random_state(random_state)
        _depend_method, _corr_method, _agg_method = checks.check_method_depend(depend_method, corr_method, agg_method)
        self._depend_matrix = None
        if &#34;_depend_matrix&#34; in kwargs:
            self._depend_matrix = kwargs.get(&#34;_depend_matrix&#34;)
        
        self.variable_groups = variable_groups
        self.type = _type
        self.N = N
        self.B = B
        self.n_aspects = n_aspects
        self.sample_method = sample_method
        self.f = f
        self.depend_method = _depend_method
        self.corr_method = _corr_method
        self.agg_method = _agg_method
        self.random_state = _random_state
        self.processes = _processes
        self.prediction = None
        self.intercept = None
        self.result = pd.DataFrame()

    def _repr_html_(self):
        return self.result._repr_html_()

    def fit(self, explainer, new_observation):
        &#34;&#34;&#34;Calculate the result of explanation
        Fit method makes calculations in place and changes the attributes.

        Parameters
        ----------
        explainer : Explainer object
            Model wrapper created using the Explainer class.
        new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
            An observation for which a prediction needs to be explained.
        
        Returns
        -----------
        None
        &#34;&#34;&#34;
        _new_observation = checks.check_new_observation(new_observation, explainer)
        checks.check_columns_in_new_observation(_new_observation, explainer)
        _variable_groups = checks.check_variable_groups(self.variable_groups, explainer)
        
        self.prediction = explainer.predict(_new_observation)[0]
        self.intercept = explainer.y_hat.mean()
        
        if self.type == &#34;default&#34;:
            self.result = utils.calculate_predict_aspect_importance(
                explainer,
                _new_observation,
                _variable_groups,
                self.N,
                self.n_aspects,
                self.sample_method,
                self.f,
                self.random_state,
            )
        else:
            self.result = utils.calculate_shap_predict_aspect_importance(
                explainer, 
                _new_observation,
                _variable_groups,
                self.N,
                self.B,
                self.processes,
                self.random_state
            )

        self.result.insert(4, &#34;min_depend&#34;, None)
        self.result.insert(5, &#34;vars_min_depend&#34;, None)

        # if there is _depend_matrix in kwargs (called from Aspect object) 
        if self._depend_matrix is not None:
            vars_min_depend, min_depend = get_min_depend_from_matrix(self._depend_matrix, 
                    self.result.variable_names
                )
        else:
            vars_min_depend, min_depend = calculate_min_depend(
                self.result.variable_names, 
                explainer.data,
                self.depend_method,
                self.corr_method,
                self.agg_method,
            )

        self.result[&#34;min_depend&#34;] = min_depend
        self.result[&#34;vars_min_depend&#34;] = vars_min_depend

    def plot(
        self,
        objects=None,
        baseline=None,
        max_aspects=10,
        show_variable_names=True,
        digits=3,
        rounding_function=np.around,
        bar_width=25,
        min_max=None,
        vcolors=None,
        title=&#34;Predict Aspect Importance&#34;,
        vertical_spacing=None,
        show=True,
    ):
        &#34;&#34;&#34;Plot the Predict Aspect Importance explanation.

        Parameters
        ----------
        objects : PredictAspectImportance object or array_like of PredictAspectImportance objects
            Additional objects to plot in subplots (default is `None`).
        baseline: float, optional
            Starting x point for bars 
            (default is 0 if `type` is `&#39;default&#39;` and average prediction if `type` is `&#39;shap&#39;`).
        max_aspects : int, optional
            Maximum number of aspects that will be presented for for each subplot
            (default is `10`).
        show_variable_names : bool, optional
            `True` shows names of variables grouped in aspects; `False` shows names of aspects
            (default is `True`).
        digits : int, optional
            Number of decimal places (`np.around`) to round contributions.
            See `rounding_function` parameter (default is `3`).
        rounding_function : function, optional
            A function that will be used for rounding numbers (default is `np.around`).
        bar_width : float, optional
            Width of bars in px (default is `16`).
        min_max : 2-tuple of float, optional
            Range of OX axis (default is `[min-0.15*(max-min), max+0.15*(max-min)]`).
        vcolors : 2-tuple of str, optional
            Color of bars (default is `[&#34;#8bdcbe&#34;, &#34;#f05a71&#34;]`).
        title : str, optional
            Title of the plot (default is `&#34;Predict Aspect Importance&#34;`).
        vertical_spacing : float &lt;0, 1&gt;, optional
            Ratio of vertical space between the plots (default is `0.2/number of rows`).
        show : bool, optional
            `True` shows the plot; `False` returns the plotly Figure object that can 
            be edited or saved using the `write_image()` method (default is `True`).

        Returns
        -------
        None or plotly.graph_objects.Figure
            Return figure that can be edited or saved. See `show` parameter.
        &#34;&#34;&#34;

        _result_list = [self.result.copy()]
        _intercept_list = [self.intercept]
        # are there any other objects to plot?
        if objects is None:
            n = 1
        elif isinstance(objects, self.__class__):
            n = 2
            _result_list += [objects.result.copy()]
            _intercept_list += [objects.intercept]
        elif isinstance(objects, (list, tuple)):
            n = len(objects) + 1
            for ob in objects:
                _global_checks.global_check_object_class(ob, self.__class__)
                _result_list += [ob.result.copy()]
                _intercept_list += [ob.intercept]
        else:
            _global_checks.global_raise_objects_class(objects, self.__class__)

        model_names = [
            result.iloc[0, result.columns.get_loc(&#34;label&#34;)] for result in _result_list
        ]

        if vertical_spacing is None:
            vertical_spacing = 0.2 / n

        # generate plot
        fig = make_subplots(
            rows=n,
            cols=1,
            shared_xaxes=True,
            vertical_spacing=vertical_spacing,
            x_title=&#34;aspect importance&#34;,
            subplot_titles=model_names,
        )

        plot_height = 78 + 71

        if vcolors is None:
            vcolors = _theme.get_aspect_importance_colors()

        if min_max is None:
            temp_min_max = [np.Inf, -np.Inf]
        else:
            temp_min_max = min_max

        for i, _result in enumerate(_result_list):
            if _result.shape[0] &lt;= max_aspects:
                m = _result.shape[0]
            else:
                m = max_aspects + 1

            if baseline is None:
                if self.type == &#39;shap&#39;:
                    baseline = _intercept_list[i]
                else: 
                    baseline = 0 
            
            _result = _result.iloc[:max_aspects, :]
            _result.loc[:, &#34;importance&#34;] = rounding_function(
                _result.loc[:, &#34;importance&#34;], digits
            )

            _result[&#34;color&#34;] = [0 if imp &gt; 0 else 1 for imp in _result[&#34;importance&#34;]]
            _result[&#34;tooltip_text&#34;] = _result.apply(
                lambda row: plot.tooltip_text(row, rounding_function, digits, self.type),
                axis=1,
            )
            _result[&#34;label_text&#34;] = _global_utils.convert_float_to_str(
                _result.importance, &#34;+&#34;
            )

            fig.add_shape(
                type=&#34;line&#34;,
                x0=baseline,
                x1=baseline,
                y0=-1,
                y1=m,
                yref=&#34;paper&#34;,
                xref=&#34;x&#34;,
                line={&#34;color&#34;: &#34;#371ea3&#34;, &#34;width&#34;: 1.5, &#34;dash&#34;: &#34;dot&#34;},
                row=i + 1,
                col=1,
            )

            fig.add_bar(
                orientation=&#34;h&#34;,
                y=[
                    &#34;, &#34;.join(variables_list)
                    for variables_list in _result[&#34;variable_names&#34;]
                ]
                if show_variable_names
                else _result[&#34;aspect_name&#34;].tolist(),
                x=_result[&#34;importance&#34;].tolist(),
                textposition=&#34;outside&#34;,
                text=_result[&#34;label_text&#34;].tolist(),
                marker_color=[vcolors[int(c)] for c in _result[&#34;color&#34;].tolist()],
                base=baseline,
                hovertext=_result[&#34;tooltip_text&#34;].tolist(),
                hoverinfo=&#34;text&#34;,
                hoverlabel={&#34;bgcolor&#34;: &#34;rgba(0,0,0,0.8)&#34;},
                showlegend=False,
                row=i + 1,
                col=1,
            )

            fig.update_yaxes(
                {
                    &#34;type&#34;: &#34;category&#34;,
                    &#34;autorange&#34;: &#34;reversed&#34;,
                    &#34;gridwidth&#34;: 2,
                    &#34;automargin&#34;: True,
                    &#34;ticks&#34;: &#34;outside&#34;,
                    &#34;tickcolor&#34;: &#34;white&#34;,
                    &#34;ticklen&#34;: 10,
                    &#34;fixedrange&#34;: True,
                },
                row=i + 1,
                col=1,
            )

            fig.update_xaxes(
                {
                    &#34;type&#34;: &#34;linear&#34;,
                    &#34;gridwidth&#34;: 2,
                    &#34;zeroline&#34;: False,
                    &#34;automargin&#34;: True,
                    &#34;ticks&#34;: &#34;outside&#34;,
                    &#34;tickcolor&#34;: &#34;white&#34;,
                    &#34;ticklen&#34;: 3,
                    &#34;fixedrange&#34;: True,
                },
                row=i + 1,
                col=1,
            )

            plot_height += m * bar_width + (m + 1) * bar_width / 4

            if min_max is None:
                cum = _result.importance.values + baseline
                min_max_margin =  cum.ptp() * 0.15 
                temp_min_max[0] = np.min(
                    [
                        temp_min_max[0],
                        cum.min() - min_max_margin,
                    ]
                )
                temp_min_max[1] = np.max(
                    [
                        temp_min_max[1],
                        cum.max() + min_max_margin,
                    ]
                )

        plot_height += (n - 1) * 70

        fig.update_xaxes({&#34;range&#34;: temp_min_max})
        fig.update_layout(
            title_text=title,
            title_x=0.15,
            font={&#34;color&#34;: &#34;#371ea3&#34;},
            template=&#34;none&#34;,
            height=plot_height,
            margin={&#34;t&#34;: 78, &#34;b&#34;: 71, &#34;r&#34;: 30},
        )

        if show:
            fig.show(config=_theme.get_default_config())
        else:
            return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>dalex._explanation.Explanation</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dalex.aspect.PredictAspectImportance.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, explainer, new_observation)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the result of explanation
Fit method makes calculations in place and changes the attributes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>explainer</code></strong> :&ensp;<code>Explainer <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Model wrapper created using the Explainer class.</dd>
<dt><strong><code>new_observation</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code> or <code>pd.DataFrame (1,p)</code></dt>
<dd>An observation for which a prediction needs to be explained.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_predict_aspect_importance/object.py#L149-L211" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def fit(self, explainer, new_observation):
    &#34;&#34;&#34;Calculate the result of explanation
    Fit method makes calculations in place and changes the attributes.

    Parameters
    ----------
    explainer : Explainer object
        Model wrapper created using the Explainer class.
    new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
        An observation for which a prediction needs to be explained.
    
    Returns
    -----------
    None
    &#34;&#34;&#34;
    _new_observation = checks.check_new_observation(new_observation, explainer)
    checks.check_columns_in_new_observation(_new_observation, explainer)
    _variable_groups = checks.check_variable_groups(self.variable_groups, explainer)
    
    self.prediction = explainer.predict(_new_observation)[0]
    self.intercept = explainer.y_hat.mean()
    
    if self.type == &#34;default&#34;:
        self.result = utils.calculate_predict_aspect_importance(
            explainer,
            _new_observation,
            _variable_groups,
            self.N,
            self.n_aspects,
            self.sample_method,
            self.f,
            self.random_state,
        )
    else:
        self.result = utils.calculate_shap_predict_aspect_importance(
            explainer, 
            _new_observation,
            _variable_groups,
            self.N,
            self.B,
            self.processes,
            self.random_state
        )

    self.result.insert(4, &#34;min_depend&#34;, None)
    self.result.insert(5, &#34;vars_min_depend&#34;, None)

    # if there is _depend_matrix in kwargs (called from Aspect object) 
    if self._depend_matrix is not None:
        vars_min_depend, min_depend = get_min_depend_from_matrix(self._depend_matrix, 
                self.result.variable_names
            )
    else:
        vars_min_depend, min_depend = calculate_min_depend(
            self.result.variable_names, 
            explainer.data,
            self.depend_method,
            self.corr_method,
            self.agg_method,
        )

    self.result[&#34;min_depend&#34;] = min_depend
    self.result[&#34;vars_min_depend&#34;] = vars_min_depend</code></pre>
</details>
</dd>
<dt id="dalex.aspect.PredictAspectImportance.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, objects=None, baseline=None, max_aspects=10, show_variable_names=True, digits=3, rounding_function=&lt;function around&gt;, bar_width=25, min_max=None, vcolors=None, title='Predict Aspect Importance', vertical_spacing=None, show=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the Predict Aspect Importance explanation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>objects</code></strong> :&ensp;<code><a title="dalex.aspect.PredictAspectImportance" href="#dalex.aspect.PredictAspectImportance">PredictAspectImportance</a> <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code> or <code>array_like</code> of <code><a title="dalex.aspect.PredictAspectImportance" href="#dalex.aspect.PredictAspectImportance">PredictAspectImportance</a> objects</code></dt>
<dd>Additional objects to plot in subplots (default is <code>None</code>).</dd>
<dt><strong><code>baseline</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Starting x point for bars
(default is 0 if <code>type</code> is <code>'default'</code> and average prediction if <code>type</code> is <code>'shap'</code>).</dd>
<dt><strong><code>max_aspects</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of aspects that will be presented for for each subplot
(default is <code>10</code>).</dd>
<dt><strong><code>show_variable_names</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd><code>True</code> shows names of variables grouped in aspects; <code>False</code> shows names of aspects
(default is <code>True</code>).</dd>
<dt><strong><code>digits</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of decimal places (<code>np.around</code>) to round contributions.
See <code>rounding_function</code> parameter (default is <code>3</code>).</dd>
<dt><strong><code>rounding_function</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>A function that will be used for rounding numbers (default is <code>np.around</code>).</dd>
<dt><strong><code>bar_width</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Width of bars in px (default is <code>16</code>).</dd>
<dt><strong><code>min_max</code></strong> :&ensp;<code>2-tuple</code> of <code>float</code>, optional</dt>
<dd>Range of OX axis (default is <code>[min-0.15*(max-min), max+0.15*(max-min)]</code>).</dd>
<dt><strong><code>vcolors</code></strong> :&ensp;<code>2-tuple</code> of <code>str</code>, optional</dt>
<dd>Color of bars (default is <code>["#8bdcbe", "#f05a71"]</code>).</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Title of the plot (default is <code>"Predict Aspect Importance"</code>).</dd>
<dt><strong><code>vertical_spacing</code></strong> :&ensp;<code>float &lt;0, 1&gt;</code>, optional</dt>
<dd>Ratio of vertical space between the plots (default is <code>0.2/number of rows</code>).</dd>
<dt><strong><code>show</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd><code>True</code> shows the plot; <code>False</code> returns the plotly Figure object that can
be edited or saved using the <code>write_image()</code> method (default is <code>True</code>).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code> or <code>plotly.graph_objects.Figure</code></dt>
<dd>Return figure that can be edited or saved. See <code>show</code> parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_predict_aspect_importance/object.py#L213-L436" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot(
    self,
    objects=None,
    baseline=None,
    max_aspects=10,
    show_variable_names=True,
    digits=3,
    rounding_function=np.around,
    bar_width=25,
    min_max=None,
    vcolors=None,
    title=&#34;Predict Aspect Importance&#34;,
    vertical_spacing=None,
    show=True,
):
    &#34;&#34;&#34;Plot the Predict Aspect Importance explanation.

    Parameters
    ----------
    objects : PredictAspectImportance object or array_like of PredictAspectImportance objects
        Additional objects to plot in subplots (default is `None`).
    baseline: float, optional
        Starting x point for bars 
        (default is 0 if `type` is `&#39;default&#39;` and average prediction if `type` is `&#39;shap&#39;`).
    max_aspects : int, optional
        Maximum number of aspects that will be presented for for each subplot
        (default is `10`).
    show_variable_names : bool, optional
        `True` shows names of variables grouped in aspects; `False` shows names of aspects
        (default is `True`).
    digits : int, optional
        Number of decimal places (`np.around`) to round contributions.
        See `rounding_function` parameter (default is `3`).
    rounding_function : function, optional
        A function that will be used for rounding numbers (default is `np.around`).
    bar_width : float, optional
        Width of bars in px (default is `16`).
    min_max : 2-tuple of float, optional
        Range of OX axis (default is `[min-0.15*(max-min), max+0.15*(max-min)]`).
    vcolors : 2-tuple of str, optional
        Color of bars (default is `[&#34;#8bdcbe&#34;, &#34;#f05a71&#34;]`).
    title : str, optional
        Title of the plot (default is `&#34;Predict Aspect Importance&#34;`).
    vertical_spacing : float &lt;0, 1&gt;, optional
        Ratio of vertical space between the plots (default is `0.2/number of rows`).
    show : bool, optional
        `True` shows the plot; `False` returns the plotly Figure object that can 
        be edited or saved using the `write_image()` method (default is `True`).

    Returns
    -------
    None or plotly.graph_objects.Figure
        Return figure that can be edited or saved. See `show` parameter.
    &#34;&#34;&#34;

    _result_list = [self.result.copy()]
    _intercept_list = [self.intercept]
    # are there any other objects to plot?
    if objects is None:
        n = 1
    elif isinstance(objects, self.__class__):
        n = 2
        _result_list += [objects.result.copy()]
        _intercept_list += [objects.intercept]
    elif isinstance(objects, (list, tuple)):
        n = len(objects) + 1
        for ob in objects:
            _global_checks.global_check_object_class(ob, self.__class__)
            _result_list += [ob.result.copy()]
            _intercept_list += [ob.intercept]
    else:
        _global_checks.global_raise_objects_class(objects, self.__class__)

    model_names = [
        result.iloc[0, result.columns.get_loc(&#34;label&#34;)] for result in _result_list
    ]

    if vertical_spacing is None:
        vertical_spacing = 0.2 / n

    # generate plot
    fig = make_subplots(
        rows=n,
        cols=1,
        shared_xaxes=True,
        vertical_spacing=vertical_spacing,
        x_title=&#34;aspect importance&#34;,
        subplot_titles=model_names,
    )

    plot_height = 78 + 71

    if vcolors is None:
        vcolors = _theme.get_aspect_importance_colors()

    if min_max is None:
        temp_min_max = [np.Inf, -np.Inf]
    else:
        temp_min_max = min_max

    for i, _result in enumerate(_result_list):
        if _result.shape[0] &lt;= max_aspects:
            m = _result.shape[0]
        else:
            m = max_aspects + 1

        if baseline is None:
            if self.type == &#39;shap&#39;:
                baseline = _intercept_list[i]
            else: 
                baseline = 0 
        
        _result = _result.iloc[:max_aspects, :]
        _result.loc[:, &#34;importance&#34;] = rounding_function(
            _result.loc[:, &#34;importance&#34;], digits
        )

        _result[&#34;color&#34;] = [0 if imp &gt; 0 else 1 for imp in _result[&#34;importance&#34;]]
        _result[&#34;tooltip_text&#34;] = _result.apply(
            lambda row: plot.tooltip_text(row, rounding_function, digits, self.type),
            axis=1,
        )
        _result[&#34;label_text&#34;] = _global_utils.convert_float_to_str(
            _result.importance, &#34;+&#34;
        )

        fig.add_shape(
            type=&#34;line&#34;,
            x0=baseline,
            x1=baseline,
            y0=-1,
            y1=m,
            yref=&#34;paper&#34;,
            xref=&#34;x&#34;,
            line={&#34;color&#34;: &#34;#371ea3&#34;, &#34;width&#34;: 1.5, &#34;dash&#34;: &#34;dot&#34;},
            row=i + 1,
            col=1,
        )

        fig.add_bar(
            orientation=&#34;h&#34;,
            y=[
                &#34;, &#34;.join(variables_list)
                for variables_list in _result[&#34;variable_names&#34;]
            ]
            if show_variable_names
            else _result[&#34;aspect_name&#34;].tolist(),
            x=_result[&#34;importance&#34;].tolist(),
            textposition=&#34;outside&#34;,
            text=_result[&#34;label_text&#34;].tolist(),
            marker_color=[vcolors[int(c)] for c in _result[&#34;color&#34;].tolist()],
            base=baseline,
            hovertext=_result[&#34;tooltip_text&#34;].tolist(),
            hoverinfo=&#34;text&#34;,
            hoverlabel={&#34;bgcolor&#34;: &#34;rgba(0,0,0,0.8)&#34;},
            showlegend=False,
            row=i + 1,
            col=1,
        )

        fig.update_yaxes(
            {
                &#34;type&#34;: &#34;category&#34;,
                &#34;autorange&#34;: &#34;reversed&#34;,
                &#34;gridwidth&#34;: 2,
                &#34;automargin&#34;: True,
                &#34;ticks&#34;: &#34;outside&#34;,
                &#34;tickcolor&#34;: &#34;white&#34;,
                &#34;ticklen&#34;: 10,
                &#34;fixedrange&#34;: True,
            },
            row=i + 1,
            col=1,
        )

        fig.update_xaxes(
            {
                &#34;type&#34;: &#34;linear&#34;,
                &#34;gridwidth&#34;: 2,
                &#34;zeroline&#34;: False,
                &#34;automargin&#34;: True,
                &#34;ticks&#34;: &#34;outside&#34;,
                &#34;tickcolor&#34;: &#34;white&#34;,
                &#34;ticklen&#34;: 3,
                &#34;fixedrange&#34;: True,
            },
            row=i + 1,
            col=1,
        )

        plot_height += m * bar_width + (m + 1) * bar_width / 4

        if min_max is None:
            cum = _result.importance.values + baseline
            min_max_margin =  cum.ptp() * 0.15 
            temp_min_max[0] = np.min(
                [
                    temp_min_max[0],
                    cum.min() - min_max_margin,
                ]
            )
            temp_min_max[1] = np.max(
                [
                    temp_min_max[1],
                    cum.max() + min_max_margin,
                ]
            )

    plot_height += (n - 1) * 70

    fig.update_xaxes({&#34;range&#34;: temp_min_max})
    fig.update_layout(
        title_text=title,
        title_x=0.15,
        font={&#34;color&#34;: &#34;#371ea3&#34;},
        template=&#34;none&#34;,
        height=plot_height,
        margin={&#34;t&#34;: 78, &#34;b&#34;: 71, &#34;r&#34;: 30},
    )

    if show:
        fig.show(config=_theme.get_default_config())
    else:
        return fig</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="dalex.aspect.PredictTriplot"><code class="flex name class">
<span>class <span class="ident">PredictTriplot</span></span>
<span>(</span><span>type='default', N=2000, B=25, sample_method='default', f=2, processes=1, random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate predict-level hierarchical aspect importance</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type</code></strong> :&ensp;<code>{'default', 'shap'}</code>, optional</dt>
<dd>Type of aspect importance/attributions (default is <code>'default'</code>, which means
the use of simplified LIME method).</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of observations that will be sampled from the <code>explainer.data</code> attribute
before the calculation of aspect importance (default is <code>2000</code>).</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'shap'</code>. Number of random paths to calculate aspect
attributions (default is <code>25</code>).
NOTE: Ignored if <code>type</code> is not <code>'shap'</code>.</dd>
<dt><strong><code>sample_method</code></strong> :&ensp;<code>{'default', 'binom'}</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code>. Sampling method for creating binary matrix
used as mask for replacing aspects in data (default is <code>'default'</code>, which means
it randomly replaces one or two zeros per row; <code>'binom'</code> replaces random number of zeros
per row).
NOTE: Ignored if <code>type</code> is not <code>'default'</code>.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Parameter specific for <code>type == 'default'</code> and <code>sample_method == 'binom'</code>. Parameter
controlling average number of replaced zeros for binomial sampling (default is <code>2</code>).
NOTE: Ignored if <code>type</code> is not <code>'default'</code> or <code>sample_method</code> is not <code>'binom'</code>.</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>
(default is <code>1</code>, which means no parallel computation).</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Set seed for random number generator (default is random seed).</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>result</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Main result attribute of an explanation.</dd>
<dt><strong><code>single_variable_importance</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Additional result attribute of an explanation (it contains information
about the importance of individual variables).</dd>
<dt><strong><code>prediction</code></strong> :&ensp;<code>float</code></dt>
<dd>Prediction for <code>new_observation</code>.</dd>
<dt><strong><code>intercept</code></strong> :&ensp;<code>float</code></dt>
<dd>Average prediction for <code>data</code>.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>{'default', 'shap'}</code></dt>
<dd>Type of aspect importance/attributions to calculate.</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of observations that will be sampled from the <code>data</code> attribute
before the calculation of aspect importance.</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of random paths to calculate aspect attributions.</dd>
<dt><strong><code>sample_method</code></strong> :&ensp;<code>{'default', 'binom'}</code></dt>
<dd>Sampling method for creating binary matrix used as mask for replacing aspects in sampled data.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>int</code></dt>
<dd>Average number of replaced zeros for binomial sampling.</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of parallel processes to use in calculations. Iterated over <code>B</code>.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>Set seed for random number generator.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li><a href="https://arxiv.org/abs/2104.03403">https://arxiv.org/abs/2104.03403</a></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_predict_triplot/object.py#L14-L495" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class PredictTriplot(Explanation):
    &#34;&#34;&#34;Calculate predict-level hierarchical aspect importance

    Parameters
    ----------
    type : {&#39;default&#39;, &#39;shap&#39;}, optional
        Type of aspect importance/attributions (default is `&#39;default&#39;`, which means 
        the use of simplified LIME method).
    N : int, optional
        Number of observations that will be sampled from the `explainer.data` attribute
        before the calculation of aspect importance (default is `2000`).
    B : int, optional
        Parameter specific for `type == &#39;shap&#39;`. Number of random paths to calculate aspect
        attributions (default is `25`).
        NOTE: Ignored if `type` is not `&#39;shap&#39;`.
    sample_method : {&#39;default&#39;, &#39;binom&#39;}, optional
        Parameter specific for `type == &#39;default&#39;`. Sampling method for creating binary matrix 
        used as mask for replacing aspects in data (default is `&#39;default&#39;`, which means 
        it randomly replaces one or two zeros per row; `&#39;binom&#39;` replaces random number of zeros 
        per row).
        NOTE: Ignored if `type` is not `&#39;default&#39;`.
    f : int, optional
        Parameter specific for `type == &#39;default&#39;` and `sample_method == &#39;binom&#39;`. Parameter 
        controlling average number of replaced zeros for binomial sampling (default is `2`). 
        NOTE: Ignored if `type` is not `&#39;default&#39;` or `sample_method` is not `&#39;binom&#39;`.
    processes : int, optional
        Number of parallel processes to use in calculations. Iterated over `B`
        (default is `1`, which means no parallel computation).
    random_state : int, optional
        Set seed for random number generator (default is random seed).

    Attributes
    -----------
    result : pd.DataFrame
        Main result attribute of an explanation.
    single_variable_importance : pd.DataFrame
        Additional result attribute of an explanation (it contains information 
        about the importance of individual variables).
    prediction : float
        Prediction for `new_observation`.
    intercept : float
        Average prediction for `data`.
    type : {&#39;default&#39;, &#39;shap&#39;}
        Type of aspect importance/attributions to calculate.
    N : int
        Number of observations that will be sampled from the `data` attribute
        before the calculation of aspect importance.
    B : int
        Number of random paths to calculate aspect attributions.
    sample_method : {&#39;default&#39;, &#39;binom&#39;}
        Sampling method for creating binary matrix used as mask for replacing aspects in sampled data.
    f : int
        Average number of replaced zeros for binomial sampling.
    processes : int
        Number of parallel processes to use in calculations. Iterated over `B`.
    random_state : int or None
        Set seed for random number generator.

    Notes
    -----
    - https://arxiv.org/abs/2104.03403
    &#34;&#34;&#34;
    def __init__(
        self,
        type=&#34;default&#34;,
        N=2000,
        B=25,
        sample_method=&#34;default&#34;,
        f=2,
        processes=1,
        random_state=None,
    ):
        types = (&#34;default&#34;, &#34;shap&#34;)
        aliases = {&#34;simplified_lime&#34;: &#34;default&#34;, &#34;lime&#34;: &#34;default&#34;}
        _type = checks.check_method_type(type, types, aliases)
        self.type = _type
        _processes = checks.check_processes(processes)
        self.processes = _processes
        _random_state = checks.check_random_state(random_state)
        self.random_state = _random_state
        self.N = N
        self.B = checks.check_B(B)
        self.sample_method = sample_method
        self.f = f
        self.result = pd.DataFrame()
        self.single_variable_importance = None
        self._hierarchical_clustering_dendrogram = None

    def _repr_html_(self):
        return self.result._repr_html_()

    def fit(self, aspect, new_observation):
        &#34;&#34;&#34;Calculate the result of explanation
        Fit method makes calculations in place and changes the attributes.

        Parameters
        ----------
        aspect : Aspect object
            Explainer wrapper created using the Aspect class.
        new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
            An observation for which a prediction needs to be explained.
        
        Returns
        -----------
        None
        &#34;&#34;&#34;

        _new_observation = checks.check_new_observation(new_observation, aspect.explainer)
        checks.check_columns_in_new_observation(_new_observation, aspect.explainer)

        self._hierarchical_clustering_dendrogram = aspect._hierarchical_clustering_dendrogram

        self.prediction = aspect.explainer.predict(_new_observation)[0]
        self.intercept = aspect.explainer.y_hat.mean()

        ## middle plot data
        self.result = (
            utils.calculate_predict_hierarchical_importance(
                aspect,
                _new_observation,
                self.type,
                self.N,
                self.B,
                self.sample_method,
                self.f,
                self.processes,
                self.random_state,
            )
        )

        self.result.insert(3, &#34;min_depend&#34;, None)
        self.result.insert(4, &#34;vars_min_depend&#34;, None)
        for index, row in self.result.iterrows():
            _matching_row = aspect._dendrogram_aspects_ordered.loc[
                pd.Series(map(set, aspect._dendrogram_aspects_ordered.variable_names))
                == set(row.variable_names)
            ]
            min_dep = _matching_row.min_depend.values[0]
            vars_min_depend = _matching_row.vars_min_depend.values[0]
            self.result.at[index, &#34;min_depend&#34;] = min_dep
            self.result.at[
                index, &#34;vars_min_depend&#34;
            ] = vars_min_depend

        ## left plot data
        self.single_variable_importance = utils.calculate_single_variable_importance(
            aspect,
            _new_observation,
            self.type,
            self.N,
            self.B, 
            self.sample_method,
            self.f,
            self.processes,
            self.random_state
        )
        
    def plot(
        self,
        absolute_value=False,
        digits=3,
        rounding_function=np.around,
        bar_width=25,
        width=1500,
        abbrev_labels=0,
        vcolors=None,
        title=&#34;Predict Triplot&#34;,
        widget=False,
        show=True
    ):
        &#34;&#34;&#34;Plot the Predict Triplot explanation (triplot visualization).

        Parameters
        ----------
        absolute_value : bool, optional
            If `True` aspect importance values are drawn as absolute values 
            (default is `False`).
        digits : int, optional
            Number of decimal places (`np.around`) to round contributions.
            See `rounding_function` parameter (default is `3`).
        rounding_function : function, optional
            A function that will be used for rounding numbers (default is `np.around`).
        bar_width : float, optional
            Width of bars in px (default is `25`).
        width : float, optional
            Width of triplot in px (default is `1500`).
        abbrev_labels : int, optional
            If greater than 0, labels for axis Y will be abbreviated according to this parameter
            (default is `0`, which means no abbreviation).
        vcolors : 2-tuple of str, optional
            Color of bars (default is `[&#34;#8bdcbe&#34;, &#34;#f05a71&#34;]`).
        title : str, optional
            Title of the plot (default is `&#34;Predict Triplot&#34;`).
        widget : bool, optional
            If `True` triplot interactive widget version is generated
            (default is `False`).
        show : bool, optional
            `True` shows the plot; `False` returns the plotly Figure object 
            (default is `True`).
            NOTE: Ignored if `widget` is `True`.

        Returns
        -------
        None or plotly.graph_objects.Figure or ipywidgets.HBox with plotly.graph_objs._figurewidget.FigureWidget
            Return figure that can be edited or saved. See `show` parameter.
        &#34;&#34;&#34;    
        _global_checks.global_check_import(&#39;kaleido&#39;, &#39;Predict Triplot&#39;)
        ## right plot
        hierarchical_clustering_dendrogram_plot_without_annotations = go.Figure(
            self._hierarchical_clustering_dendrogram
        )
        variables_order = list(
            hierarchical_clustering_dendrogram_plot_without_annotations.layout.yaxis.ticktext
        )

        ## middle plot
        (
            hierarchical_importance_dendrogram_plot_without_annotations,
            updated_dendro_traces,
        ) = plot.plot_predict_hierarchical_importance(
            hierarchical_clustering_dendrogram_plot_without_annotations,
            self.result,
            rounding_function,
            digits,
            absolute_value,
            self.type,
        )

        hierarchical_clustering_dendrogram_plot = plot.add_text_to_dendrogram(
            hierarchical_clustering_dendrogram_plot_without_annotations,
            updated_dendro_traces,
            rounding_function,
            digits,
            type=&#34;clustering&#34;,
        )

        hierarchical_importance_dendrogram_plot = plot.add_text_to_dendrogram(
            hierarchical_importance_dendrogram_plot_without_annotations,
            updated_dendro_traces,
            rounding_function,
            digits,
            type=&#34;importance&#34;,
        )

        ## left plot
        fig = plot.plot_single_aspects_importance(
            self.single_variable_importance,
            variables_order,
            rounding_function,
            digits,
            vcolors,
        )
        fig.layout[&#34;xaxis&#34;][&#34;range&#34;] = (
            fig.layout[&#34;xaxis&#34;][&#34;range&#34;][0],
            fig.layout[&#34;xaxis&#34;][&#34;range&#34;][1] * 1.05,
        )
        m = len(variables_order)
        y_vals = [-5 - i * 10 for i in range(m)]
        fig.data[0][&#34;y&#34;] = y_vals

        ## triplot

        fig.add_shape(
            type=&#34;line&#34;,
            x0=0,
            x1=0,
            y0=-0.01,
            y1=1.01,
            yref=&#34;paper&#34;,
            xref=&#34;x2&#34;,
            line={&#34;color&#34;: &#34;#371ea3&#34;, &#34;width&#34;: 1.5, &#34;dash&#34;: &#34;dot&#34;},
        )

        min_x_imp, max_x_imp = np.Inf, -np.Inf
        for data in hierarchical_importance_dendrogram_plot[&#34;data&#34;][::-1]:
            data[&#34;xaxis&#34;] = &#34;x2&#34;
            data[&#34;hoverinfo&#34;] = &#34;text&#34;
            data[&#34;line&#34;] = {&#34;color&#34;: &#34;#46bac2&#34;, &#34;width&#34;: 2}
            fig.add_trace(data)
            min_x_imp = np.min([min_x_imp, np.min(data[&#34;x&#34;])])
            max_x_imp = np.max([max_x_imp, np.max(data[&#34;x&#34;])])
        min_max_margin_imp = (max_x_imp - min_x_imp) * 0.15

        min_x_clust, max_x_clust = np.Inf, -np.Inf
        for data in hierarchical_clustering_dendrogram_plot[&#34;data&#34;]:
            data[&#34;xaxis&#34;] = &#34;x3&#34;
            data[&#34;hoverinfo&#34;] = &#34;text&#34;
            data[&#34;line&#34;] = {&#34;color&#34;: &#34;#46bac2&#34;, &#34;width&#34;: 2}
            fig.add_trace(data)
            min_x_clust = np.min([min_x_clust, np.min(data[&#34;x&#34;])])
            max_x_clust = np.max([max_x_clust, np.max(data[&#34;x&#34;])])
        min_max_margin_clust = (max_x_clust - min_x_clust) * 0.15

        plot_height = 78 + 71 + m * bar_width + (m + 1) * bar_width / 4
        ticktext = plot.get_ticktext_for_plot(
            self.single_variable_importance, variables_order, abbrev_labels
        )

        fig.update_layout(
            xaxis={
                &#34;autorange&#34;: False,
                &#34;domain&#34;: [0, 0.33],
                &#34;mirror&#34;: False,
                &#34;showgrid&#34;: False,
                &#34;showline&#34;: False,
                &#34;zeroline&#34;: False,
                &#34;ticks&#34;: &#34;&#34;,
                &#34;title_text&#34;: &#34;Local variable importance&#34;,
            },
            xaxis2={
                &#34;domain&#34;: [0.33, 0.66],
                &#34;mirror&#34;: False,
                &#34;showgrid&#34;: False,
                &#34;showline&#34;: False,
                &#34;zeroline&#34;: False,
                &#34;showticklabels&#34;: True,
                &#34;tickvals&#34;: [0],
                &#34;ticktext&#34;: [&#34;&#34;],
                &#34;ticks&#34;: &#34;&#34;,
                &#34;title_text&#34;: &#34;Hierarchical aspect importance&#34;,
                &#34;autorange&#34;: False,
                &#34;fixedrange&#34;: True,
                &#34;range&#34;: [
                    min_x_imp - min_max_margin_imp,
                    max_x_imp + min_max_margin_imp,
                ],
            },
            xaxis3={
                &#34;domain&#34;: [0.66, 0.99],
                &#34;mirror&#34;: False,
                &#34;showgrid&#34;: False,
                &#34;showline&#34;: False,
                &#34;zeroline&#34;: False,
                &#34;showticklabels&#34;: True,
                &#34;tickvals&#34;: [0],
                &#34;ticktext&#34;: [&#34;&#34;],
                &#34;ticks&#34;: &#34;&#34;,
                &#34;title_text&#34;: &#34;Hierarchical clustering&#34;,
                &#34;autorange&#34;: False,
                &#34;fixedrange&#34;: True,
                &#34;range&#34;: [
                    min_x_clust - min_max_margin_clust,
                    max_x_clust + min_max_margin_clust,
                ],
            },
            yaxis={
                &#34;mirror&#34;: False,
                &#34;ticks&#34;: &#34;&#34;,
                &#34;fixedrange&#34;: True,
                &#34;gridwidth&#34;: 1,
                &#34;type&#34;: &#34;linear&#34;,
                &#34;tickmode&#34;: &#34;array&#34;,
                &#34;tickvals&#34;: y_vals,
                &#34;ticktext&#34;: ticktext,
            },
            title_text=title,
            title_x=0.5,
            font={&#34;color&#34;: &#34;#371ea3&#34;},
            template=&#34;none&#34;,
            margin={&#34;t&#34;: 78, &#34;b&#34;: 71, &#34;r&#34;: 30},
            width=width,
            height=plot_height,
            showlegend=False,
            hovermode=&#34;closest&#34;
        )

        fig, middle_point = plot._add_points_on_dendrogram_traces(fig)

        ##################################################################

        if widget:
            _global_checks.global_check_import(&#39;ipywidgets&#39;, &#39;Predict Triplot&#39;)
            from ipywidgets import HBox, Layout
            fig = go.FigureWidget(fig, layout={&#34;autosize&#34;: True, &#34;hoverdistance&#34;: 100})
            original_bar_colors = deepcopy(list(fig.data[0][&#34;marker&#34;][&#34;color&#34;]))
            original_text_colors = deepcopy(list(fig.data[0][&#34;textfont&#34;][&#34;color&#34;]))
            k = len(fig.data)
            updated_dendro_traces_in_full_figure = list(
                np.array(updated_dendro_traces) + (k - 1) / 2 + 1
            ) + list((k - 1) / 2 - np.array(updated_dendro_traces))

            def _update_childs(x, y, fig, k, selected, selected_y_cord):
                for i in range(1, k):
                    if middle_point[i] == (x, y):
                        fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                        fig.data[i][&#34;line&#34;][&#34;width&#34;] = 3
                        fig.data[k - i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                        fig.data[k - i][&#34;line&#34;][&#34;width&#34;] = 3
                        selected.append(i)
                        selected.append(k - i)
                        if (fig.data[i][&#34;y&#34;][0] + 5) % 10 == 0:
                            selected_y_cord.append((fig.data[i][&#34;y&#34;][0] + 5) // -10)
                        if (fig.data[i][&#34;y&#34;][-1] - 5) % 10 == 0:
                            selected_y_cord.append((fig.data[i][&#34;y&#34;][-1] + 5) // -10)
                        _update_childs(
                            fig.data[i][&#34;x&#34;][0],
                            fig.data[i][&#34;y&#34;][0],
                            fig,
                            k,
                            selected,
                            selected_y_cord,
                        )
                        _update_childs(
                            fig.data[i][&#34;x&#34;][-1],
                            fig.data[i][&#34;y&#34;][-1],
                            fig,
                            k,
                            selected,
                            selected_y_cord,
                        )

            def _update_trace(trace, points, selector):
                if len(points.point_inds) == 1:
                    selected_ind = points.trace_index
                    with fig.batch_update():
                        if selected_ind not in updated_dendro_traces_in_full_figure:
                            for i in range(1, k):
                                fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                                fig.data[i][&#34;line&#34;][&#34;width&#34;] = 2
                                fig.data[i][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                                fig.data[i][&#34;textfont&#34;][&#34;size&#34;] = 12
                            fig.data[0][&#34;marker&#34;][&#34;color&#34;] = original_bar_colors
                            fig.data[0][&#34;textfont&#34;][&#34;color&#34;] = original_text_colors
                        else:
                            selected = [selected_ind, k - selected_ind]
                            selected_y_cord = []
                            if (fig.data[selected_ind][&#34;y&#34;][0] - 5) % 10 == 0:
                                selected_y_cord.append(
                                    (fig.data[selected_ind][&#34;y&#34;][0] + 5) // -10
                                )
                            if (fig.data[selected_ind][&#34;y&#34;][-1] - 5) % 10 == 0:
                                selected_y_cord.append(
                                    (fig.data[selected_ind][&#34;y&#34;][-1] + 5) // -10
                                )
                            fig.data[selected_ind][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                            fig.data[selected_ind][&#34;line&#34;][&#34;width&#34;] = 3
                            fig.data[selected_ind][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                            fig.data[selected_ind][&#34;textfont&#34;][&#34;size&#34;] = 14
                            fig.data[k - selected_ind][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                            fig.data[k - selected_ind][&#34;line&#34;][&#34;width&#34;] = 3
                            fig.data[k - selected_ind][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                            fig.data[k - selected_ind][&#34;textfont&#34;][&#34;size&#34;] = 14
                            _update_childs(
                                fig.data[selected_ind][&#34;x&#34;][0],
                                fig.data[selected_ind][&#34;y&#34;][0],
                                fig,
                                k,
                                selected,
                                selected_y_cord,
                            )
                            _update_childs(
                                fig.data[selected_ind][&#34;x&#34;][-1],
                                fig.data[selected_ind][&#34;y&#34;][-1],
                                fig,
                                k,
                                selected,
                                selected_y_cord,
                            )
                            for i in range(1, k):
                                if i not in [selected_ind, k - selected_ind]:
                                    fig.data[i][&#34;textfont&#34;][&#34;color&#34;] = &#34;#ceced9&#34;
                                    fig.data[i][&#34;textfont&#34;][&#34;size&#34;] = 12
                                    if i not in selected:
                                        fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#ceced9&#34;
                                        fig.data[i][&#34;line&#34;][&#34;width&#34;] = 1

                            bars_colors_list = deepcopy(original_bar_colors)
                            text_colors_list = deepcopy(original_text_colors)
                            for i in range(m):
                                if i not in selected_y_cord:
                                    bars_colors_list[i] = &#34;#ceced9&#34;
                                    text_colors_list[i] = &#34;#ceced9&#34;
                            fig.data[0][&#34;marker&#34;][&#34;color&#34;] = bars_colors_list
                            fig.data[0][&#34;textfont&#34;][&#34;color&#34;] = text_colors_list

            for i in range(1, k):
                fig.data[i].on_click(_update_trace)
            return HBox([fig], layout=Layout(overflow=&#39;scroll&#39;, width=f&#39;{fig.layout.width}px&#39;))
        if show:
            fig.show(config=_theme.get_default_config())
        else:
            return fig</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>dalex._explanation.Explanation</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="dalex.aspect.PredictTriplot.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, aspect, new_observation)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the result of explanation
Fit method makes calculations in place and changes the attributes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>aspect</code></strong> :&ensp;<code><a title="dalex.aspect.Aspect" href="#dalex.aspect.Aspect">Aspect</a> <a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></dt>
<dd>Explainer wrapper created using the Aspect class.</dd>
<dt><strong><code>new_observation</code></strong> :&ensp;<code>pd.Series</code> or <code>np.ndarray (1d)</code> or <code>pd.DataFrame (1,p)</code></dt>
<dd>An observation for which a prediction needs to be explained.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_predict_triplot/object.py#L105-L169" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def fit(self, aspect, new_observation):
    &#34;&#34;&#34;Calculate the result of explanation
    Fit method makes calculations in place and changes the attributes.

    Parameters
    ----------
    aspect : Aspect object
        Explainer wrapper created using the Aspect class.
    new_observation : pd.Series or np.ndarray (1d) or pd.DataFrame (1,p)
        An observation for which a prediction needs to be explained.
    
    Returns
    -----------
    None
    &#34;&#34;&#34;

    _new_observation = checks.check_new_observation(new_observation, aspect.explainer)
    checks.check_columns_in_new_observation(_new_observation, aspect.explainer)

    self._hierarchical_clustering_dendrogram = aspect._hierarchical_clustering_dendrogram

    self.prediction = aspect.explainer.predict(_new_observation)[0]
    self.intercept = aspect.explainer.y_hat.mean()

    ## middle plot data
    self.result = (
        utils.calculate_predict_hierarchical_importance(
            aspect,
            _new_observation,
            self.type,
            self.N,
            self.B,
            self.sample_method,
            self.f,
            self.processes,
            self.random_state,
        )
    )

    self.result.insert(3, &#34;min_depend&#34;, None)
    self.result.insert(4, &#34;vars_min_depend&#34;, None)
    for index, row in self.result.iterrows():
        _matching_row = aspect._dendrogram_aspects_ordered.loc[
            pd.Series(map(set, aspect._dendrogram_aspects_ordered.variable_names))
            == set(row.variable_names)
        ]
        min_dep = _matching_row.min_depend.values[0]
        vars_min_depend = _matching_row.vars_min_depend.values[0]
        self.result.at[index, &#34;min_depend&#34;] = min_dep
        self.result.at[
            index, &#34;vars_min_depend&#34;
        ] = vars_min_depend

    ## left plot data
    self.single_variable_importance = utils.calculate_single_variable_importance(
        aspect,
        _new_observation,
        self.type,
        self.N,
        self.B, 
        self.sample_method,
        self.f,
        self.processes,
        self.random_state
    )</code></pre>
</details>
</dd>
<dt id="dalex.aspect.PredictTriplot.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, absolute_value=False, digits=3, rounding_function=&lt;function around&gt;, bar_width=25, width=1500, abbrev_labels=0, vcolors=None, title='Predict Triplot', widget=False, show=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the Predict Triplot explanation (triplot visualization).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>absolute_value</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code> aspect importance values are drawn as absolute values
(default is <code>False</code>).</dd>
<dt><strong><code>digits</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of decimal places (<code>np.around</code>) to round contributions.
See <code>rounding_function</code> parameter (default is <code>3</code>).</dd>
<dt><strong><code>rounding_function</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>A function that will be used for rounding numbers (default is <code>np.around</code>).</dd>
<dt><strong><code>bar_width</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Width of bars in px (default is <code>25</code>).</dd>
<dt><strong><code>width</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Width of triplot in px (default is <code>1500</code>).</dd>
<dt><strong><code>abbrev_labels</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>If greater than 0, labels for axis Y will be abbreviated according to this parameter
(default is <code>0</code>, which means no abbreviation).</dd>
<dt><strong><code>vcolors</code></strong> :&ensp;<code>2-tuple</code> of <code>str</code>, optional</dt>
<dd>Color of bars (default is <code>["#8bdcbe", "#f05a71"]</code>).</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Title of the plot (default is <code>"Predict Triplot"</code>).</dd>
<dt><strong><code>widget</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code> triplot interactive widget version is generated
(default is <code>False</code>).</dd>
<dt><strong><code>show</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd><code>True</code> shows the plot; <code>False</code> returns the plotly Figure object
(default is <code>True</code>).
NOTE: Ignored if <code>widget</code> is <code>True</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code> or <code>plotly.graph_objects.Figure</code> or <code>ipywidgets.HBox with plotly.graph_objs._figurewidget.FigureWidget</code></dt>
<dd>Return figure that can be edited or saved. See <code>show</code> parameter.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/ModelOriented/DALEX/blob/654f69e45153fe324960d5757b29f10b914c45ff/dalex/aspect/_predict_triplot/object.py#L171-L495" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def plot(
    self,
    absolute_value=False,
    digits=3,
    rounding_function=np.around,
    bar_width=25,
    width=1500,
    abbrev_labels=0,
    vcolors=None,
    title=&#34;Predict Triplot&#34;,
    widget=False,
    show=True
):
    &#34;&#34;&#34;Plot the Predict Triplot explanation (triplot visualization).

    Parameters
    ----------
    absolute_value : bool, optional
        If `True` aspect importance values are drawn as absolute values 
        (default is `False`).
    digits : int, optional
        Number of decimal places (`np.around`) to round contributions.
        See `rounding_function` parameter (default is `3`).
    rounding_function : function, optional
        A function that will be used for rounding numbers (default is `np.around`).
    bar_width : float, optional
        Width of bars in px (default is `25`).
    width : float, optional
        Width of triplot in px (default is `1500`).
    abbrev_labels : int, optional
        If greater than 0, labels for axis Y will be abbreviated according to this parameter
        (default is `0`, which means no abbreviation).
    vcolors : 2-tuple of str, optional
        Color of bars (default is `[&#34;#8bdcbe&#34;, &#34;#f05a71&#34;]`).
    title : str, optional
        Title of the plot (default is `&#34;Predict Triplot&#34;`).
    widget : bool, optional
        If `True` triplot interactive widget version is generated
        (default is `False`).
    show : bool, optional
        `True` shows the plot; `False` returns the plotly Figure object 
        (default is `True`).
        NOTE: Ignored if `widget` is `True`.

    Returns
    -------
    None or plotly.graph_objects.Figure or ipywidgets.HBox with plotly.graph_objs._figurewidget.FigureWidget
        Return figure that can be edited or saved. See `show` parameter.
    &#34;&#34;&#34;    
    _global_checks.global_check_import(&#39;kaleido&#39;, &#39;Predict Triplot&#39;)
    ## right plot
    hierarchical_clustering_dendrogram_plot_without_annotations = go.Figure(
        self._hierarchical_clustering_dendrogram
    )
    variables_order = list(
        hierarchical_clustering_dendrogram_plot_without_annotations.layout.yaxis.ticktext
    )

    ## middle plot
    (
        hierarchical_importance_dendrogram_plot_without_annotations,
        updated_dendro_traces,
    ) = plot.plot_predict_hierarchical_importance(
        hierarchical_clustering_dendrogram_plot_without_annotations,
        self.result,
        rounding_function,
        digits,
        absolute_value,
        self.type,
    )

    hierarchical_clustering_dendrogram_plot = plot.add_text_to_dendrogram(
        hierarchical_clustering_dendrogram_plot_without_annotations,
        updated_dendro_traces,
        rounding_function,
        digits,
        type=&#34;clustering&#34;,
    )

    hierarchical_importance_dendrogram_plot = plot.add_text_to_dendrogram(
        hierarchical_importance_dendrogram_plot_without_annotations,
        updated_dendro_traces,
        rounding_function,
        digits,
        type=&#34;importance&#34;,
    )

    ## left plot
    fig = plot.plot_single_aspects_importance(
        self.single_variable_importance,
        variables_order,
        rounding_function,
        digits,
        vcolors,
    )
    fig.layout[&#34;xaxis&#34;][&#34;range&#34;] = (
        fig.layout[&#34;xaxis&#34;][&#34;range&#34;][0],
        fig.layout[&#34;xaxis&#34;][&#34;range&#34;][1] * 1.05,
    )
    m = len(variables_order)
    y_vals = [-5 - i * 10 for i in range(m)]
    fig.data[0][&#34;y&#34;] = y_vals

    ## triplot

    fig.add_shape(
        type=&#34;line&#34;,
        x0=0,
        x1=0,
        y0=-0.01,
        y1=1.01,
        yref=&#34;paper&#34;,
        xref=&#34;x2&#34;,
        line={&#34;color&#34;: &#34;#371ea3&#34;, &#34;width&#34;: 1.5, &#34;dash&#34;: &#34;dot&#34;},
    )

    min_x_imp, max_x_imp = np.Inf, -np.Inf
    for data in hierarchical_importance_dendrogram_plot[&#34;data&#34;][::-1]:
        data[&#34;xaxis&#34;] = &#34;x2&#34;
        data[&#34;hoverinfo&#34;] = &#34;text&#34;
        data[&#34;line&#34;] = {&#34;color&#34;: &#34;#46bac2&#34;, &#34;width&#34;: 2}
        fig.add_trace(data)
        min_x_imp = np.min([min_x_imp, np.min(data[&#34;x&#34;])])
        max_x_imp = np.max([max_x_imp, np.max(data[&#34;x&#34;])])
    min_max_margin_imp = (max_x_imp - min_x_imp) * 0.15

    min_x_clust, max_x_clust = np.Inf, -np.Inf
    for data in hierarchical_clustering_dendrogram_plot[&#34;data&#34;]:
        data[&#34;xaxis&#34;] = &#34;x3&#34;
        data[&#34;hoverinfo&#34;] = &#34;text&#34;
        data[&#34;line&#34;] = {&#34;color&#34;: &#34;#46bac2&#34;, &#34;width&#34;: 2}
        fig.add_trace(data)
        min_x_clust = np.min([min_x_clust, np.min(data[&#34;x&#34;])])
        max_x_clust = np.max([max_x_clust, np.max(data[&#34;x&#34;])])
    min_max_margin_clust = (max_x_clust - min_x_clust) * 0.15

    plot_height = 78 + 71 + m * bar_width + (m + 1) * bar_width / 4
    ticktext = plot.get_ticktext_for_plot(
        self.single_variable_importance, variables_order, abbrev_labels
    )

    fig.update_layout(
        xaxis={
            &#34;autorange&#34;: False,
            &#34;domain&#34;: [0, 0.33],
            &#34;mirror&#34;: False,
            &#34;showgrid&#34;: False,
            &#34;showline&#34;: False,
            &#34;zeroline&#34;: False,
            &#34;ticks&#34;: &#34;&#34;,
            &#34;title_text&#34;: &#34;Local variable importance&#34;,
        },
        xaxis2={
            &#34;domain&#34;: [0.33, 0.66],
            &#34;mirror&#34;: False,
            &#34;showgrid&#34;: False,
            &#34;showline&#34;: False,
            &#34;zeroline&#34;: False,
            &#34;showticklabels&#34;: True,
            &#34;tickvals&#34;: [0],
            &#34;ticktext&#34;: [&#34;&#34;],
            &#34;ticks&#34;: &#34;&#34;,
            &#34;title_text&#34;: &#34;Hierarchical aspect importance&#34;,
            &#34;autorange&#34;: False,
            &#34;fixedrange&#34;: True,
            &#34;range&#34;: [
                min_x_imp - min_max_margin_imp,
                max_x_imp + min_max_margin_imp,
            ],
        },
        xaxis3={
            &#34;domain&#34;: [0.66, 0.99],
            &#34;mirror&#34;: False,
            &#34;showgrid&#34;: False,
            &#34;showline&#34;: False,
            &#34;zeroline&#34;: False,
            &#34;showticklabels&#34;: True,
            &#34;tickvals&#34;: [0],
            &#34;ticktext&#34;: [&#34;&#34;],
            &#34;ticks&#34;: &#34;&#34;,
            &#34;title_text&#34;: &#34;Hierarchical clustering&#34;,
            &#34;autorange&#34;: False,
            &#34;fixedrange&#34;: True,
            &#34;range&#34;: [
                min_x_clust - min_max_margin_clust,
                max_x_clust + min_max_margin_clust,
            ],
        },
        yaxis={
            &#34;mirror&#34;: False,
            &#34;ticks&#34;: &#34;&#34;,
            &#34;fixedrange&#34;: True,
            &#34;gridwidth&#34;: 1,
            &#34;type&#34;: &#34;linear&#34;,
            &#34;tickmode&#34;: &#34;array&#34;,
            &#34;tickvals&#34;: y_vals,
            &#34;ticktext&#34;: ticktext,
        },
        title_text=title,
        title_x=0.5,
        font={&#34;color&#34;: &#34;#371ea3&#34;},
        template=&#34;none&#34;,
        margin={&#34;t&#34;: 78, &#34;b&#34;: 71, &#34;r&#34;: 30},
        width=width,
        height=plot_height,
        showlegend=False,
        hovermode=&#34;closest&#34;
    )

    fig, middle_point = plot._add_points_on_dendrogram_traces(fig)

    ##################################################################

    if widget:
        _global_checks.global_check_import(&#39;ipywidgets&#39;, &#39;Predict Triplot&#39;)
        from ipywidgets import HBox, Layout
        fig = go.FigureWidget(fig, layout={&#34;autosize&#34;: True, &#34;hoverdistance&#34;: 100})
        original_bar_colors = deepcopy(list(fig.data[0][&#34;marker&#34;][&#34;color&#34;]))
        original_text_colors = deepcopy(list(fig.data[0][&#34;textfont&#34;][&#34;color&#34;]))
        k = len(fig.data)
        updated_dendro_traces_in_full_figure = list(
            np.array(updated_dendro_traces) + (k - 1) / 2 + 1
        ) + list((k - 1) / 2 - np.array(updated_dendro_traces))

        def _update_childs(x, y, fig, k, selected, selected_y_cord):
            for i in range(1, k):
                if middle_point[i] == (x, y):
                    fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                    fig.data[i][&#34;line&#34;][&#34;width&#34;] = 3
                    fig.data[k - i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                    fig.data[k - i][&#34;line&#34;][&#34;width&#34;] = 3
                    selected.append(i)
                    selected.append(k - i)
                    if (fig.data[i][&#34;y&#34;][0] + 5) % 10 == 0:
                        selected_y_cord.append((fig.data[i][&#34;y&#34;][0] + 5) // -10)
                    if (fig.data[i][&#34;y&#34;][-1] - 5) % 10 == 0:
                        selected_y_cord.append((fig.data[i][&#34;y&#34;][-1] + 5) // -10)
                    _update_childs(
                        fig.data[i][&#34;x&#34;][0],
                        fig.data[i][&#34;y&#34;][0],
                        fig,
                        k,
                        selected,
                        selected_y_cord,
                    )
                    _update_childs(
                        fig.data[i][&#34;x&#34;][-1],
                        fig.data[i][&#34;y&#34;][-1],
                        fig,
                        k,
                        selected,
                        selected_y_cord,
                    )

        def _update_trace(trace, points, selector):
            if len(points.point_inds) == 1:
                selected_ind = points.trace_index
                with fig.batch_update():
                    if selected_ind not in updated_dendro_traces_in_full_figure:
                        for i in range(1, k):
                            fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                            fig.data[i][&#34;line&#34;][&#34;width&#34;] = 2
                            fig.data[i][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                            fig.data[i][&#34;textfont&#34;][&#34;size&#34;] = 12
                        fig.data[0][&#34;marker&#34;][&#34;color&#34;] = original_bar_colors
                        fig.data[0][&#34;textfont&#34;][&#34;color&#34;] = original_text_colors
                    else:
                        selected = [selected_ind, k - selected_ind]
                        selected_y_cord = []
                        if (fig.data[selected_ind][&#34;y&#34;][0] - 5) % 10 == 0:
                            selected_y_cord.append(
                                (fig.data[selected_ind][&#34;y&#34;][0] + 5) // -10
                            )
                        if (fig.data[selected_ind][&#34;y&#34;][-1] - 5) % 10 == 0:
                            selected_y_cord.append(
                                (fig.data[selected_ind][&#34;y&#34;][-1] + 5) // -10
                            )
                        fig.data[selected_ind][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                        fig.data[selected_ind][&#34;line&#34;][&#34;width&#34;] = 3
                        fig.data[selected_ind][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                        fig.data[selected_ind][&#34;textfont&#34;][&#34;size&#34;] = 14
                        fig.data[k - selected_ind][&#34;line&#34;][&#34;color&#34;] = &#34;#46bac2&#34;
                        fig.data[k - selected_ind][&#34;line&#34;][&#34;width&#34;] = 3
                        fig.data[k - selected_ind][&#34;textfont&#34;][&#34;color&#34;] = &#34;#371ea3&#34;
                        fig.data[k - selected_ind][&#34;textfont&#34;][&#34;size&#34;] = 14
                        _update_childs(
                            fig.data[selected_ind][&#34;x&#34;][0],
                            fig.data[selected_ind][&#34;y&#34;][0],
                            fig,
                            k,
                            selected,
                            selected_y_cord,
                        )
                        _update_childs(
                            fig.data[selected_ind][&#34;x&#34;][-1],
                            fig.data[selected_ind][&#34;y&#34;][-1],
                            fig,
                            k,
                            selected,
                            selected_y_cord,
                        )
                        for i in range(1, k):
                            if i not in [selected_ind, k - selected_ind]:
                                fig.data[i][&#34;textfont&#34;][&#34;color&#34;] = &#34;#ceced9&#34;
                                fig.data[i][&#34;textfont&#34;][&#34;size&#34;] = 12
                                if i not in selected:
                                    fig.data[i][&#34;line&#34;][&#34;color&#34;] = &#34;#ceced9&#34;
                                    fig.data[i][&#34;line&#34;][&#34;width&#34;] = 1

                        bars_colors_list = deepcopy(original_bar_colors)
                        text_colors_list = deepcopy(original_text_colors)
                        for i in range(m):
                            if i not in selected_y_cord:
                                bars_colors_list[i] = &#34;#ceced9&#34;
                                text_colors_list[i] = &#34;#ceced9&#34;
                        fig.data[0][&#34;marker&#34;][&#34;color&#34;] = bars_colors_list
                        fig.data[0][&#34;textfont&#34;][&#34;color&#34;] = text_colors_list

        for i in range(1, k):
            fig.data[i].on_click(_update_trace)
        return HBox([fig], layout=Layout(overflow=&#39;scroll&#39;, width=f&#39;{fig.layout.width}px&#39;))
    if show:
        fig.show(config=_theme.get_default_config())
    else:
        return fig</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="dalex Home" href="https://dalex.drwhy.ai/">
<img src="https://raw.githubusercontent.com/ModelOriented/DALEX-docs/master/docs/misc/dalex_even.png" alt=""> dalex
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dalex" href="../index.html">dalex</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="dalex.aspect.checks" href="checks.html">dalex.aspect.checks</a></code></li>
<li><code><a title="dalex.aspect.object" href="object.html">dalex.aspect.object</a></code></li>
<li><code><a title="dalex.aspect.plot" href="plot.html">dalex.aspect.plot</a></code></li>
<li><code><a title="dalex.aspect.utils" href="utils.html">dalex.aspect.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dalex.aspect.Aspect" href="#dalex.aspect.Aspect">Aspect</a></code></h4>
<ul class="two-column">
<li><code><a title="dalex.aspect.Aspect.get_aspects" href="#dalex.aspect.Aspect.get_aspects">get_aspects</a></code></li>
<li><code><a title="dalex.aspect.Aspect.model_parts" href="#dalex.aspect.Aspect.model_parts">model_parts</a></code></li>
<li><code><a title="dalex.aspect.Aspect.model_triplot" href="#dalex.aspect.Aspect.model_triplot">model_triplot</a></code></li>
<li><code><a title="dalex.aspect.Aspect.plot_dendrogram" href="#dalex.aspect.Aspect.plot_dendrogram">plot_dendrogram</a></code></li>
<li><code><a title="dalex.aspect.Aspect.predict_parts" href="#dalex.aspect.Aspect.predict_parts">predict_parts</a></code></li>
<li><code><a title="dalex.aspect.Aspect.predict_triplot" href="#dalex.aspect.Aspect.predict_triplot">predict_triplot</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dalex.aspect.ModelAspectImportance" href="#dalex.aspect.ModelAspectImportance">ModelAspectImportance</a></code></h4>
<ul class="">
<li><code><a title="dalex.aspect.ModelAspectImportance.fit" href="#dalex.aspect.ModelAspectImportance.fit">fit</a></code></li>
<li><code><a title="dalex.aspect.ModelAspectImportance.plot" href="#dalex.aspect.ModelAspectImportance.plot">plot</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dalex.aspect.ModelTriplot" href="#dalex.aspect.ModelTriplot">ModelTriplot</a></code></h4>
<ul class="">
<li><code><a title="dalex.aspect.ModelTriplot.fit" href="#dalex.aspect.ModelTriplot.fit">fit</a></code></li>
<li><code><a title="dalex.aspect.ModelTriplot.plot" href="#dalex.aspect.ModelTriplot.plot">plot</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dalex.aspect.PredictAspectImportance" href="#dalex.aspect.PredictAspectImportance">PredictAspectImportance</a></code></h4>
<ul class="">
<li><code><a title="dalex.aspect.PredictAspectImportance.fit" href="#dalex.aspect.PredictAspectImportance.fit">fit</a></code></li>
<li><code><a title="dalex.aspect.PredictAspectImportance.plot" href="#dalex.aspect.PredictAspectImportance.plot">plot</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dalex.aspect.PredictTriplot" href="#dalex.aspect.PredictTriplot">PredictTriplot</a></code></h4>
<ul class="">
<li><code><a title="dalex.aspect.PredictTriplot.fit" href="#dalex.aspect.PredictTriplot.fit">fit</a></code></li>
<li><code><a title="dalex.aspect.PredictTriplot.plot" href="#dalex.aspect.PredictTriplot.plot">plot</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>